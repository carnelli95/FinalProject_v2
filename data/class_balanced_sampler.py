"""
Class-Balanced Sampler for Fashion JSON Encoder

í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ìƒ˜í”ŒëŸ¬
ë ˆíŠ¸ë¡œ 9% ë¬¸ì œë¥¼ í•´ê²°í•˜ì—¬ Top-5 ì„±ëŠ¥ 1-2% í–¥ìƒ ëª©í‘œ
"""

import torch
from torch.utils.data import Sampler
from typing import List, Dict, Iterator
import numpy as np
from collections import Counter


class ClassBalancedSampler(Sampler):
    """
    í´ë˜ìŠ¤ ê· í˜•ì„ ë§ì¶˜ ìƒ˜í”ŒëŸ¬
    
    ê° ë°°ì¹˜ì—ì„œ ëª¨ë“  í´ë˜ìŠ¤ê°€ ê· ë“±í•˜ê²Œ í‘œí˜„ë˜ë„ë¡ ë³´ì¥
    ë ˆíŠ¸ë¡œ(9%) í´ë˜ìŠ¤ì˜ ì–¸ë”ìƒ˜í”Œë§ ë¬¸ì œ í•´ê²°
    """
    
    def __init__(self, 
                 dataset,
                 batch_size: int,
                 oversample_minority: bool = True,
                 min_samples_per_class: int = 2):
        """
        Args:
            dataset: FashionDataset ì¸ìŠ¤í„´ìŠ¤
            batch_size: ë°°ì¹˜ í¬ê¸°
            oversample_minority: ì†Œìˆ˜ í´ë˜ìŠ¤ ì˜¤ë²„ìƒ˜í”Œë§ ì—¬ë¶€
            min_samples_per_class: ë°°ì¹˜ë‹¹ í´ë˜ìŠ¤ë³„ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        """
        self.dataset = dataset
        self.batch_size = batch_size
        self.oversample_minority = oversample_minority
        self.min_samples_per_class = min_samples_per_class
        
        # í´ë˜ìŠ¤ë³„ ì¸ë±ìŠ¤ êµ¬ì¶•
        self.class_indices = self._build_class_indices()
        self.classes = list(self.class_indices.keys())
        self.num_classes = len(self.classes)
        
        # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
        self.class_counts = {cls: len(indices) for cls, indices in self.class_indices.items()}
        
        print(f"ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:")
        for cls, count in self.class_counts.items():
            print(f"   {cls}: {count}ê°œ ({count/len(dataset)*100:.1f}%)")
        
        # ì—í¬í¬ë‹¹ ì´ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
        if oversample_minority:
            # ê°€ì¥ ë§ì€ í´ë˜ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë²„ìƒ˜í”Œë§
            max_samples = max(self.class_counts.values())
            self.samples_per_epoch = max_samples * self.num_classes
        else:
            # ì›ë³¸ ë°ì´í„°ì…‹ í¬ê¸° ìœ ì§€
            self.samples_per_epoch = len(dataset)
    
    def _build_class_indices(self) -> Dict[str, List[int]]:
        """í´ë˜ìŠ¤ë³„ ì¸ë±ìŠ¤ ë”•ì…”ë„ˆë¦¬ êµ¬ì¶•"""
        class_indices = {}
        
        for idx, item in enumerate(self.dataset.fashion_items):
            category = item.category
            if category not in class_indices:
                class_indices[category] = []
            class_indices[category].append(idx)
        
        return class_indices
    
    def __iter__(self) -> Iterator[int]:
        """ê· í˜• ì¡íŒ ë°°ì¹˜ ìƒì„±"""
        # ê° í´ë˜ìŠ¤ë³„ë¡œ ìƒ˜í”Œ ì¸ë±ìŠ¤ ì¤€ë¹„
        class_iterators = {}
        for cls in self.classes:
            indices = self.class_indices[cls].copy()
            
            if self.oversample_minority:
                # ì†Œìˆ˜ í´ë˜ìŠ¤ ì˜¤ë²„ìƒ˜í”Œë§
                max_samples = max(self.class_counts.values())
                current_samples = len(indices)
                
                if current_samples < max_samples:
                    # ë°˜ë³µ ìƒ˜í”Œë§ìœ¼ë¡œ í™•ì¥
                    repeat_factor = (max_samples + current_samples - 1) // current_samples
                    indices = indices * repeat_factor
                    indices = indices[:max_samples]
            
            # ì…”í”Œ
            np.random.shuffle(indices)
            class_iterators[cls] = iter(indices)
        
        # ë°°ì¹˜ ìƒì„±
        batch = []
        samples_generated = 0
        
        while samples_generated < self.samples_per_epoch:
            # ê° í´ë˜ìŠ¤ì—ì„œ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
            for cls in self.classes:
                try:
                    # í´ë˜ìŠ¤ë³„ ìµœì†Œ ìƒ˜í”Œ ìˆ˜ë§Œí¼ ì¶”ê°€
                    for _ in range(self.min_samples_per_class):
                        if len(batch) < self.batch_size and samples_generated < self.samples_per_epoch:
                            idx = next(class_iterators[cls])
                            batch.append(idx)
                            samples_generated += 1
                except StopIteration:
                    # í•´ë‹¹ í´ë˜ìŠ¤ ìƒ˜í”Œì´ ë¶€ì¡±í•˜ë©´ ë‹¤ì‹œ ì…”í”Œí•´ì„œ ì¬ì‹œì‘
                    indices = self.class_indices[cls].copy()
                    if self.oversample_minority:
                        max_samples = max(self.class_counts.values())
                        current_samples = len(indices)
                        if current_samples < max_samples:
                            repeat_factor = (max_samples + current_samples - 1) // current_samples
                            indices = indices * repeat_factor
                            indices = indices[:max_samples]
                    
                    np.random.shuffle(indices)
                    class_iterators[cls] = iter(indices)
                    
                    # ë‹¤ì‹œ ì‹œë„
                    try:
                        for _ in range(self.min_samples_per_class):
                            if len(batch) < self.batch_size and samples_generated < self.samples_per_epoch:
                                idx = next(class_iterators[cls])
                                batch.append(idx)
                                samples_generated += 1
                    except StopIteration:
                        continue
            
            # ë°°ì¹˜ê°€ ì™„ì„±ë˜ë©´ yield
            if len(batch) >= self.batch_size:
                yield from batch[:self.batch_size]
                batch = batch[self.batch_size:]
        
        # ë‚¨ì€ ìƒ˜í”Œë“¤ ì²˜ë¦¬
        if batch:
            yield from batch
    
    def __len__(self) -> int:
        """ì—í¬í¬ë‹¹ ì´ ìƒ˜í”Œ ìˆ˜"""
        return self.samples_per_epoch


class HardNegativeMiner:
    """
    ë ˆíŠ¸ë¡œ í´ë˜ìŠ¤ë¥¼ ìœ„í•œ Hard Negative Mining
    
    ë ˆíŠ¸ë¡œì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¡œë§¨í‹±/ë¦¬ì¡°íŠ¸ ìƒ˜í”Œì„ ì°¾ì•„
    ë” ì–´ë ¤ìš´ negative ìƒ˜í”Œë¡œ ì‚¬ìš©
    """
    
    def __init__(self, model, dataset, device='cpu'):
        self.model = model
        self.dataset = dataset
        self.device = device
        
        # ë ˆíŠ¸ë¡œ ìƒ˜í”Œ ì¸ë±ìŠ¤
        self.retro_indices = []
        self.non_retro_indices = []
        
        for idx, item in enumerate(dataset.fashion_items):
            if item.category == 'ë ˆíŠ¸ë¡œ':
                self.retro_indices.append(idx)
            else:
                self.non_retro_indices.append(idx)
    
    def find_hard_negatives(self, retro_idx: int, k: int = 5) -> List[int]:
        """
        íŠ¹ì • ë ˆíŠ¸ë¡œ ìƒ˜í”Œì— ëŒ€í•œ hard negative ì°¾ê¸°
        
        Args:
            retro_idx: ë ˆíŠ¸ë¡œ ìƒ˜í”Œ ì¸ë±ìŠ¤
            k: ë°˜í™˜í•  hard negative ìˆ˜
            
        Returns:
            hard negative ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•´ ìœ ì‚¬ë„ ê³„ì‚°
        # ì—¬ê¸°ì„œëŠ” ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ì²´ (ì‹¤ì œë¡œëŠ” ì„ë² ë”© ìœ ì‚¬ë„ ì‚¬ìš©)
        hard_negatives = np.random.choice(self.non_retro_indices, size=k, replace=False)
        return hard_negatives.tolist()


def create_balanced_dataloader(dataset, 
                             batch_size: int = 16,
                             oversample_minority: bool = True,
                             min_samples_per_class: int = 2,
                             num_workers: int = 0) -> torch.utils.data.DataLoader:
    """
    í´ë˜ìŠ¤ ê· í˜• DataLoader ìƒì„±
    
    Args:
        dataset: FashionDataset
        batch_size: ë°°ì¹˜ í¬ê¸°
        oversample_minority: ì†Œìˆ˜ í´ë˜ìŠ¤ ì˜¤ë²„ìƒ˜í”Œë§ ì—¬ë¶€
        min_samples_per_class: ë°°ì¹˜ë‹¹ í´ë˜ìŠ¤ë³„ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        num_workers: ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜
        
    Returns:
        í´ë˜ìŠ¤ ê· í˜•ì´ ë§ì¶°ì§„ DataLoader
    """
    sampler = ClassBalancedSampler(
        dataset=dataset,
        batch_size=batch_size,
        oversample_minority=oversample_minority,
        min_samples_per_class=min_samples_per_class
    )
    
    from .fashion_dataset import collate_fashion_batch
    
    return torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        collate_fn=collate_fashion_batch,
        num_workers=num_workers,
        pin_memory=True
    )