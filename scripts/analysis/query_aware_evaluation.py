#!/usr/bin/env python3
"""
Query-Aware Evaluation System

êµìˆ˜ë‹˜ ì‹œë‚˜ë¦¬ì˜¤ì— ë§ì¶˜ í‰ê°€ ì‹œìŠ¤í…œ:
- ë°©í–¥ A: í‰ê°€ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ë¦¬ (All queries vs Best-seller queries)
- ë°©í–¥ B: Query-aware Evaluation (íŒë§¤ëŸ‰/í’ˆì§ˆ/ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§)

ëª©í‘œ:
- All queries â†’ Recall@10 â‰ˆ 75~80%
- Best-seller queries â†’ Recall@10 â‰ˆ 85~92%
"""

import sys
import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any
import torch
from torch.utils.data import DataLoader

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from main import FashionEncoderSystem
from utils.config import TrainingConfig


class QueryAwareEvaluator:
    """Query-aware í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self, system: FashionEncoderSystem):
        self.system = system
        self.data_module = system.data_module
        self.trainer = system.trainer
        
        # í‰ê°€ ê²°ê³¼ ì €ì¥
        self.evaluation_results = {}
        
    def analyze_dataset_quality(self) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ í’ˆì§ˆ ë¶„ì„ ë° í•„í„°ë§ ê¸°ì¤€ ì„¤ì •"""
        print("ğŸ“Š ë°ì´í„°ì…‹ í’ˆì§ˆ ë¶„ì„ ì¤‘...")
        
        # í•™ìŠµ ë°ì´í„°ì…‹ì—ì„œ í’ˆì§ˆ ì§€í‘œ ì¶”ì¶œ
        train_dataset = self.data_module.train_dataset
        fashion_items = train_dataset.fashion_items
        
        # 1. ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ ë¶„ì„
        category_counts = {}
        for item in fashion_items:
            category = item.category
            category_counts[category] = category_counts.get(category, 0) + 1
        
        total_items = len(fashion_items)
        category_distribution = {
            cat: {"count": count, "percentage": count/total_items*100}
            for cat, count in category_counts.items()
        }
        
        # 2. ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ ë¶„ì„
        metadata_completeness = {
            'style': 0,
            'material': 0,
            'detail': 0,
            'silhouette': 0
        }
        
        for item in fashion_items:
            if item.style and len(item.style) > 0:
                metadata_completeness['style'] += 1
            if item.material and len(item.material) > 0:
                metadata_completeness['material'] += 1
            if item.detail and len(item.detail) > 0:
                metadata_completeness['detail'] += 1
            if item.silhouette:
                metadata_completeness['silhouette'] += 1
        
        # ì™„ì„±ë„ë¥¼ í¼ì„¼íŠ¸ë¡œ ë³€í™˜
        for field in metadata_completeness:
            metadata_completeness[field] = metadata_completeness[field] / total_items * 100
        
        # 3. í’ˆì§ˆ ê¸°ì¤€ ì„¤ì • (ì‹œë®¬ë ˆì´ì…˜)
        # ì‹¤ì œë¡œëŠ” íŒë§¤ëŸ‰, ì´ë¯¸ì§€ í’ˆì§ˆ ë“±ì˜ ë°ì´í„°ê°€ í•„ìš”í•˜ì§€ë§Œ
        # ì—¬ê¸°ì„œëŠ” ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜
        quality_scores = []
        for item in fashion_items:
            score = 0
            # ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ ê¸°ë°˜ ì ìˆ˜
            if item.style and len(item.style) > 0:
                score += 25
            if item.material and len(item.material) > 0:
                score += 25
            if item.detail and len(item.detail) > 0:
                score += 25
            if item.silhouette:
                score += 25
            quality_scores.append(score)
        
        quality_scores = np.array(quality_scores)
        
        # í’ˆì§ˆ ë¶„í¬ ë¶„ì„
        quality_analysis = {
            'mean_score': float(quality_scores.mean()),
            'std_score': float(quality_scores.std()),
            'min_score': float(quality_scores.min()),
            'max_score': float(quality_scores.max()),
            'high_quality_threshold': float(np.percentile(quality_scores, 80)),  # ìƒìœ„ 20%
            'best_seller_threshold': float(np.percentile(quality_scores, 90))   # ìƒìœ„ 10%
        }
        
        analysis_results = {
            'total_items': total_items,
            'category_distribution': category_distribution,
            'metadata_completeness': metadata_completeness,
            'quality_analysis': quality_analysis,
            'quality_scores': quality_scores.tolist()
        }
        
        print(f"âœ… ë°ì´í„°ì…‹ ë¶„ì„ ì™„ë£Œ:")
        print(f"   ì´ ì•„ì´í…œ: {total_items}")
        print(f"   ì¹´í…Œê³ ë¦¬ ë¶„í¬: {category_distribution}")
        print(f"   ë©”íƒ€ë°ì´í„° ì™„ì„±ë„: {metadata_completeness}")
        print(f"   í’ˆì§ˆ ì ìˆ˜ í‰ê· : {quality_analysis['mean_score']:.1f}")
        print(f"   Best-seller ì„ê³„ê°’: {quality_analysis['best_seller_threshold']:.1f}")
        
        return analysis_results
    
    def create_query_subsets(self, quality_analysis: Dict[str, Any]) -> Dict[str, List[int]]:
        """ì¿¼ë¦¬ ì„œë¸Œì…‹ ìƒì„±"""
        print("ğŸ¯ ì¿¼ë¦¬ ì„œë¸Œì…‹ ìƒì„± ì¤‘...")
        
        fashion_items = self.data_module.train_dataset.fashion_items
        quality_scores = np.array(quality_analysis['quality_scores'])
        
        # 1. All queries (ì „ì²´ ë°ì´í„°)
        all_indices = list(range(len(fashion_items)))
        
        # 2. High-quality queries (ìƒìœ„ 20%)
        high_quality_threshold = quality_analysis['quality_analysis']['high_quality_threshold']
        high_quality_indices = [i for i, score in enumerate(quality_scores) 
                               if score >= high_quality_threshold]
        
        # 3. Best-seller queries (ìƒìœ„ 10%)
        best_seller_threshold = quality_analysis['quality_analysis']['best_seller_threshold']
        best_seller_indices = [i for i, score in enumerate(quality_scores) 
                              if score >= best_seller_threshold]
        
        # 4. Category-balanced subset (ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ê· ë“±í•˜ê²Œ)
        category_indices = {}
        for i, item in enumerate(fashion_items):
            category = item.category
            if category not in category_indices:
                category_indices[category] = []
            category_indices[category].append(i)
        
        # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ìµœì†Œ ê°œìˆ˜ë§Œí¼ ì„ íƒ
        min_category_size = min(len(indices) for indices in category_indices.values())
        balanced_indices = []
        for category, indices in category_indices.items():
            # í’ˆì§ˆ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ì„ íƒ
            category_scores = [(i, quality_scores[i]) for i in indices]
            category_scores.sort(key=lambda x: x[1], reverse=True)
            selected = [i for i, _ in category_scores[:min_category_size//2]]  # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ì ˆë°˜
            balanced_indices.extend(selected)
        
        query_subsets = {
            'all_queries': all_indices,
            'high_quality': high_quality_indices,
            'best_seller': best_seller_indices,
            'category_balanced': balanced_indices
        }
        
        print(f"âœ… ì¿¼ë¦¬ ì„œë¸Œì…‹ ìƒì„± ì™„ë£Œ:")
        for name, indices in query_subsets.items():
            print(f"   {name}: {len(indices)}ê°œ ({len(indices)/len(all_indices)*100:.1f}%)")
        
        return query_subsets
    
    def evaluate_on_subset(self, subset_name: str, query_indices: List[int]) -> Dict[str, float]:
        """íŠ¹ì • ì„œë¸Œì…‹ì— ëŒ€í•œ í‰ê°€ ìˆ˜í–‰"""
        print(f"ğŸ” {subset_name} í‰ê°€ ì¤‘... ({len(query_indices)}ê°œ ì¿¼ë¦¬)")
        
        # ì„œë¸Œì…‹ ë°ì´í„°ë¡œë” ìƒì„±
        subset_dataset = self._create_subset_dataset(query_indices)
        
        from data.fashion_dataset import collate_fashion_batch
        subset_loader = DataLoader(
            subset_dataset,
            batch_size=self.system.config.batch_size,
            shuffle=False,
            num_workers=0,
            collate_fn=collate_fashion_batch
        )
        
        # í‰ê°€ ìˆ˜í–‰
        self.trainer.contrastive_learner.eval()
        
        all_similarities = []
        with torch.no_grad():
            for batch in subset_loader:
                batch = self.trainer._move_batch_to_device(batch)
                json_batch = self.trainer._convert_batch_to_dict(batch)
                
                embeddings = self.trainer.contrastive_learner.get_embeddings(batch.images, json_batch)
                similarities = embeddings['similarity_matrix']
                all_similarities.append(similarities.cpu())
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        if all_similarities:
            batch_metrics = []
            for similarities in all_similarities:
                batch_metric = self._compute_enhanced_metrics(similarities)
                batch_metrics.append(batch_metric)
            
            # í‰ê·  ê³„ì‚°
            metrics = {}
            if batch_metrics:
                # ëª¨ë“  ë°°ì¹˜ì—ì„œ ê³µí†µìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” í‚¤ë§Œ ì‚¬ìš©
                common_keys = set(batch_metrics[0].keys())
                for batch_metric in batch_metrics[1:]:
                    common_keys &= set(batch_metric.keys())
                
                for key in common_keys:
                    metrics[key] = sum(m[key] for m in batch_metrics) / len(batch_metrics)
                
                # ëˆ„ë½ëœ í‚¤ë“¤ì€ 0ìœ¼ë¡œ ì„¤ì •
                for key in ['recall_at_3', 'recall_at_5', 'recall_at_10', 'recall_at_20']:
                    if key not in metrics:
                        metrics[key] = 0.0
        else:
            metrics = self._get_empty_metrics()
        
        print(f"âœ… {subset_name} í‰ê°€ ì™„ë£Œ:")
        print(f"   Recall@5: {metrics.get('recall_at_5', 0)*100:.1f}%")
        print(f"   Recall@10: {metrics.get('recall_at_10', 0)*100:.1f}%")
        print(f"   Top-1: {metrics.get('top1_accuracy', 0)*100:.1f}%")
        print(f"   MRR: {metrics.get('mean_reciprocal_rank', 0):.3f}")
        
        return metrics
    
    def _create_subset_dataset(self, indices: List[int]):
        """ì¸ë±ìŠ¤ ê¸°ë°˜ ì„œë¸Œì…‹ ë°ì´í„°ì…‹ ìƒì„±"""
        from torch.utils.data import Subset
        return Subset(self.data_module.train_dataset, indices)
    
    def _compute_enhanced_metrics(self, similarity_matrix: torch.Tensor) -> Dict[str, float]:
        """í–¥ìƒëœ ë©”íŠ¸ë¦­ ê³„ì‚° (Recall@K í¬í•¨)"""
        batch_size = similarity_matrix.size(0)
        
        # Top-1 accuracy
        top1_correct = (similarity_matrix.argmax(dim=1) == torch.arange(batch_size)).float().mean()
        
        # Top-K accuracy
        metrics = {'top1_accuracy': top1_correct.item()}
        
        for k in [3, 5, 10, 20]:
            if k <= batch_size:
                topk_indices = similarity_matrix.topk(k=k, dim=1)[1]
                topk_correct = (topk_indices == torch.arange(batch_size).unsqueeze(1)).any(dim=1).float().mean()
                metrics[f'recall_at_{k}'] = topk_correct.item()
                metrics[f'top{k}_accuracy'] = topk_correct.item()
        
        # Mean reciprocal rank
        ranks = (similarity_matrix.argsort(dim=1, descending=True) == torch.arange(batch_size).unsqueeze(1)).nonzero()[:, 1] + 1
        mrr = (1.0 / ranks.float()).mean()
        metrics['mean_reciprocal_rank'] = mrr.item()
        
        # ì¶”ê°€ ë©”íŠ¸ë¦­
        metrics['avg_positive_similarity'] = similarity_matrix.diag().mean().item()
        
        # Negative similarity (off-diagonal elements)
        mask = torch.eye(batch_size, dtype=torch.bool)
        negative_similarities = similarity_matrix[~mask]
        if len(negative_similarities) > 0:
            metrics['avg_negative_similarity'] = negative_similarities.mean().item()
        else:
            metrics['avg_negative_similarity'] = 0.0
        
        return metrics
    
    def _get_empty_metrics(self) -> Dict[str, float]:
        """ë¹ˆ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return {
            'top1_accuracy': 0.0,
            'recall_at_3': 0.0,
            'recall_at_5': 0.0,
            'recall_at_10': 0.0,
            'recall_at_20': 0.0,
            'top3_accuracy': 0.0,
            'top5_accuracy': 0.0,
            'top10_accuracy': 0.0,
            'top20_accuracy': 0.0,
            'mean_reciprocal_rank': 0.0,
            'avg_positive_similarity': 0.0,
            'avg_negative_similarity': 0.0
        }
    
    def run_comprehensive_evaluation(self) -> Dict[str, Any]:
        """í¬ê´„ì ì¸ í‰ê°€ ì‹¤í–‰"""
        print("ğŸš€ Query-Aware í¬ê´„ì  í‰ê°€ ì‹œì‘")
        print("=" * 60)
        
        # 1. ë°ì´í„°ì…‹ í’ˆì§ˆ ë¶„ì„
        quality_analysis = self.analyze_dataset_quality()
        
        # 2. ì¿¼ë¦¬ ì„œë¸Œì…‹ ìƒì„±
        query_subsets = self.create_query_subsets(quality_analysis)
        
        # 3. ê° ì„œë¸Œì…‹ì— ëŒ€í•œ í‰ê°€
        evaluation_results = {}
        
        for subset_name, query_indices in query_subsets.items():
            print(f"\n{'='*40}")
            print(f"í‰ê°€ ì¤‘: {subset_name}")
            print(f"{'='*40}")
            
            metrics = self.evaluate_on_subset(subset_name, query_indices)
            evaluation_results[subset_name] = {
                'metrics': metrics,
                'query_count': len(query_indices),
                'percentage': len(query_indices) / len(query_subsets['all_queries']) * 100
            }
        
        # 4. ê²°ê³¼ ì¢…í•©
        comprehensive_results = {
            'timestamp': datetime.now().isoformat(),
            'dataset_analysis': quality_analysis,
            'query_subsets': {name: len(indices) for name, indices in query_subsets.items()},
            'evaluation_results': evaluation_results,
            'summary': self._create_evaluation_summary(evaluation_results)
        }
        
        # 5. ê²°ê³¼ ì¶œë ¥
        self._print_evaluation_summary(comprehensive_results)
        
        return comprehensive_results
    
    def _create_evaluation_summary(self, evaluation_results: Dict[str, Any]) -> Dict[str, Any]:
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        summary = {}
        
        # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ
        for subset_name, result in evaluation_results.items():
            metrics = result['metrics']
            summary[subset_name] = {
                'recall_at_5': metrics.get('recall_at_5', 0) * 100,
                'recall_at_10': metrics.get('recall_at_10', 0) * 100,
                'top1_accuracy': metrics.get('top1_accuracy', 0) * 100,
                'mrr': metrics.get('mean_reciprocal_rank', 0),
                'query_count': result['query_count']
            }
        
        # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
        all_recall_10 = summary.get('all_queries', {}).get('recall_at_10', 0)
        best_seller_recall_10 = summary.get('best_seller', {}).get('recall_at_10', 0)
        
        summary['goal_achievement'] = {
            'all_queries_target': '75-80%',
            'all_queries_actual': f"{all_recall_10:.1f}%",
            'all_queries_achieved': 75 <= all_recall_10 <= 80,
            
            'best_seller_target': '85-92%',
            'best_seller_actual': f"{best_seller_recall_10:.1f}%",
            'best_seller_achieved': 85 <= best_seller_recall_10 <= 92
        }
        
        return summary
    
    def _print_evaluation_summary(self, results: Dict[str, Any]):
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print("ğŸ“Š Query-Aware í‰ê°€ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*60}")
        
        summary = results['summary']
        
        print(f"\nğŸ¯ ëª©í‘œ ë‹¬ì„± í˜„í™©:")
        goal = summary['goal_achievement']
        print(f"   All Queries Recall@10:")
        print(f"     ëª©í‘œ: {goal['all_queries_target']}")
        print(f"     ì‹¤ì œ: {goal['all_queries_actual']}")
        print(f"     ë‹¬ì„±: {'âœ…' if goal['all_queries_achieved'] else 'âŒ'}")
        
        print(f"   Best-seller Queries Recall@10:")
        print(f"     ëª©í‘œ: {goal['best_seller_target']}")
        print(f"     ì‹¤ì œ: {goal['best_seller_actual']}")
        print(f"     ë‹¬ì„±: {'âœ…' if goal['best_seller_achieved'] else 'âŒ'}")
        
        print(f"\nğŸ“ˆ ìƒì„¸ ê²°ê³¼:")
        for subset_name, metrics in summary.items():
            if subset_name == 'goal_achievement':
                continue
            
            print(f"   {subset_name}:")
            print(f"     ì¿¼ë¦¬ ìˆ˜: {metrics['query_count']}")
            print(f"     Recall@5: {metrics['recall_at_5']:.1f}%")
            print(f"     Recall@10: {metrics['recall_at_10']:.1f}%")
            print(f"     Top-1: {metrics['top1_accuracy']:.1f}%")
            print(f"     MRR: {metrics['mrr']:.3f}")
        
        print(f"\nğŸ’¡ ì¸ì‚¬ì´íŠ¸:")
        all_r10 = summary.get('all_queries', {}).get('recall_at_10', 0)
        best_r10 = summary.get('best_seller', {}).get('recall_at_10', 0)
        improvement = best_r10 - all_r10
        
        print(f"   Best-seller ì¿¼ë¦¬ëŠ” ì „ì²´ ëŒ€ë¹„ {improvement:.1f}%p ë†’ì€ ì„±ëŠ¥")
        print(f"   Query-aware í‰ê°€ë¡œ ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ë°˜ì˜")
        
        if goal['all_queries_achieved'] and goal['best_seller_achieved']:
            print(f"   ğŸ‰ ëª¨ë“  ëª©í‘œ ë‹¬ì„±! êµìˆ˜ë‹˜ ì‹œë‚˜ë¦¬ì˜¤ ì™„ë²½ ëŒ€ì‘")
        elif goal['best_seller_achieved']:
            print(f"   âœ… Best-seller ëª©í‘œ ë‹¬ì„±! í•µì‹¬ ì‹œë‚˜ë¦¬ì˜¤ ì„±ê³µ")
        else:
            print(f"   ğŸ“ˆ ì¶”ê°€ íŠœë‹ìœ¼ë¡œ ëª©í‘œ ë‹¬ì„± ê°€ëŠ¥")


def run_query_aware_evaluation():
    """Query-aware í‰ê°€ ì‹¤í–‰"""
    print("ğŸ¯ Query-Aware Evaluation System")
    print("=" * 60)
    print("êµìˆ˜ë‹˜ ì‹œë‚˜ë¦¬ì˜¤ ë§ì¶¤ í‰ê°€:")
    print("- ë°©í–¥ A: í‰ê°€ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ë¦¬")
    print("- ë°©í–¥ B: Query-aware Evaluation")
    print("=" * 60)
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ
    dataset_path = "C:/sample/ë¼ë²¨ë§ë°ì´í„°"
    
    # ê¸°ì¡´ ëª¨ë¸ ì„¤ì • (Baseline v1)
    config = TrainingConfig()
    config.temperature = 0.1
    config.batch_size = 16
    config.max_epochs = 8
    
    try:
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        system = FashionEncoderSystem()
        system.config = config
        
        # ë°ì´í„° ì„¤ì •
        print("ğŸ“ ë°ì´í„° ì„¤ì • ì¤‘...")
        system.setup_data(dataset_path)
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        print("ğŸ‹ï¸ íŠ¸ë ˆì´ë„ˆ ì„¤ì • ì¤‘...")
        system.setup_trainer()
        
        # ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ìˆë‹¤ë©´)
        checkpoint_path = "checkpoints/best_model.pt"
        if Path(checkpoint_path).exists():
            print(f"ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
            system.trainer.load_checkpoint(checkpoint_path)
        else:
            print("âš ï¸ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ëª¨ë¸ ìƒíƒœë¡œ í‰ê°€í•©ë‹ˆë‹¤.")
        
        # Query-aware í‰ê°€ ì‹¤í–‰
        evaluator = QueryAwareEvaluator(system)
        results = evaluator.run_comprehensive_evaluation()
        
        # ê²°ê³¼ ì €ì¥
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        results_file = results_dir / "query_aware_evaluation_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {results_file}")
        
        # ì •ë¦¬
        system.cleanup()
        
        print(f"\nâœ¨ Query-Aware í‰ê°€ ì™„ë£Œ!")
        
        return results
        
    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    run_query_aware_evaluation()