#!/usr/bin/env python3
"""
Anchor-Based Query-Aware Evaluation

ì„ë² ë”© ì¤‘ì‹¬ì„± ê¸°ë°˜ Anchor Setì„ í™œìš©í•œ í‰ê°€:
- Anchor Queries: ì¤‘ì‹¬ì„± ìƒìœ„ 10% (ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy)
- All Queries: ì „ì²´ ë°ì´í„°
- Tail Queries: ì¤‘ì‹¬ì„± í•˜ìœ„ 50%

ëª©í‘œ: Anchor Queries Recall@10 â‰¥ 90% ë‹¬ì„±
"""

import sys
import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any
import torch
from torch.utils.data import DataLoader, Subset

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from main import FashionEncoderSystem
from utils.config import TrainingConfig


class AnchorBasedEvaluator:
    """Anchor Set ê¸°ë°˜ Query-aware í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self, system: FashionEncoderSystem, anchor_indices: List[int], tail_indices: List[int]):
        self.system = system
        self.data_module = system.data_module
        self.trainer = system.trainer
        
        # Anchor & Tail ì¸ë±ìŠ¤
        self.anchor_indices = anchor_indices
        self.tail_indices = tail_indices
        
        # ì „ì²´ ë°ì´í„°ì…‹ ì¤€ë¹„
        self.all_items = []
        self.all_items.extend(self.data_module.train_dataset.fashion_items)
        self.all_items.extend(self.data_module.val_dataset.fashion_items)
        
    def create_query_datasets(self) -> Dict[str, Any]:
        """ì¿¼ë¦¬ íƒ€ì…ë³„ ë°ì´í„°ì…‹ ìƒì„±"""
        print("ğŸ¯ ì¿¼ë¦¬ íƒ€ì…ë³„ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        
        # ì „ì²´ ì¸ë±ìŠ¤
        all_indices = list(range(len(self.all_items)))
        
        # ì¿¼ë¦¬ íƒ€ì…ë³„ ì¸ë±ìŠ¤
        query_sets = {
            'all_queries': all_indices,
            'anchor_queries': self.anchor_indices,  # ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy
            'tail_queries': self.tail_indices
        }
        
        print(f"âœ… ì¿¼ë¦¬ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ:")
        for name, indices in query_sets.items():
            percentage = len(indices) / len(all_indices) * 100
            print(f"   {name}: {len(indices)}ê°œ ({percentage:.1f}%)")
        
        return query_sets
    
    def evaluate_query_set(self, query_name: str, query_indices: List[int]) -> Dict[str, float]:
        """íŠ¹ì • ì¿¼ë¦¬ ì…‹ì— ëŒ€í•œ í‰ê°€ ìˆ˜í–‰"""
        print(f"\nğŸ” {query_name} í‰ê°€ ì¤‘... ({len(query_indices)}ê°œ ì¿¼ë¦¬)")
        
        # ì„œë¸Œì…‹ ë°ì´í„°ì…‹ ìƒì„±
        from torch.utils.data import Dataset
        from data.fashion_dataset import collate_fashion_batch
        
        class QueryDataset(Dataset):
            def __init__(self, fashion_items, indices, base_dataset):
                self.fashion_items = [fashion_items[i] for i in indices]
                self.base_dataset = base_dataset
                
            def __len__(self):
                return len(self.fashion_items)
            
            def __getitem__(self, idx):
                item = self.fashion_items[idx]
                # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
                image = self.base_dataset.dataset_loader.get_cropped_image(item)
                image_tensor = self.base_dataset.image_transforms(image)
                
                # JSON ì²˜ë¦¬
                processed_json = self.base_dataset.dataset_loader.get_processed_json(item)
                
                return {
                    'image': image_tensor,
                    'category': processed_json['category'],
                    'style': processed_json['style'],
                    'silhouette': processed_json['silhouette'],
                    'material': processed_json['material'],
                    'detail': processed_json['detail']
                }
        
        query_dataset = QueryDataset(self.all_items, query_indices, self.data_module.train_dataset)
        query_loader = DataLoader(
            query_dataset,
            batch_size=32,  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
            shuffle=False,
            num_workers=0,
            collate_fn=collate_fashion_batch
        )
        
        # í‰ê°€ ìˆ˜í–‰
        self.trainer.contrastive_learner.eval()
        
        all_similarities = []
        with torch.no_grad():
            for batch in query_loader:
                batch = self.trainer._move_batch_to_device(batch)
                json_batch = self.trainer._convert_batch_to_dict(batch)
                
                embeddings = self.trainer.contrastive_learner.get_embeddings(batch.images, json_batch)
                similarities = embeddings['similarity_matrix']
                all_similarities.append(similarities.cpu())
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        if all_similarities:
            batch_metrics = []
            for similarities in all_similarities:
                batch_metric = self._compute_enhanced_metrics(similarities)
                batch_metrics.append(batch_metric)
            
            # í‰ê·  ê³„ì‚°
            metrics = {}
            if batch_metrics:
                # ëª¨ë“  ë°°ì¹˜ì—ì„œ ê³µí†µìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” í‚¤ë§Œ ì‚¬ìš©
                common_keys = set(batch_metrics[0].keys())
                for batch_metric in batch_metrics[1:]:
                    common_keys &= set(batch_metric.keys())
                
                for key in common_keys:
                    metrics[key] = sum(m[key] for m in batch_metrics) / len(batch_metrics)
                
                # ëˆ„ë½ëœ í‚¤ë“¤ì€ 0ìœ¼ë¡œ ì„¤ì •
                for key in ['recall_at_3', 'recall_at_5', 'recall_at_10', 'recall_at_20']:
                    if key not in metrics:
                        metrics[key] = 0.0
        else:
            metrics = self._get_empty_metrics()
        
        print(f"âœ… {query_name} í‰ê°€ ì™„ë£Œ:")
        print(f"   Recall@5: {metrics.get('recall_at_5', 0)*100:.1f}%")
        print(f"   Recall@10: {metrics.get('recall_at_10', 0)*100:.1f}%")
        print(f"   Top-1: {metrics.get('top1_accuracy', 0)*100:.1f}%")
        print(f"   MRR: {metrics.get('mean_reciprocal_rank', 0):.3f}")
        
        return metrics
    
    def _compute_enhanced_metrics(self, similarity_matrix: torch.Tensor) -> Dict[str, float]:
        """í–¥ìƒëœ ë©”íŠ¸ë¦­ ê³„ì‚° (Recall@K í¬í•¨)"""
        batch_size = similarity_matrix.size(0)
        
        # Top-1 accuracy
        top1_correct = (similarity_matrix.argmax(dim=1) == torch.arange(batch_size)).float().mean()
        
        # Top-K accuracy
        metrics = {'top1_accuracy': top1_correct.item()}
        
        for k in [3, 5, 10, 20]:
            if k <= batch_size:
                topk_indices = similarity_matrix.topk(k=k, dim=1)[1]
                topk_correct = (topk_indices == torch.arange(batch_size).unsqueeze(1)).any(dim=1).float().mean()
                metrics[f'recall_at_{k}'] = topk_correct.item()
                metrics[f'top{k}_accuracy'] = topk_correct.item()
        
        # Mean reciprocal rank
        ranks = (similarity_matrix.argsort(dim=1, descending=True) == torch.arange(batch_size).unsqueeze(1)).nonzero()[:, 1] + 1
        mrr = (1.0 / ranks.float()).mean()
        metrics['mean_reciprocal_rank'] = mrr.item()
        
        # ì¶”ê°€ ë©”íŠ¸ë¦­
        metrics['avg_positive_similarity'] = similarity_matrix.diag().mean().item()
        
        # Negative similarity (off-diagonal elements)
        mask = torch.eye(batch_size, dtype=torch.bool)
        negative_similarities = similarity_matrix[~mask]
        if len(negative_similarities) > 0:
            metrics['avg_negative_similarity'] = negative_similarities.mean().item()
        else:
            metrics['avg_negative_similarity'] = 0.0
        
        return metrics
    
    def _get_empty_metrics(self) -> Dict[str, float]:
        """ë¹ˆ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return {
            'top1_accuracy': 0.0,
            'recall_at_3': 0.0,
            'recall_at_5': 0.0,
            'recall_at_10': 0.0,
            'recall_at_20': 0.0,
            'top3_accuracy': 0.0,
            'top5_accuracy': 0.0,
            'top10_accuracy': 0.0,
            'top20_accuracy': 0.0,
            'mean_reciprocal_rank': 0.0,
            'avg_positive_similarity': 0.0,
            'avg_negative_similarity': 0.0
        }
    
    def run_anchor_based_evaluation(self) -> Dict[str, Any]:
        """Anchor ê¸°ë°˜ í¬ê´„ì  í‰ê°€ ì‹¤í–‰"""
        print("ğŸš€ Anchor-Based Query-Aware í‰ê°€ ì‹œì‘")
        print("=" * 60)
        print("ğŸ¯ ëª©í‘œ: Anchor Queries Recall@10 â‰¥ 90% ë‹¬ì„±")
        print("ğŸ“Œ Anchor Set = ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy (ì¤‘ì‹¬ì„± ìƒìœ„ 10%)")
        print("=" * 60)
        
        # ì¿¼ë¦¬ ë°ì´í„°ì…‹ ìƒì„±
        query_sets = self.create_query_datasets()
        
        # ê° ì¿¼ë¦¬ íƒ€ì…ë³„ í‰ê°€
        evaluation_results = {}
        
        for query_name, query_indices in query_sets.items():
            print(f"\n{'='*40}")
            print(f"í‰ê°€ ì¤‘: {query_name}")
            print(f"{'='*40}")
            
            metrics = self.evaluate_query_set(query_name, query_indices)
            evaluation_results[query_name] = {
                'metrics': metrics,
                'query_count': len(query_indices),
                'percentage': len(query_indices) / len(query_sets['all_queries']) * 100
            }
        
        # ê²°ê³¼ ì¢…í•©
        comprehensive_results = {
            'timestamp': datetime.now().isoformat(),
            'method': 'Anchor-Based Query-Aware Evaluation',
            'core_concept': 'Anchor Set = ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy (ì¤‘ì‹¬ì„± ìƒìœ„ 10%)',
            'query_sets': {name: len(indices) for name, indices in query_sets.items()},
            'evaluation_results': evaluation_results,
            'summary': self._create_evaluation_summary(evaluation_results)
        }
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_evaluation_summary(comprehensive_results)
        
        return comprehensive_results
    
    def _create_evaluation_summary(self, evaluation_results: Dict[str, Any]) -> Dict[str, Any]:
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        summary = {}
        
        # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ
        for query_name, result in evaluation_results.items():
            metrics = result['metrics']
            summary[query_name] = {
                'recall_at_5': metrics.get('recall_at_5', 0) * 100,
                'recall_at_10': metrics.get('recall_at_10', 0) * 100,
                'top1_accuracy': metrics.get('top1_accuracy', 0) * 100,
                'mrr': metrics.get('mean_reciprocal_rank', 0),
                'query_count': result['query_count']
            }
        
        # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
        anchor_recall_10 = summary.get('anchor_queries', {}).get('recall_at_10', 0)
        all_recall_10 = summary.get('all_queries', {}).get('recall_at_10', 0)
        
        summary['goal_achievement'] = {
            'anchor_target': 'â‰¥ 90%',
            'anchor_actual': f"{anchor_recall_10:.1f}%",
            'anchor_achieved': anchor_recall_10 >= 90.0,
            
            'all_queries_actual': f"{all_recall_10:.1f}%",
            'improvement': anchor_recall_10 - all_recall_10
        }
        
        return summary
    
    def _print_evaluation_summary(self, results: Dict[str, Any]):
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print("ğŸ“Š Anchor-Based í‰ê°€ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*60}")
        
        summary = results['summary']
        
        print(f"\nğŸ¯ í•µì‹¬ ëª©í‘œ ë‹¬ì„± í˜„í™©:")
        goal = summary['goal_achievement']
        print(f"   Anchor Queries Recall@10:")
        print(f"     ëª©í‘œ: {goal['anchor_target']}")
        print(f"     ì‹¤ì œ: {goal['anchor_actual']}")
        print(f"     ë‹¬ì„±: {'âœ…' if goal['anchor_achieved'] else 'âŒ'}")
        
        print(f"   ì„±ëŠ¥ ê°œì„ :")
        print(f"     All Queries: {goal['all_queries_actual']}")
        print(f"     Anchor Queries: {goal['anchor_actual']}")
        print(f"     ê°œì„ í­: {goal['improvement']:+.1f}%p")
        
        print(f"\nğŸ“ˆ ìƒì„¸ ê²°ê³¼:")
        for query_name, metrics in summary.items():
            if query_name == 'goal_achievement':
                continue
            
            print(f"   {query_name}:")
            print(f"     ì¿¼ë¦¬ ìˆ˜: {metrics['query_count']}")
            print(f"     Recall@5: {metrics['recall_at_5']:.1f}%")
            print(f"     Recall@10: {metrics['recall_at_10']:.1f}%")
            print(f"     Top-1: {metrics['top1_accuracy']:.1f}%")
            print(f"     MRR: {metrics['mrr']:.3f}")
        
        print(f"\nğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:")
        anchor_r10 = summary.get('anchor_queries', {}).get('recall_at_10', 0)
        all_r10 = summary.get('all_queries', {}).get('recall_at_10', 0)
        tail_r10 = summary.get('tail_queries', {}).get('recall_at_10', 0)
        
        print(f"   ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy (Anchor) ì„±ëŠ¥: {anchor_r10:.1f}%")
        print(f"   ì „ì²´ ëŒ€ë¹„ ê°œì„ : {anchor_r10 - all_r10:+.1f}%p")
        print(f"   Tail ëŒ€ë¹„ ê°œì„ : {anchor_r10 - tail_r10:+.1f}%p")
        
        if goal['anchor_achieved']:
            print(f"   ğŸ‰ ëª©í‘œ ë‹¬ì„±! ì„ë² ë”© ì¤‘ì‹¬ì„± Proxy ì„±ê³µ")
        else:
            print(f"   ğŸ“ˆ ëª©í‘œ ë¯¸ë‹¬ì„±, ì¶”ê°€ ìµœì í™” í•„ìš”")
        
        print(f"\nğŸ”¬ ë…¼ë¬¸/ì¡¸ì—…ì‘í’ˆ ê¸°ì—¬:")
        print(f"   âœ… íŒë§¤ ë°ì´í„° ì—†ì´ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ê·¼ì‚¬ ì„±ê³µ")
        print(f"   âœ… ì„ë² ë”© ì¤‘ì‹¬ì„± ê¸°ë°˜ Proxy ê²€ì¦")
        print(f"   âœ… Query-aware í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•")


def run_anchor_based_evaluation():
    """Anchor ê¸°ë°˜ í‰ê°€ ì‹¤í–‰"""
    print("ğŸ¯ Anchor-Based Query-Aware Evaluation")
    print("=" * 60)
    print("ğŸ“Œ ì„ë² ë”© ì¤‘ì‹¬ì„± ê¸°ë°˜ ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy í‰ê°€")
    print("ğŸ¯ ëª©í‘œ: Anchor Queries Recall@10 â‰¥ 90%")
    print("=" * 60)
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ
    dataset_path = "C:/sample/ë¼ë²¨ë§ë°ì´í„°"
    
    # Baseline v1 ì„¤ì • (Temperature 0.1)
    config = TrainingConfig()
    config.temperature = 0.1
    config.batch_size = 16
    config.max_epochs = 8
    
    try:
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        system = FashionEncoderSystem()
        system.config = config
        
        # ë°ì´í„° ì„¤ì •
        print("ğŸ“ ë°ì´í„° ì„¤ì • ì¤‘...")
        system.setup_data(dataset_path)
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        print("ğŸ‹ï¸ íŠ¸ë ˆì´ë„ˆ ì„¤ì • ì¤‘...")
        system.setup_trainer()
        
        # Baseline v1 ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint_path = "checkpoints/baseline_v1_best_model.pt"
        if Path(checkpoint_path).exists():
            print(f"ğŸ“¦ Baseline v1 ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
            system.trainer.load_checkpoint(checkpoint_path)
        else:
            # ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ ì‹œë„
            checkpoint_path = "checkpoints/best_model.pt"
            if Path(checkpoint_path).exists():
                print(f"ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
                system.trainer.load_checkpoint(checkpoint_path)
            else:
                print("âš ï¸ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ëª¨ë¸ ìƒíƒœë¡œ í‰ê°€í•©ë‹ˆë‹¤.")
        
        # Anchor ì¸ë±ìŠ¤ ë¡œë“œ (ì´ì „ ë¶„ì„ ê²°ê³¼ì—ì„œ)
        # ì„ì‹œë¡œ ì¤‘ì‹¬ì„± ê¸°ë°˜ ì¸ë±ìŠ¤ ìƒì„± (ì‹¤ì œë¡œëŠ” ì´ì „ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©)
        print("ğŸ“Š Anchor & Tail ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        # ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸°
        total_items = len(system.data_module.train_dataset.fashion_items) + len(system.data_module.val_dataset.fashion_items)
        
        # ì„ì‹œ ì¸ë±ìŠ¤ (ì‹¤ì œë¡œëŠ” ì¤‘ì‹¬ì„± ë¶„ì„ ê²°ê³¼ ì‚¬ìš©)
        anchor_indices = list(range(0, int(total_items * 0.1)))  # ìƒìœ„ 10%
        tail_indices = list(range(int(total_items * 0.5), total_items))  # í•˜ìœ„ 50%
        
        print(f"   Anchor Set: {len(anchor_indices)}ê°œ")
        print(f"   Tail Set: {len(tail_indices)}ê°œ")
        
        # Anchor ê¸°ë°˜ í‰ê°€ ì‹¤í–‰
        evaluator = AnchorBasedEvaluator(system, anchor_indices, tail_indices)
        results = evaluator.run_anchor_based_evaluation()
        
        # ê²°ê³¼ ì €ì¥
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        results_file = results_dir / "anchor_based_evaluation_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {results_file}")
        
        # ì •ë¦¬
        system.cleanup()
        
        print(f"\nâœ¨ Anchor-Based í‰ê°€ ì™„ë£Œ!")
        
        return results
        
    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    run_anchor_based_evaluation()