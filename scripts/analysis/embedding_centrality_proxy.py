#!/usr/bin/env python3
"""
ì„ë² ë”© ì¤‘ì‹¬ì„± ê¸°ë°˜ ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ì‹œìŠ¤í…œ

ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´:
"ë² ìŠ¤íŠ¸ì…€ëŸ¬ë¥¼ íŒë§¤ ë°ì´í„° ì—†ì´, ì„ë² ë”© ê³µê°„ì˜ ì¤‘ì‹¬ì„±ìœ¼ë¡œ ê·¼ì‚¬(proxy)í•œë‹¤."

ğŸ§  ê°œë… ì§ê´€:
ì„ë² ë”© ê³µê°„ì—ì„œ ë§ì€ ìƒí’ˆê³¼ ë¹„ìŠ·í•œ ë””ìì¸ â†’ íŠ¸ë Œë“œì„± ë””ìì¸ â†’ ì˜ íŒ”ë¦´ ê°€ëŠ¥ì„± â†‘
ì¦‰, "ì¤‘ì‹¬ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ëŒ€ì¤‘ì ì´ë‹¤"

ğŸ§± ì„¤ê³„ êµ¬ì¡°:
STEP 2-1: ì „ì²´ embedding ì¶”ì¶œ (ì´ë¯¸ì§€ë§Œ)
STEP 2-2: ê¸€ë¡œë²Œ ì¤‘ì‹¬ ë²¡í„° ê³„ì‚°
STEP 2-3: ì¤‘ì‹¬ì„± ì ìˆ˜ ê³„ì‚° (Cosine Similarity)
STEP 2-4: ìƒìœ„ 10% ì„ íƒ â†’ Anchor Set (ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy)
"""

import sys
import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any
import torch
from torch.utils.data import DataLoader
from sklearn.metrics.pairwise import cosine_similarity

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from main import FashionEncoderSystem
from utils.config import TrainingConfig


class EmbeddingCentralityProxy:
    """ì„ë² ë”© ì¤‘ì‹¬ì„± ê¸°ë°˜ ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ì‹œìŠ¤í…œ"""
    
    def __init__(self, system: FashionEncoderSystem):
        self.system = system
        self.data_module = system.data_module
        self.trainer = system.trainer
        
        # ì„ë² ë”© ë°ì´í„° ì €ì¥
        self.image_embeddings = None
        self.json_embeddings = None
        self.fashion_items = None
        
        # ì¤‘ì‹¬ì„± ë¶„ì„ ê²°ê³¼
        self.global_center = None
        self.centrality_scores = None
        self.anchor_indices = None
        self.tail_indices = None
        
    def extract_all_embeddings(self) -> Dict[str, Any]:
        """STEP 2-1: ì „ì²´ embedding ì¶”ì¶œ"""
        print("ğŸ” STEP 2-1: ì „ì²´ ì„ë² ë”© ì¶”ì¶œ ì¤‘...")
        print("   ëŒ€ìƒ: Train + Validation ì „ì²´")
        print("   ë°©ë²•: ì´ë¯¸ì§€ embeddingë§Œ ì‚¬ìš© (JSON X)")
        
        # ì „ì²´ ë°ì´í„°ì…‹ ì¤€ë¹„ (Train + Validation)
        all_items = []
        all_items.extend(self.data_module.train_dataset.fashion_items)
        all_items.extend(self.data_module.val_dataset.fashion_items)
        
        print(f"   ì´ ì•„ì´í…œ ìˆ˜: {len(all_items)}")
        
        # ì „ì²´ ë°ì´í„°ë¡œë” ìƒì„±
        from torch.utils.data import Dataset, DataLoader
        from data.fashion_dataset import collate_fashion_batch
        
        class FullDataset(Dataset):
            def __init__(self, fashion_items, base_dataset):
                self.fashion_items = fashion_items
                self.base_dataset = base_dataset
                
            def __len__(self):
                return len(self.fashion_items)
            
            def __getitem__(self, idx):
                item = self.fashion_items[idx]
                # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
                image = self.base_dataset.dataset_loader.get_cropped_image(item)
                image_tensor = self.base_dataset.image_transforms(image)
                
                # JSON ì²˜ë¦¬
                processed_json = self.base_dataset.dataset_loader.get_processed_json(item)
                
                return {
                    'image': image_tensor,
                    'category': processed_json['category'],
                    'style': processed_json['style'],
                    'silhouette': processed_json['silhouette'],
                    'material': processed_json['material'],
                    'detail': processed_json['detail']
                }
        
        full_dataset = FullDataset(all_items, self.data_module.train_dataset)
        full_loader = DataLoader(
            full_dataset,
            batch_size=32,  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
            shuffle=False,
            num_workers=0,
            collate_fn=collate_fashion_batch
        )
        
        # ì„ë² ë”© ì¶”ì¶œ
        self.trainer.contrastive_learner.eval()
        
        all_image_embeddings = []
        all_json_embeddings = []
        
        print(f"   ë°°ì¹˜ ìˆ˜: {len(full_loader)}")
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(full_loader):
                if batch_idx % 10 == 0:
                    print(f"   ì§„í–‰ë¥ : {batch_idx}/{len(full_loader)} ({batch_idx/len(full_loader)*100:.1f}%)")
                
                batch = self.trainer._move_batch_to_device(batch)
                json_batch = self.trainer._convert_batch_to_dict(batch)
                
                # ì„ë² ë”© ì¶”ì¶œ
                embeddings = self.trainer.contrastive_learner.get_embeddings(batch.images, json_batch)
                
                all_image_embeddings.append(embeddings['image_embeddings'].cpu())
                all_json_embeddings.append(embeddings['json_embeddings'].cpu())
        
        # ê²°í•©
        self.image_embeddings = torch.cat(all_image_embeddings, dim=0).numpy()
        self.json_embeddings = torch.cat(all_json_embeddings, dim=0).numpy()
        self.fashion_items = all_items
        
        print(f"âœ… ì„ë² ë”© ì¶”ì¶œ ì™„ë£Œ:")
        print(f"   ì´ë¯¸ì§€ ì„ë² ë”©: {self.image_embeddings.shape}")
        print(f"   JSON ì„ë² ë”©: {self.json_embeddings.shape}")
        print(f"   ì•„ì´í…œ ìˆ˜: {len(self.fashion_items)}")
        
        return {
            'image_embeddings': self.image_embeddings,
            'json_embeddings': self.json_embeddings,
            'num_items': len(self.fashion_items)
        }
    
    def compute_global_center(self) -> np.ndarray:
        """STEP 2-2: ê¸€ë¡œë²Œ ì¤‘ì‹¬ ë²¡í„° ê³„ì‚°"""
        print("\nğŸ¯ STEP 2-2: ê¸€ë¡œë²Œ ì¤‘ì‹¬ ë²¡í„° ê³„ì‚° ì¤‘...")
        
        if self.image_embeddings is None:
            raise ValueError("ë¨¼ì € extract_all_embeddings()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ê¸€ë¡œë²Œ ì¤‘ì‹¬ ê³„ì‚° (ì´ë¯¸ì§€ ì„ë² ë”©ë§Œ ì‚¬ìš©)
        global_center = np.mean(self.image_embeddings, axis=0)
        
        # ì •ê·œí™”
        global_center = global_center / np.linalg.norm(global_center)
        
        self.global_center = global_center
        
        print(f"âœ… ê¸€ë¡œë²Œ ì¤‘ì‹¬ ë²¡í„° ê³„ì‚° ì™„ë£Œ:")
        print(f"   ì°¨ì›: {global_center.shape}")
        print(f"   ë…¸ë¦„: {np.linalg.norm(global_center):.6f}")
        print(f"   ì˜ë¯¸: 'ì „ì²´ íŒ¨ì…˜ ë°ì´í„°ì˜ í‰ê·  ìŠ¤íƒ€ì¼'")
        
        return global_center
    
    def compute_centrality_scores(self) -> np.ndarray:
        """STEP 2-3: ì¤‘ì‹¬ì„± ì ìˆ˜ ê³„ì‚° (Cosine Similarity)"""
        print("\nğŸ“ STEP 2-3: ì¤‘ì‹¬ì„± ì ìˆ˜ ê³„ì‚° ì¤‘...")
        
        if self.global_center is None:
            raise ValueError("ë¨¼ì € compute_global_center()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ê° ìƒí’ˆì— ëŒ€í•´ ì¤‘ì‹¬ì„± ì ìˆ˜ ê³„ì‚°
        centrality_scores = []
        
        for i, embedding in enumerate(self.image_embeddings):
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            score = np.dot(embedding, self.global_center) / (
                np.linalg.norm(embedding) * np.linalg.norm(self.global_center)
            )
            centrality_scores.append(score)
        
        self.centrality_scores = np.array(centrality_scores)
        
        # í†µê³„ ì •ë³´
        print(f"âœ… ì¤‘ì‹¬ì„± ì ìˆ˜ ê³„ì‚° ì™„ë£Œ:")
        print(f"   í‰ê· : {self.centrality_scores.mean():.4f}")
        print(f"   í‘œì¤€í¸ì°¨: {self.centrality_scores.std():.4f}")
        print(f"   ìµœì†Œê°’: {self.centrality_scores.min():.4f}")
        print(f"   ìµœëŒ€ê°’: {self.centrality_scores.max():.4f}")
        
        # ë¶„í¬ ì •ë³´
        percentiles = [10, 25, 50, 75, 90, 95]
        print(f"   ë¶„ìœ„ìˆ˜:")
        for p in percentiles:
            value = np.percentile(self.centrality_scores, p)
            print(f"     {p}%: {value:.4f}")
        
        return self.centrality_scores
    
    def create_anchor_and_tail_sets(self, anchor_percentile: int = 90, tail_percentile: int = 50) -> Dict[str, Any]:
        """STEP 2-4: Anchor Set (ìƒìœ„ 10%) ë° Tail Set (í•˜ìœ„ 50%) ìƒì„±"""
        print(f"\nâš“ STEP 2-4: Anchor Set (ìƒìœ„ {100-anchor_percentile}%) ë° Tail Set (í•˜ìœ„ {tail_percentile}%) ìƒì„± ì¤‘...")
        
        if self.centrality_scores is None:
            raise ValueError("ë¨¼ì € compute_centrality_scores()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ì„ê³„ê°’ ê³„ì‚°
        anchor_threshold = np.percentile(self.centrality_scores, anchor_percentile)
        tail_threshold = np.percentile(self.centrality_scores, tail_percentile)
        
        # ì¸ë±ìŠ¤ ì„ íƒ
        self.anchor_indices = np.where(self.centrality_scores >= anchor_threshold)[0]
        self.tail_indices = np.where(self.centrality_scores <= tail_threshold)[0]
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ ë¶„ì„
        anchor_categories = {}
        tail_categories = {}
        all_categories = {}
        
        for idx in self.anchor_indices:
            category = self.fashion_items[idx].category
            anchor_categories[category] = anchor_categories.get(category, 0) + 1
        
        for idx in self.tail_indices:
            category = self.fashion_items[idx].category
            tail_categories[category] = tail_categories.get(category, 0) + 1
        
        for item in self.fashion_items:
            category = item.category
            all_categories[category] = all_categories.get(category, 0) + 1
        
        print(f"âœ… Anchor & Tail Set ìƒì„± ì™„ë£Œ:")
        print(f"   Anchor ì„ê³„ê°’: {anchor_threshold:.4f}")
        print(f"   Tail ì„ê³„ê°’: {tail_threshold:.4f}")
        print(f"   Anchor Set í¬ê¸°: {len(self.anchor_indices)} ({len(self.anchor_indices)/len(self.fashion_items)*100:.1f}%)")
        print(f"   Tail Set í¬ê¸°: {len(self.tail_indices)} ({len(self.tail_indices)/len(self.fashion_items)*100:.1f}%)")
        
        print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
        print(f"   ì „ì²´:")
        for cat, count in all_categories.items():
            print(f"     {cat}: {count}ê°œ ({count/len(self.fashion_items)*100:.1f}%)")
        
        print(f"   Anchor Set (ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy):")
        for cat, count in anchor_categories.items():
            total_cat = all_categories[cat]
            print(f"     {cat}: {count}ê°œ ({count/total_cat*100:.1f}% of {cat})")
        
        print(f"   Tail Set:")
        for cat, count in tail_categories.items():
            total_cat = all_categories[cat]
            print(f"     {cat}: {count}ê°œ ({count/total_cat*100:.1f}% of {cat})")
        
        return {
            'anchor_indices': self.anchor_indices,
            'tail_indices': self.tail_indices,
            'anchor_threshold': anchor_threshold,
            'tail_threshold': tail_threshold,
            'anchor_categories': anchor_categories,
            'tail_categories': tail_categories,
            'all_categories': all_categories
        }
    
    def analyze_centrality_distribution(self) -> Dict[str, Any]:
        """ì¤‘ì‹¬ì„± ë¶„í¬ ìƒì„¸ ë¶„ì„"""
        print("\nğŸ“ˆ ì¤‘ì‹¬ì„± ë¶„í¬ ìƒì„¸ ë¶„ì„ ì¤‘...")
        
        if self.centrality_scores is None:
            raise ValueError("ë¨¼ì € compute_centrality_scores()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ì‹¬ì„± ë¶„ì„
        category_centrality = {}
        for i, item in enumerate(self.fashion_items):
            category = item.category
            if category not in category_centrality:
                category_centrality[category] = []
            category_centrality[category].append(self.centrality_scores[i])
        
        # í†µê³„ ê³„ì‚°
        category_stats = {}
        for category, scores in category_centrality.items():
            scores = np.array(scores)
            category_stats[category] = {
                'mean': float(scores.mean()),
                'std': float(scores.std()),
                'min': float(scores.min()),
                'max': float(scores.max()),
                'count': len(scores),
                'median': float(np.median(scores))
            }
        
        print(f"âœ… ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ì‹¬ì„± ë¶„ì„:")
        for category, stats in category_stats.items():
            print(f"   {category}:")
            print(f"     í‰ê· : {stats['mean']:.4f} Â± {stats['std']:.4f}")
            print(f"     ë²”ìœ„: [{stats['min']:.4f}, {stats['max']:.4f}]")
            print(f"     ì¤‘ì•™ê°’: {stats['median']:.4f}")
            print(f"     ìƒ˜í”Œ ìˆ˜: {stats['count']}")
        
        # ì „ì²´ ë¶„í¬ ë¶„ì„
        overall_stats = {
            'mean': float(self.centrality_scores.mean()),
            'std': float(self.centrality_scores.std()),
            'min': float(self.centrality_scores.min()),
            'max': float(self.centrality_scores.max()),
            'median': float(np.median(self.centrality_scores)),
            'skewness': float(self._compute_skewness(self.centrality_scores)),
            'kurtosis': float(self._compute_kurtosis(self.centrality_scores))
        }
        
        print(f"\nğŸ“Š ì „ì²´ ë¶„í¬ íŠ¹ì„±:")
        print(f"   í‰ê· : {overall_stats['mean']:.4f}")
        print(f"   í‘œì¤€í¸ì°¨: {overall_stats['std']:.4f}")
        print(f"   ì™œë„(Skewness): {overall_stats['skewness']:.4f}")
        print(f"   ì²¨ë„(Kurtosis): {overall_stats['kurtosis']:.4f}")
        
        return {
            'category_stats': category_stats,
            'overall_stats': overall_stats,
            'category_centrality': {k: [float(x) for x in v] for k, v in category_centrality.items()}
        }
    
    def _compute_skewness(self, data):
        """ì™œë„ ê³„ì‚°"""
        mean = np.mean(data)
        std = np.std(data)
        return np.mean(((data - mean) / std) ** 3)
    
    def _compute_kurtosis(self, data):
        """ì²¨ë„ ê³„ì‚°"""
        mean = np.mean(data)
        std = np.std(data)
        return np.mean(((data - mean) / std) ** 4) - 3
    
    def visualize_centrality_distribution(self, save_path: str = "results/centrality_analysis.png"):
        """ì¤‘ì‹¬ì„± ë¶„í¬ ì‹œê°í™” (ê°„ë‹¨ ë²„ì „)"""
        print(f"\nğŸ“Š ì¤‘ì‹¬ì„± ë¶„í¬ ì‹œê°í™” ìƒëµ (matplotlib ì˜ì¡´ì„± ë¬¸ì œ)")
        
        if self.centrality_scores is None:
            raise ValueError("ë¨¼ì € compute_centrality_scores()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°„ë‹¨ ë¶„ì„ë§Œ ìˆ˜í–‰
        print(f"âœ… ë¶„í¬ ë¶„ì„ (í…ìŠ¤íŠ¸):")
        print(f"   í‰ê· : {self.centrality_scores.mean():.4f}")
        print(f"   í‘œì¤€í¸ì°¨: {self.centrality_scores.std():.4f}")
        print(f"   ë²”ìœ„: [{self.centrality_scores.min():.4f}, {self.centrality_scores.max():.4f}]")
        
        # íˆìŠ¤í† ê·¸ë¨ í…ìŠ¤íŠ¸ ë²„ì „
        hist, bin_edges = np.histogram(self.centrality_scores, bins=10)
        print(f"   íˆìŠ¤í† ê·¸ë¨ (10 bins):")
        for i in range(len(hist)):
            bar = "â–ˆ" * int(hist[i] / max(hist) * 20)  # ìµœëŒ€ 20ì ë§‰ëŒ€
            print(f"     [{bin_edges[i]:.3f}-{bin_edges[i+1]:.3f}]: {hist[i]:4d} {bar}")
        
        return True
    
    def run_complete_analysis(self, anchor_percentile: int = 90, tail_percentile: int = 50) -> Dict[str, Any]:
        """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸš€ ì„ë² ë”© ì¤‘ì‹¬ì„± ê¸°ë°˜ ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ë¶„ì„ ì‹œì‘")
        print("=" * 80)
        print("ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´: 'ë² ìŠ¤íŠ¸ì…€ëŸ¬ë¥¼ íŒë§¤ ë°ì´í„° ì—†ì´, ì„ë² ë”© ê³µê°„ì˜ ì¤‘ì‹¬ì„±ìœ¼ë¡œ ê·¼ì‚¬'")
        print("ğŸ§  ê°œë… ì§ê´€: 'ì¤‘ì‹¬ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ëŒ€ì¤‘ì ì´ë‹¤'")
        print("=" * 80)
        
        # STEP 2-1: ì„ë² ë”© ì¶”ì¶œ
        embedding_info = self.extract_all_embeddings()
        
        # STEP 2-2: ê¸€ë¡œë²Œ ì¤‘ì‹¬ ê³„ì‚°
        global_center = self.compute_global_center()
        
        # STEP 2-3: ì¤‘ì‹¬ì„± ì ìˆ˜ ê³„ì‚°
        centrality_scores = self.compute_centrality_scores()
        
        # STEP 2-4: Anchor & Tail Set ìƒì„±
        sets_info = self.create_anchor_and_tail_sets(anchor_percentile, tail_percentile)
        
        # ìƒì„¸ ë¶„ì„
        distribution_analysis = self.analyze_centrality_distribution()
        
        # ì‹œê°í™”
        self.visualize_centrality_distribution()
        
        # ê²°ê³¼ ì¢…í•© (JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •)
        complete_results = {
            'timestamp': datetime.now().isoformat(),
            'method': 'Embedding Centrality Proxy',
            'core_idea': 'ë² ìŠ¤íŠ¸ì…€ëŸ¬ë¥¼ íŒë§¤ ë°ì´í„° ì—†ì´, ì„ë² ë”© ê³µê°„ì˜ ì¤‘ì‹¬ì„±ìœ¼ë¡œ ê·¼ì‚¬',
            'intuition': 'ì¤‘ì‹¬ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ëŒ€ì¤‘ì ì´ë‹¤',
            
            'embedding_info': {
                'num_items': embedding_info['num_items'],
                'image_embeddings_shape': list(embedding_info['image_embeddings'].shape),
                'json_embeddings_shape': list(embedding_info['json_embeddings'].shape)
            },
            'global_center': {
                'norm': float(np.linalg.norm(global_center)),
                'dimension': int(global_center.shape[0])
            },
            'centrality_analysis': {
                'statistics': {
                    'mean': float(centrality_scores.mean()),
                    'std': float(centrality_scores.std()),
                    'min': float(centrality_scores.min()),
                    'max': float(centrality_scores.max())
                }
            },
            'sets_info': {
                'anchor_indices': self.anchor_indices.tolist(),
                'tail_indices': self.tail_indices.tolist(),
                'anchor_threshold': float(sets_info['anchor_threshold']),
                'tail_threshold': float(sets_info['tail_threshold']),
                'anchor_categories': sets_info['anchor_categories'],
                'tail_categories': sets_info['tail_categories'],
                'all_categories': sets_info['all_categories']
            },
            'distribution_analysis': distribution_analysis,
            
            'parameters': {
                'anchor_percentile': anchor_percentile,
                'tail_percentile': tail_percentile,
                'embedding_type': 'image_only',
                'similarity_metric': 'cosine'
            }
        }
        
        print(f"\nğŸ‰ ì „ì²´ ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“Š ì£¼ìš” ê²°ê³¼:")
        print(f"   ì´ ì•„ì´í…œ: {len(self.fashion_items)}")
        print(f"   Anchor Set (ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy): {len(self.anchor_indices)}ê°œ ({len(self.anchor_indices)/len(self.fashion_items)*100:.1f}%)")
        print(f"   Tail Set: {len(self.tail_indices)}ê°œ ({len(self.tail_indices)/len(self.fashion_items)*100:.1f}%)")
        print(f"   ì¤‘ì‹¬ì„± ì ìˆ˜ ë²”ìœ„: [{centrality_scores.min():.4f}, {centrality_scores.max():.4f}]")
        
        return complete_results


def run_embedding_centrality_analysis():
    """ì„ë² ë”© ì¤‘ì‹¬ì„± ë¶„ì„ ì‹¤í–‰"""
    print("ğŸ¯ ì„ë² ë”© ì¤‘ì‹¬ì„± ê¸°ë°˜ ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ì‹œìŠ¤í…œ")
    print("=" * 80)
    print("ğŸ“Œ ë…¼ë¬¸/ì¡¸ì—…ì‘í’ˆì˜ í•µì‹¬ ì•„ì´ë””ì–´ êµ¬í˜„")
    print("ğŸ¯ ëª©í‘œ: 'ë² ìŠ¤íŠ¸ì…€ëŸ¬ë¥¼ íŒë§¤ ë°ì´í„° ì—†ì´, ì„ë² ë”© ê³µê°„ì˜ ì¤‘ì‹¬ì„±ìœ¼ë¡œ ê·¼ì‚¬'")
    print("=" * 80)
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ
    dataset_path = "C:/sample/ë¼ë²¨ë§ë°ì´í„°"
    
    # Baseline v1 ì„¤ì • (Temperature 0.1)
    config = TrainingConfig()
    config.temperature = 0.1
    config.batch_size = 16
    config.max_epochs = 8
    
    try:
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        system = FashionEncoderSystem()
        system.config = config
        
        # ë°ì´í„° ì„¤ì •
        print("ğŸ“ ë°ì´í„° ì„¤ì • ì¤‘...")
        system.setup_data(dataset_path)
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        print("ğŸ‹ï¸ íŠ¸ë ˆì´ë„ˆ ì„¤ì • ì¤‘...")
        system.setup_trainer()
        
        # Baseline v1 ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint_path = "checkpoints/baseline_v1_best_model.pt"
        if Path(checkpoint_path).exists():
            print(f"ğŸ“¦ Baseline v1 ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
            system.trainer.load_checkpoint(checkpoint_path)
        else:
            # ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ ì‹œë„
            checkpoint_path = "checkpoints/best_model.pt"
            if Path(checkpoint_path).exists():
                print(f"ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
                system.trainer.load_checkpoint(checkpoint_path)
            else:
                print("âš ï¸ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ëª¨ë¸ ìƒíƒœë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
        
        # ì¤‘ì‹¬ì„± ë¶„ì„ ì‹¤í–‰
        analyzer = EmbeddingCentralityProxy(system)
        results = analyzer.run_complete_analysis(
            anchor_percentile=90,  # ìƒìœ„ 10%
            tail_percentile=50     # í•˜ìœ„ 50%
        )
        
        # ê²°ê³¼ ì €ì¥
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        results_file = results_dir / "embedding_centrality_analysis.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {results_file}")
        
        # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
        print(f"\nğŸ”® ë‹¤ìŒ ë‹¨ê³„:")
        print(f"   1. Query-Aware Evaluationì— Anchor Set ì ìš©")
        print(f"   2. Anchor Queries Recall@10 â‰¥ 90% ëª©í‘œ ë‹¬ì„± í™•ì¸")
        print(f"   3. Sensitivity Analysis (5%, 10%, 15% ë¹„êµ)")
        print(f"   4. ë…¼ë¬¸/ë°œí‘œ ìë£Œìš© ê²°ê³¼ ì •ë¦¬")
        
        # ì •ë¦¬
        system.cleanup()
        
        print(f"\nâœ¨ ì„ë² ë”© ì¤‘ì‹¬ì„± ë¶„ì„ ì™„ë£Œ!")
        
        return results
        
    except Exception as e:
        print(f"\nâŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    run_embedding_centrality_analysis()