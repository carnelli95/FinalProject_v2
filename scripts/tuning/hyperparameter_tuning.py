#!/usr/bin/env python3
"""
Optuna ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ê°œì„ ëœ ë²„ì „)

ëª©í‘œ:
1ìˆœìœ„: maximize (positive_similarity - negative_similarity)
2ìˆœìœ„: MRR, Category-aware Precision@5
"""

import optuna
import torch
import json
import time
import numpy as np
from pathlib import Path
from typing import Dict, Any, Tuple
from collections import defaultdict

from data.fashion_dataset import FashionDataModule
from training.trainer import create_trainer_from_data_module
from utils.config import TrainingConfig


class HyperparameterTuner:
    """Optuna ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë„ˆ (ê°œì„ ëœ ëª©ì  í•¨ìˆ˜)"""
    
    def __init__(self, 
                 dataset_path: str = "C:/sample/ë¼ë²¨ë§ë°ì´í„°",
                 n_trials: int = 20,
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):
        self.dataset_path = dataset_path
        self.n_trials = n_trials
        self.device = device
        
        # ë°ì´í„° ëª¨ë“ˆ ë¯¸ë¦¬ ì¤€ë¹„
        print("ğŸ“‚ ë°ì´í„° ëª¨ë“ˆ ì¤€ë¹„ ì¤‘...")
        self.data_module = FashionDataModule(
            dataset_path=dataset_path,
            target_categories=['ë ˆíŠ¸ë¡œ', 'ë¡œë§¨í‹±', 'ë¦¬ì¡°íŠ¸'],
            batch_size=64  # ê¸°ë³¸ê°’, ë‚˜ì¤‘ì— íŠœë‹ë¨
        )
        self.data_module.setup()
        print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(self.data_module.train_dataset)} í•™ìŠµ ìƒ˜í”Œ")
        
    def compute_category_aware_metrics(self, trainer, val_loader) -> Dict[str, float]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì •ë°€ë„ ë° ê³ ê¸‰ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        trainer.contrastive_learner.eval()
        
        all_image_embeddings = []
        all_json_embeddings = []
        all_categories = []
        
        with torch.no_grad():
            for batch in val_loader:
                batch = trainer._move_batch_to_device(batch)
                json_batch = trainer._convert_batch_to_dict(batch)
                
                # ì„ë² ë”© ê³„ì‚°
                embeddings = trainer.contrastive_learner.get_embeddings(batch.images, json_batch)
                
                all_image_embeddings.append(embeddings['image_embeddings'].cpu())
                all_json_embeddings.append(embeddings['json_embeddings'].cpu())
                all_categories.append(batch.category_ids.cpu())
        
        # ì „ì²´ ì„ë² ë”© ì—°ê²°
        image_embeddings = torch.cat(all_image_embeddings, dim=0)
        json_embeddings = torch.cat(all_json_embeddings, dim=0)
        categories = torch.cat(all_categories, dim=0)
        
        # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚° (ì •ì‚¬ê°í˜• í–‰ë ¬)
        similarity_matrix = torch.matmul(image_embeddings, json_embeddings.T)
        
        # 1ìˆœìœ„ ëª©í‘œ: positive/negative similarity gap
        batch_size = similarity_matrix.size(0)
        positive_similarities = similarity_matrix.diag()
        
        # negative similarities (ëŒ€ê°ì„  ì œì™¸)
        mask = torch.eye(batch_size, dtype=torch.bool)
        negative_similarities = similarity_matrix[~mask]
        
        pos_sim_mean = positive_similarities.mean().item()
        neg_sim_mean = negative_similarities.mean().item()
        similarity_gap = pos_sim_mean - neg_sim_mean
        
        # 2ìˆœìœ„ ëª©í‘œ: Category-aware Precision@5
        category_precision_5 = self._compute_category_precision_at_k(
            similarity_matrix, categories, k=5
        )
        
        # MRR ê³„ì‚°
        ranks = (similarity_matrix.argsort(dim=1, descending=True) == 
                torch.arange(similarity_matrix.size(0)).unsqueeze(1)).nonzero()[:, 1] + 1
        mrr = (1.0 / ranks.float()).mean().item()
        
        # Top-5 ì •í™•ë„
        top5_indices = similarity_matrix.topk(k=min(5, similarity_matrix.size(0)), dim=1)[1]
        top5_correct = (top5_indices == torch.arange(similarity_matrix.size(0)).unsqueeze(1)).any(dim=1).float().mean().item()
        
        return {
            'similarity_gap': similarity_gap,
            'positive_similarity': pos_sim_mean,
            'negative_similarity': neg_sim_mean,
            'category_precision_5': category_precision_5,
            'mrr': mrr,
            'top5_accuracy': top5_correct
        }
    
    def _compute_category_precision_at_k(self, similarity_matrix: torch.Tensor, 
                                       categories: torch.Tensor, k: int = 5) -> float:
        """ì¹´í…Œê³ ë¦¬ë³„ Precision@K ê³„ì‚°"""
        batch_size = similarity_matrix.size(0)
        category_precisions = []
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê³„ì‚°
        unique_categories = categories.unique()
        
        for category in unique_categories:
            # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ì¸ë±ìŠ¤ë“¤
            category_mask = (categories == category)
            category_indices = category_mask.nonzero().squeeze(-1)
            
            if len(category_indices) < 2:  # ì¹´í…Œê³ ë¦¬ì— ìƒ˜í”Œì´ 1ê°œ ì´í•˜ë©´ ê±´ë„ˆë›°ê¸°
                continue
            
            category_similarities = similarity_matrix[category_indices][:, category_indices]
            
            # Top-K ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê°™ì€ ì¹´í…Œê³ ë¦¬ ë¹„ìœ¨ ê³„ì‚°
            topk_indices = category_similarities.topk(k=min(k+1, category_similarities.size(1)), dim=1)[1]
            
            # ìê¸° ìì‹  ì œì™¸í•˜ê³  ê³„ì‚°
            precision_scores = []
            for i, topk in enumerate(topk_indices):
                # ìê¸° ìì‹ (ì²« ë²ˆì§¸) ì œì™¸
                relevant_topk = topk[1:k+1] if len(topk) > 1 else topk[1:]
                if len(relevant_topk) > 0:
                    precision = len(relevant_topk) / min(k, len(relevant_topk))
                    precision_scores.append(precision)
            
            if precision_scores:
                category_precisions.append(np.mean(precision_scores))
        
        return np.mean(category_precisions) if category_precisions else 0.0
        
    def objective(self, trial: optuna.Trial) -> float:
        """Optuna ëª©ì  í•¨ìˆ˜ - 1ìˆœìœ„: similarity gap ìµœëŒ€í™”"""
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ (ì‚¬ìš©ì ì¶”ì²œ ë²”ìœ„)
        config = TrainingConfig(
            # í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°
            learning_rate=trial.suggest_categorical('learning_rate', [1e-4, 3e-4, 5e-4]),
            temperature=trial.suggest_categorical('temperature', [0.03, 0.05, 0.07, 0.1]),
            batch_size=trial.suggest_categorical('batch_size', [64, 96, 128]),
            
            # ëª¨ë¸ êµ¬ì¡°
            embedding_dim=trial.suggest_categorical('embedding_dim', [128, 256]),
            hidden_dim=trial.suggest_categorical('hidden_dim', [256, 512, 768]),
            dropout_rate=trial.suggest_float('dropout_rate', 0.1, 0.3),
            
            # ì •ê·œí™”
            weight_decay=trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True),
            
            # ê³ ì •ê°’
            output_dim=512,  # ê³ ì •
            max_epochs=10,   # íŠœë‹ìš© ì§§ì€ ì—í¬í¬
        )
        
        print(f"\nğŸ” Trial {trial.number + 1}/{self.n_trials}")
        print(f"   í•™ìŠµë¥ : {config.learning_rate:.6f}")
        print(f"   ì˜¨ë„: {config.temperature:.3f}")
        print(f"   ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {config.batch_size}")
        print(f"   ì„ë² ë”© ì°¨ì›: {config.embedding_dim}")
        print(f"   ì€ë‹‰ì¸µ ì°¨ì›: {config.hidden_dim}")
        
        try:
            # ë°ì´í„° ë¡œë” ì—…ë°ì´íŠ¸
            self.data_module.batch_size = config.batch_size
            self.data_module._train_dataloader = None
            self.data_module._val_dataloader = None
            
            train_loader = self.data_module.train_dataloader()
            val_loader = self.data_module.val_dataloader()
            
            # íŠ¸ë ˆì´ë„ˆ ìƒì„±
            trainer = create_trainer_from_data_module(
                data_module=self.data_module,
                config=config,
                device=self.device,
                checkpoint_dir=f'tuning_checkpoints/trial_{trial.number}',
                log_dir=f'tuning_logs/trial_{trial.number}'
            )
            
            # í•™ìŠµ ì‹¤í–‰
            print(f"   ğŸš€ í•™ìŠµ ì‹œì‘...")
            start_time = time.time()
            
            results = trainer.train_contrastive_learning(
                train_loader=train_loader,
                val_loader=val_loader,
                num_epochs=config.max_epochs
            )
            
            elapsed = time.time() - start_time
            
            # ê³ ê¸‰ ë©”íŠ¸ë¦­ ê³„ì‚°
            advanced_metrics = self.compute_category_aware_metrics(trainer, val_loader)
            
            # 1ìˆœìœ„ ëª©ì  í•¨ìˆ˜: positive/negative similarity gap
            similarity_gap = advanced_metrics['similarity_gap']
            
            # 2ìˆœìœ„ ë©”íŠ¸ë¦­ë“¤
            category_precision_5 = advanced_metrics['category_precision_5']
            mrr = advanced_metrics['mrr']
            top5_accuracy = advanced_metrics['top5_accuracy']
            
            print(f"   â±ï¸ í•™ìŠµ ì™„ë£Œ: {elapsed:.1f}ì´ˆ")
            print(f"   ğŸ“Š ê²°ê³¼:")
            print(f"      ğŸ¯ Similarity Gap: {similarity_gap:.4f}")
            print(f"      ğŸ“ˆ Category P@5: {category_precision_5:.4f}")
            print(f"      ğŸ” MRR: {mrr:.4f}")
            print(f"      âœ… Top-5 ì •í™•ë„: {top5_accuracy:.4f}")
            print(f"      â• Positive Sim: {advanced_metrics['positive_similarity']:.4f}")
            print(f"      â– Negative Sim: {advanced_metrics['negative_similarity']:.4f}")
            
            # ë³µí•© ëª©ì  í•¨ìˆ˜ (ê°€ì¤‘ í‰ê· )
            # 1ìˆœìœ„: similarity gap (ê°€ì¤‘ì¹˜ 0.7)
            # 2ìˆœìœ„: category precision@5 + MRR (ê°€ì¤‘ì¹˜ 0.3)
            objective_value = (
                0.7 * similarity_gap + 
                0.2 * category_precision_5 + 
                0.1 * mrr
            )
            
            print(f"      ğŸ† ëª©ì í•¨ìˆ˜ ê°’: {objective_value:.4f}")
            
            # ì¤‘ê°„ ê²°ê³¼ ë³´ê³  (pruningìš©)
            trial.report(objective_value, config.max_epochs)
            
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            trainer.close()
            
            return objective_value
            
        except Exception as e:
            print(f"   âŒ Trial ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return -1.0  # ì‹¤íŒ¨í•œ ê²½ìš° ìµœì†Œê°’ ë°˜í™˜
    
    def run_tuning(self) -> Dict[str, Any]:
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰"""
        print(f"\nğŸ¯ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")
        print(f"   ì‹œí–‰ íšŸìˆ˜: {self.n_trials}")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   ğŸ¥‡ 1ìˆœìœ„ ëª©í‘œ: maximize (positive_similarity - negative_similarity)")
        print(f"   ğŸ¥ˆ 2ìˆœìœ„ ëª©í‘œ: Category-aware Precision@5, MRR")
        
        # Optuna ìŠ¤í„°ë”” ìƒì„±
        study = optuna.create_study(
            direction='maximize',  # ë³µí•© ëª©ì  í•¨ìˆ˜ ìµœëŒ€í™”
            sampler=optuna.samplers.TPESampler(seed=42),
            pruner=optuna.pruners.MedianPruner(
                n_startup_trials=3,
                n_warmup_steps=3
            )
        )
        
        # íŠœë‹ ì‹¤í–‰
        start_time = time.time()
        study.optimize(self.objective, n_trials=self.n_trials)
        total_time = time.time() - start_time
        
        # ê²°ê³¼ ë¶„ì„
        best_trial = study.best_trial
        best_params = best_trial.params
        best_value = best_trial.value
        
        print(f"\nğŸ† íŠœë‹ ì™„ë£Œ!")
        print(f"   ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
        print(f"   ìµœê³  ëª©ì í•¨ìˆ˜ ê°’: {best_value:.4f}")
        print(f"\nğŸ“‹ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
        for key, value in best_params.items():
            print(f"   {key}: {value}")
        
        # ìƒìœ„ 3ê°œ trial ë¶„ì„
        print(f"\nğŸ¥‡ ìƒìœ„ 3ê°œ Trial ê²°ê³¼:")
        sorted_trials = sorted(study.trials, key=lambda t: t.value if t.value else -999, reverse=True)
        for i, trial in enumerate(sorted_trials[:3]):
            if trial.value is not None:
                print(f"   {i+1}ìœ„: Trial {trial.number}, ì ìˆ˜: {trial.value:.4f}")
                print(f"        lr: {trial.params.get('learning_rate', 'N/A')}, "
                      f"temp: {trial.params.get('temperature', 'N/A')}, "
                      f"batch: {trial.params.get('batch_size', 'N/A')}")
        
        # ê²°ê³¼ ì €ì¥
        results = {
            'best_params': best_params,
            'best_value': best_value,
            'n_trials': self.n_trials,
            'total_time': total_time,
            'objective_function': 'similarity_gap + category_precision@5 + mrr',
            'top_trials': [
                {
                    'number': trial.number,
                    'value': trial.value,
                    'params': trial.params,
                    'state': trial.state.name
                }
                for trial in sorted_trials[:5] if trial.value is not None
            ],
            'all_trials': [
                {
                    'number': trial.number,
                    'value': trial.value,
                    'params': trial.params,
                    'state': trial.state.name
                }
                for trial in study.trials
            ]
        }
        
        # ê²°ê³¼ íŒŒì¼ ì €ì¥
        results_dir = Path("tuning_results")
        results_dir.mkdir(exist_ok=True)
        
        timestamp = int(time.time())
        results_file = results_dir / f"optuna_similarity_gap_tuning_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ ê²°ê³¼ ì €ì¥: {results_file}")
        
        return results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("Fashion JSON Encoder í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ê°œì„ ëœ ë²„ì „)")
    print("=" * 70)
    print("ğŸ¯ ëª©í‘œ:")
    print("   1ìˆœìœ„: maximize (positive_similarity - negative_similarity)")
    print("   2ìˆœìœ„: Category-aware Precision@5, MRR")
    print("   êµìˆ˜ë‹˜ ìš”êµ¬ì‚¬í•­: ë™ì¼ ì¹´í…Œê³ ë¦¬ ë‚´ ì •ë ¬ ì •í™•ë„ â‰¥ 0.9")
    
    # íŠœë„ˆ ìƒì„± ë° ì‹¤í–‰
    tuner = HyperparameterTuner(
        dataset_path="C:/sample/ë¼ë²¨ë§ë°ì´í„°",
        n_trials=12,  # ì‹œì‘ì€ 12íšŒë¡œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
        device='cpu'  # CPUì—ì„œ ì•ˆì •ì ìœ¼ë¡œ
    )
    
    try:
        results = tuner.run_tuning()
        
        print(f"\nğŸ‰ íŠœë‹ ì„±ê³µ!")
        print(f"   ìµœì  ì„¤ì •:")
        best_params = results['best_params']
        print(f"     í•™ìŠµë¥ : {best_params.get('learning_rate', 'N/A')}")
        print(f"     ì˜¨ë„: {best_params.get('temperature', 'N/A')}")
        print(f"     ë°°ì¹˜ í¬ê¸°: {best_params.get('batch_size', 'N/A')}")
        print(f"     ì„ë² ë”© ì°¨ì›: {best_params.get('embedding_dim', 'N/A')}")
        print(f"     ì€ë‹‰ì¸µ ì°¨ì›: {best_params.get('hidden_dim', 'N/A')}")
        
        print(f"\nğŸ“ˆ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"   1. ìµœì  ì„¤ì •ìœ¼ë¡œ ë³¸ê²© í•™ìŠµ (50-100 ì—í¬í¬)")
        print(f"   2. Category-aware ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§")
        print(f"   3. Positive/Negative similarity gap í™•ì¸")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ íŠœë‹ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()