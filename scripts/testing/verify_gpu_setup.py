#!/usr/bin/env python3
"""
GPU í™˜ê²½ ë° ëŒ€ìš©ëŸ‰ í•™ìŠµ ì¤€ë¹„ ìƒíƒœ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import torch
import torchvision
import transformers
import numpy as np
from pathlib import Path

def check_python_version():
    """Python ë²„ì „ í™•ì¸"""
    print("ğŸ Python ë²„ì „ í™•ì¸")
    version = sys.version_info
    print(f"   í˜„ì¬ ë²„ì „: {version.major}.{version.minor}.{version.micro}")
    
    if version.major == 3 and 9 <= version.minor <= 11:
        print("   âœ… ê¶Œì¥ ë²„ì „ ë²”ìœ„ (3.9-3.11)")
        return True
    else:
        print("   âš ï¸  ê¶Œì¥: Python 3.9-3.11")
        return False

def check_cuda_setup():
    """CUDA ì„¤ì • í™•ì¸"""
    print("\nğŸ”¥ CUDA í™˜ê²½ í™•ì¸")
    
    # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
    cuda_available = torch.cuda.is_available()
    print(f"   CUDA ì‚¬ìš© ê°€ëŠ¥: {'âœ…' if cuda_available else 'âŒ'}")
    
    if not cuda_available:
        print("   âŒ CUDAê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ PyTorchê°€ CPU ë²„ì „ì…ë‹ˆë‹¤")
        return False
    
    # CUDA ë²„ì „
    cuda_version = torch.version.cuda
    print(f"   CUDA ë²„ì „: {cuda_version}")
    
    # GPU ì •ë³´
    gpu_count = torch.cuda.device_count()
    print(f"   GPU ê°œìˆ˜: {gpu_count}")
    
    total_memory = 0
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        total_memory += gpu_memory
        print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    
    print(f"   ì´ GPU ë©”ëª¨ë¦¬: {total_memory:.1f}GB")
    
    # 65GB í•™ìŠµì„ ìœ„í•œ ê¶Œì¥ì‚¬í•­
    if total_memory >= 48:  # 24GB x 2 ì´ìƒ
        print("   âœ… 65GB ëŒ€ìš©ëŸ‰ í•™ìŠµ ê°€ëŠ¥")
        return True
    elif total_memory >= 24:  # 24GB ì´ìƒ
        print("   âš ï¸  ë‹¨ì¼ GPU í•™ìŠµ ê°€ëŠ¥, ë¶„ì‚° í•™ìŠµ ê¶Œì¥")
        return True
    else:
        print("   âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (ìµœì†Œ 24GB ê¶Œì¥)")
        return False

def check_pytorch_version():
    """PyTorch ë²„ì „ í™•ì¸"""
    print("\nğŸ”¥ PyTorch ë²„ì „ í™•ì¸")
    
    torch_version = torch.__version__
    torchvision_version = torchvision.__version__
    
    print(f"   PyTorch: {torch_version}")
    print(f"   TorchVision: {torchvision_version}")
    
    # ë²„ì „ íŒŒì‹±
    torch_major, torch_minor = map(int, torch_version.split('.')[:2])
    
    if torch_major >= 2 and torch_minor >= 1:
        print("   âœ… ê¶Œì¥ ë²„ì „ (2.1.0+)")
        return True
    elif torch_major >= 2:
        print("   âš ï¸  ìµœì†Œ ë²„ì „ ì¶©ì¡±, 2.1.0+ ê¶Œì¥")
        return True
    else:
        print("   âŒ PyTorch 2.0+ í•„ìš”")
        return False

def check_transformers():
    """Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸"""
    print("\nğŸ¤— Transformers í™•ì¸")
    
    transformers_version = transformers.__version__
    print(f"   Transformers: {transformers_version}")
    
    # CLIP ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
    try:
        from transformers import CLIPVisionModel
        model = CLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")
        print("   âœ… CLIP ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        return True
    except Exception as e:
        print(f"   âŒ CLIP ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def check_mixed_precision():
    """Mixed Precision ì§€ì› í™•ì¸"""
    print("\nâš¡ Mixed Precision í™•ì¸")
    
    if not torch.cuda.is_available():
        print("   âŒ CUDA í•„ìš”")
        return False
    
    try:
        # AMP í…ŒìŠ¤íŠ¸
        scaler = torch.cuda.amp.GradScaler()
        
        # ê°„ë‹¨í•œ ì—°ì‚° í…ŒìŠ¤íŠ¸
        x = torch.randn(2, 3, device='cuda')
        with torch.cuda.amp.autocast():
            y = x * 2
        
        print("   âœ… Mixed Precision ì§€ì›")
        return True
    except Exception as e:
        print(f"   âŒ Mixed Precision ì‹¤íŒ¨: {e}")
        return False

def check_distributed_training():
    """ë¶„ì‚° í•™ìŠµ ì§€ì› í™•ì¸"""
    print("\nğŸ”— ë¶„ì‚° í•™ìŠµ í™•ì¸")
    
    try:
        import torch.distributed as dist
        
        # NCCL ë°±ì—”ë“œ í™•ì¸
        if torch.distributed.is_nccl_available():
            print("   âœ… NCCL ë°±ì—”ë“œ ì‚¬ìš© ê°€ëŠ¥")
        else:
            print("   âš ï¸  NCCL ë°±ì—”ë“œ ì‚¬ìš© ë¶ˆê°€")
        
        # ë©€í‹° GPU í™•ì¸
        gpu_count = torch.cuda.device_count()
        if gpu_count > 1:
            print(f"   âœ… ë©€í‹° GPU ë¶„ì‚° í•™ìŠµ ê°€ëŠ¥ ({gpu_count}ê°œ)")
            return True
        else:
            print("   âš ï¸  ë‹¨ì¼ GPU (ë¶„ì‚° í•™ìŠµ ë¶ˆê°€)")
            return True
            
    except Exception as e:
        print(f"   âŒ ë¶„ì‚° í•™ìŠµ ì„¤ì • ì‹¤íŒ¨: {e}")
        return False

def check_memory_optimization():
    """ë©”ëª¨ë¦¬ ìµœì í™” ê¸°ëŠ¥ í™•ì¸"""
    print("\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” í™•ì¸")
    
    try:
        import psutil
        import gc
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
        memory = psutil.virtual_memory()
        print(f"   ì‹œìŠ¤í…œ RAM: {memory.total / 1024**3:.1f}GB")
        print(f"   ì‚¬ìš© ê°€ëŠ¥: {memory.available / 1024**3:.1f}GB")
        
        if memory.total >= 64 * 1024**3:  # 64GB
            print("   âœ… ì¶©ë¶„í•œ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬")
        else:
            print("   âš ï¸  ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ë¶€ì¡± (64GB ê¶Œì¥)")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í…ŒìŠ¤íŠ¸
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
            print("   âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ê¸°ëŠ¥ ì •ìƒ")
        
        return True
        
    except Exception as e:
        print(f"   âŒ ë©”ëª¨ë¦¬ ìµœì í™” í™•ì¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ì „ì²´ í™˜ê²½ ê²€ì¦"""
    print("ğŸ¯ 65GB ëŒ€ìš©ëŸ‰ í•™ìŠµ í™˜ê²½ ê²€ì¦")
    print("=" * 60)
    
    checks = [
        check_python_version(),
        check_cuda_setup(),
        check_pytorch_version(),
        check_transformers(),
        check_mixed_precision(),
        check_distributed_training(),
        check_memory_optimization()
    ]
    
    passed = sum(checks)
    total = len(checks)
    
    print("\n" + "=" * 60)
    print(f"ğŸ¯ ê²€ì¦ ê²°ê³¼: {passed}/{total} í†µê³¼")
    
    if passed == total:
        print("âœ… ëª¨ë“  ê²€ì¦ í†µê³¼! 65GB ëŒ€ìš©ëŸ‰ í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ")
        print("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print("   1. 65GB ë°ì´í„°ì…‹ ê²½ë¡œ í™•ì¸")
        print("   2. scripts/training/large_scale_v2_training.py ì‹¤í–‰")
        print("   3. í•™ìŠµ ëª¨ë‹ˆí„°ë§ ë° ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬")
    elif passed >= 5:
        print("âš ï¸  ëŒ€ë¶€ë¶„ ê²€ì¦ í†µê³¼, ì¼ë¶€ ìµœì í™” í•„ìš”")
        print("   ê¸°ë³¸ í•™ìŠµì€ ê°€ëŠ¥í•˜ì§€ë§Œ ì„±ëŠ¥ ìµœì í™” ê¶Œì¥")
    else:
        print("âŒ í™˜ê²½ ì„¤ì • í•„ìš”")
        print("   Python, CUDA, PyTorch ì„¤ì¹˜ ë° ì„¤ì • í™•ì¸ í•„ìš”")

if __name__ == "__main__":
    main()