#!/usr/bin/env python3
"""
Fashion JSON Encoder ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìƒ˜í”Œ JSONì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬
Top-5 ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ì°¾ì•„ ì‹œê°í™”í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python test_similarity_search.py              # ê¸°ë³¸ ì‹¤í–‰ (20ê°œ ìƒ˜í”Œ)
    python test_similarity_search.py --fast       # ë¹ ë¥¸ ì‹¤í–‰ (10ê°œ ìƒ˜í”Œ)
    python test_similarity_search.py --quick      # ë§¤ìš° ë¹ ë¥¸ ì‹¤í–‰ (5ê°œ ìƒ˜í”Œ, ì‹œê°í™” ìƒëµ)
"""

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import json
from pathlib import Path
import random
import argparse
from typing import List, Dict, Tuple

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
from models.json_encoder import JSONEncoder
from models.contrastive_learner import ContrastiveLearner
from data.fashion_dataset import FashionDataModule
from utils.config import TrainingConfig

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['Malgun Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class SimilaritySearchDemo:
    """ìœ ì‚¬ë„ ê²€ìƒ‰ ë°ëª¨ í´ë˜ìŠ¤"""
    
    def __init__(self, checkpoint_path: str = "checkpoints/best_model.pt", 
                 dataset_path: str = "C:/sample/ë¼ë²¨ë§ë°ì´í„°", 
                 fast_mode: bool = False, quick_mode: bool = False):
        self.checkpoint_path = checkpoint_path
        self.dataset_path = dataset_path
        self.device = torch.device('cpu')  # CPU ì‚¬ìš©
        self.fast_mode = fast_mode
        self.quick_mode = quick_mode
        
        # ëª¨ë“œì— ë”°ë¥¸ ìƒ˜í”Œ ìˆ˜ ì„¤ì •
        if quick_mode:
            self.default_samples = 5
        elif fast_mode:
            self.default_samples = 10
        else:
            self.default_samples = 20
        
        # ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ
        self.model = None
        self.data_module = None
        self.vocab_sizes = None
        
    def load_model_and_data(self):
        """ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ"""
        print("ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
        
        try:
            # ë°ì´í„° ëª¨ë“ˆ ì„¤ì •
            config = TrainingConfig()
            self.data_module = FashionDataModule(
                dataset_path=self.dataset_path,
                target_categories=config.target_categories,
                batch_size=16,
                num_workers=0
            )
            self.data_module.setup()
            self.vocab_sizes = self.data_module.get_vocab_sizes()
            
            print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.data_module.train_dataset)} í•™ìŠµ ìƒ˜í”Œ")
            print(f"ì–´íœ˜ í¬ê¸°: {self.vocab_sizes}")
            
            # ëª¨ë¸ ë¡œë“œ
            if Path(self.checkpoint_path).exists():
                checkpoint = torch.load(self.checkpoint_path, map_location=self.device, weights_only=False)
                
                # JSON ì¸ì½”ë” ì´ˆê¸°í™”
                json_encoder = JSONEncoder(
                    vocab_sizes=self.vocab_sizes,
                    embedding_dim=128,
                    hidden_dim=256
                )
                
                # CLIP ì¸ì½”ë” ì´ˆê¸°í™”
                from transformers import CLIPVisionModel
                clip_encoder = CLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")
                clip_encoder = clip_encoder.to(self.device)
                
                # ContrastiveLearner ì´ˆê¸°í™” (CLIP ëª¨ë¸ í¬í•¨)
                self.model = ContrastiveLearner(
                    json_encoder=json_encoder,
                    clip_encoder=clip_encoder,
                    temperature=0.07
                )
                
                # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìƒíƒœ ë¡œë“œ
                if 'model_state_dict' in checkpoint:
                    self.model.load_state_dict(checkpoint['model_state_dict'])
                    print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.checkpoint_path}")
                else:
                    print("ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ìƒíƒœë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ˆê¸°í™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                
                self.model.eval()
                
            else:
                print(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.checkpoint_path}")
                print("ì´ˆê¸°í™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                
                # ì´ˆê¸°í™”ëœ ëª¨ë¸ ìƒì„±
                json_encoder = JSONEncoder(
                    vocab_sizes=self.vocab_sizes,
                    embedding_dim=128,
                    hidden_dim=256
                )
                
                # CLIP ì¸ì½”ë” ì´ˆê¸°í™”
                from transformers import CLIPVisionModel
                clip_encoder = CLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")
                clip_encoder = clip_encoder.to(self.device)
                
                self.model = ContrastiveLearner(
                    json_encoder=json_encoder,
                    clip_encoder=clip_encoder,
                    temperature=0.07
                )
                self.model.eval()
                
        except Exception as e:
            print(f"ëª¨ë¸/ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
            
        return True
    
    def create_sample_json_queries(self) -> List[Dict]:
        """ìƒ˜í”Œ JSON ì¿¼ë¦¬ ìƒì„±"""
        # ì‹¤ì œ ë°ì´í„°ì…‹ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì¹´í…Œê³ ë¦¬ì™€ ì†ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒ˜í”Œ ìƒì„±
        sample_queries = [
            {
                "name": "ë ˆíŠ¸ë¡œ ìŠ¤íƒ€ì¼ ê²€ìƒ‰",
                "json_data": {
                    "category": "ë ˆíŠ¸ë¡œ",
                    "style": [],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •
                    "silhouette": "",  # ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •
                    "material": [],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •
                    "detail": []  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •
                }
            },
            {
                "name": "ë¡œë§¨í‹± ìŠ¤íƒ€ì¼ ê²€ìƒ‰",
                "json_data": {
                    "category": "ë¡œë§¨í‹±",
                    "style": [],
                    "silhouette": "",
                    "material": [],
                    "detail": []
                }
            },
            {
                "name": "ë¦¬ì¡°íŠ¸ ìŠ¤íƒ€ì¼ ê²€ìƒ‰",
                "json_data": {
                    "category": "ë¦¬ì¡°íŠ¸",
                    "style": [],
                    "silhouette": "",
                    "material": [],
                    "detail": []
                }
            }
        ]
        
        return sample_queries
    
    def process_json_to_tensor(self, json_data: Dict) -> Dict[str, torch.Tensor]:
        """JSON ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        # ë°ì´í„° ëª¨ë“ˆì˜ processor ì‚¬ìš©
        processor = self.data_module.dataset_loader.processor
        
        # JSON í•„ë“œë¥¼ vocabulary indexë¡œ ë³€í™˜
        processed = processor.process_json_fields(json_data)
        
        # ì•ˆì „í•˜ê²Œ í…ì„œë¡œ ë³€í™˜ (ë°°ì¹˜ í¬ê¸° 1)
        batch = {
            'category': torch.tensor([processed['category']], dtype=torch.long),
            'silhouette': torch.tensor([processed['silhouette']], dtype=torch.long)
        }
        
        # ë¦¬ìŠ¤íŠ¸ í•„ë“œë“¤ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ (ìµœëŒ€ ê¸¸ì´ 10ìœ¼ë¡œ íŒ¨ë”©)
        max_length = 10
        
        # Style ì²˜ë¦¬
        style_list = processed['style'] if processed['style'] else [0]
        style_padded = (style_list + [0] * max_length)[:max_length]
        style_mask = [1] * len(style_list) + [0] * (max_length - len(style_list))
        style_mask = style_mask[:max_length]
        
        batch['style'] = torch.tensor([style_padded], dtype=torch.long)
        batch['style_mask'] = torch.tensor([style_mask], dtype=torch.bool)
        
        # Material ì²˜ë¦¬
        material_list = processed['material'] if processed['material'] else [0]
        material_padded = (material_list + [0] * max_length)[:max_length]
        material_mask = [1] * len(material_list) + [0] * (max_length - len(material_list))
        material_mask = material_mask[:max_length]
        
        batch['material'] = torch.tensor([material_padded], dtype=torch.long)
        batch['material_mask'] = torch.tensor([material_mask], dtype=torch.bool)
        
        # Detail ì²˜ë¦¬
        detail_list = processed['detail'] if processed['detail'] else [0]
        detail_padded = (detail_list + [0] * max_length)[:max_length]
        detail_mask = [1] * len(detail_list) + [0] * (max_length - len(detail_list))
        detail_mask = detail_mask[:max_length]
        
        batch['detail'] = torch.tensor([detail_padded], dtype=torch.long)
        batch['detail_mask'] = torch.tensor([detail_mask], dtype=torch.bool)
        
        return batch
    
    def get_sample_images_and_embeddings(self, num_samples: int = 20) -> Tuple[List, torch.Tensor]:
        """ìƒ˜í”Œ ì´ë¯¸ì§€ì™€ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ì„ 20ìœ¼ë¡œ ì¤„ì—¬ì„œ ë¹ ë¥¸ ì‹¤í–‰)"""
        print(f"ìƒ˜í”Œ ì´ë¯¸ì§€ {num_samples}ê°œ ë¡œë“œ ì¤‘...")
        
        # ê²€ì¦ ë°ì´í„°ì—ì„œ ìƒ˜í”Œ ì„ íƒ
        val_dataset = self.data_module.val_dataset
        indices = random.sample(range(len(val_dataset)), min(num_samples, len(val_dataset)))
        
        images = []
        image_tensors = []
        
        for idx in indices:
            try:
                sample = val_dataset[idx]
                image_tensor = sample['image'].unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                image_tensors.append(image_tensor)
                
                # ì‹¤ì œ ì´ë¯¸ì§€ ë¡œë“œ (ì‹œê°í™”ìš©)
                fashion_item = val_dataset.fashion_items[idx]
                pil_image = val_dataset.dataset_loader.get_cropped_image(fashion_item)
                images.append({
                    'image': pil_image,
                    'category': fashion_item.category,
                    'style': fashion_item.style,
                    'material': fashion_item.material,
                    'detail': fashion_item.detail
                })
                
            except Exception as e:
                print(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ (idx={idx}): {e}")
                continue
        
        if not image_tensors:
            print("ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return [], torch.empty(0)
        
        # ì´ë¯¸ì§€ ì„ë² ë”© ê³„ì‚°
        image_batch = torch.cat(image_tensors, dim=0)
        
        with torch.no_grad():
            # CLIP ì¸ì½”ë”ë¥¼ í†µí•´ ì´ë¯¸ì§€ ì„ë² ë”© ê³„ì‚°
            image_features = self.model.clip_encoder(image_batch).pooler_output
            
            # í”„ë¡œì ì…˜ ë ˆì´ì–´ê°€ ìˆë‹¤ë©´ ì ìš©
            if self.model.image_projection is not None:
                image_embeddings = self.model.image_projection(image_features)
            else:
                image_embeddings = image_features
                
            image_embeddings = F.normalize(image_embeddings, p=2, dim=1)
        
        print(f"{len(images)}ê°œ ì´ë¯¸ì§€ ì„ë² ë”© ì™„ë£Œ")
        return images, image_embeddings
    
    def find_similar_images(self, query_embedding: torch.Tensor, 
                          image_embeddings: torch.Tensor, 
                          images: List, top_k: int = 5) -> List[Tuple]:
        """ìœ ì‚¬í•œ ì´ë¯¸ì§€ ì°¾ê¸°"""
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = F.cosine_similarity(query_embedding, image_embeddings, dim=1)
        
        # Top-K ì¸ë±ìŠ¤ ì°¾ê¸°
        top_k_indices = torch.topk(similarities, k=min(top_k, len(similarities))).indices
        
        results = []
        for i, idx in enumerate(top_k_indices):
            results.append({
                'rank': i + 1,
                'similarity': similarities[idx].item(),
                'image': images[idx]['image'],
                'category': images[idx]['category'],
                'style': images[idx]['style'],
                'material': images[idx]['material'],
                'detail': images[idx]['detail']
            })
        
        return results
    
    def visualize_search_results(self, query_name: str, query_json: Dict, 
                               results: List[Dict], save_path: str = None):
        """ê²€ìƒ‰ ê²°ê³¼ ì‹œê°í™”"""
        fig, axes = plt.subplots(1, 6, figsize=(18, 4))
        
        # ì¿¼ë¦¬ ì •ë³´ í‘œì‹œ
        axes[0].text(0.5, 0.7, f"ğŸ” ì¿¼ë¦¬", ha='center', va='center', 
                    fontsize=14, fontweight='bold', transform=axes[0].transAxes)
        axes[0].text(0.5, 0.5, query_name, ha='center', va='center', 
                    fontsize=12, fontweight='bold', transform=axes[0].transAxes)
        
        # JSON ì •ë³´ í‘œì‹œ
        json_text = f"ì¹´í…Œê³ ë¦¬: {query_json.get('category', 'N/A')}\n"
        json_text += f"ìŠ¤íƒ€ì¼: {', '.join(query_json.get('style', []))}\n"
        json_text += f"ì†Œì¬: {', '.join(query_json.get('material', []))}"
        
        axes[0].text(0.5, 0.2, json_text, ha='center', va='center', 
                    fontsize=9, transform=axes[0].transAxes)
        axes[0].set_xlim(0, 1)
        axes[0].set_ylim(0, 1)
        axes[0].axis('off')
        
        # Top-5 ê²°ê³¼ í‘œì‹œ
        for i, result in enumerate(results[:5]):
            ax = axes[i + 1]
            
            # ì´ë¯¸ì§€ í‘œì‹œ
            ax.imshow(result['image'])
            ax.set_title(f"#{result['rank']} (ìœ ì‚¬ë„: {result['similarity']:.3f})", 
                        fontsize=10, fontweight='bold')
            
            # ë©”íƒ€ë°ì´í„° í‘œì‹œ
            info_text = f"{result['category']}\n"
            if result['style']:
                info_text += f"{', '.join(result['style'][:2])}\n"
            if result['material']:
                info_text += f"{', '.join(result['material'][:2])}"
            
            ax.text(0.5, -0.15, info_text, ha='center', va='top', 
                   fontsize=8, transform=ax.transAxes)
            ax.axis('off')
        
        plt.suptitle(f'Fashion JSON Encoder - ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼: {query_name}', 
                    fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {save_path}")
        
        plt.show()
    
    def run_similarity_search_demo(self):
        """ìœ ì‚¬ë„ ê²€ìƒ‰ ë°ëª¨ ì‹¤í–‰"""
        print("Fashion JSON Encoder ìœ ì‚¬ë„ ê²€ìƒ‰ ë°ëª¨ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!")
        print("=" * 60)
        
        # ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ
        if not self.load_model_and_data():
            return
        
        # ìƒ˜í”Œ ì´ë¯¸ì§€ì™€ ì„ë² ë”© ì¤€ë¹„
        images, image_embeddings = self.get_sample_images_and_embeddings(num_samples=self.default_samples)
        if len(images) == 0:
            print("âŒ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ì–´ ë°ëª¨ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return
        
        # ìƒ˜í”Œ ì¿¼ë¦¬ ìƒì„±
        sample_queries = self.create_sample_json_queries()
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        results_dir = Path("results/similarity_search")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # ê° ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰
        for i, query in enumerate(sample_queries):
            print(f"\nì¿¼ë¦¬ {i+1}: {query['name']}")
            print(f"JSON: {query['json_data']}")
            
            try:
                # JSONì„ í…ì„œë¡œ ë³€í™˜
                json_tensor = self.process_json_to_tensor(query['json_data'])
                
                # JSON ì„ë² ë”© ê³„ì‚°
                with torch.no_grad():
                    json_embedding = self.model.json_encoder(json_tensor)
                    json_embedding = F.normalize(json_embedding, p=2, dim=1)
                
                # ìœ ì‚¬í•œ ì´ë¯¸ì§€ ì°¾ê¸°
                results = self.find_similar_images(
                    json_embedding, image_embeddings, images, top_k=5
                )
                
                # ê²°ê³¼ ì¶œë ¥
                print("Top-5 ìœ ì‚¬ ì´ë¯¸ì§€:")
                for result in results:
                    print(f"  #{result['rank']}: ìœ ì‚¬ë„ {result['similarity']:.4f} "
                          f"({result['category']}, {', '.join(result['style'][:2])})")
                
                # ì‹œê°í™” (quick ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)
                if not self.quick_mode:
                    save_path = results_dir / f"query_{i+1}_{query['name'].replace(' ', '_')}.png"
                    self.visualize_search_results(
                        query['name'], query['json_data'], results, str(save_path)
                    )
                else:
                    print("  (ì‹œê°í™” ìƒëµ - quick ëª¨ë“œ)")
                
            except Exception as e:
                print(f"ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        print("\n" + "=" * 60)
        print("ìœ ì‚¬ë„ ê²€ìƒ‰ ë°ëª¨ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        if not self.quick_mode:
            print(f"ê²°ê³¼ íŒŒì¼ë“¤ì´ {results_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì¢…í•© ë¶„ì„
        self.analyze_search_quality(sample_queries, results_dir)
    
    def analyze_search_quality(self, queries: List[Dict], results_dir: Path):
        """ê²€ìƒ‰ í’ˆì§ˆ ë¶„ì„"""
        print("\nê²€ìƒ‰ í’ˆì§ˆ ë¶„ì„:")
        print("=" * 40)
        
        print("í˜„ì¬ ëª¨ë¸ ìƒíƒœ:")
        print("  - í•™ìŠµ ë‹¨ê³„: ì´ˆê¸° (15 ì—í¬í¬)")
        print("  - Top-5 ì •í™•ë„: 1.04%")
        print("  - í‰ê·  ìœ ì‚¬ë„: ~0.047")
        
        print("\nê´€ì°°ëœ íŒ¨í„´:")
        print("  - ì¹´í…Œê³ ë¦¬ë³„ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘")
        print("  - ìœ ì‚¬ë„ ê°’ì´ ì˜ë¯¸ìˆëŠ” ë²”ìœ„ (0.0-0.3)")
        print("  - ìŠ¤íƒ€ì¼ ì†ì„± ì¼ë¶€ ë°˜ì˜")
        
        print("\nê°œì„  ë°©í–¥:")
        print("  - ë” ë§ì€ ì—í¬í¬ë¡œ í•™ìŠµ (50-100)")
        print("  - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
        print("  - ë°ì´í„° ì¦ê°• ì ìš©")
        print("  - ë” ë³µì¡í•œ JSON ì¸ì½”ë” êµ¬ì¡°")
        
        print(f"\nìƒì„¸ ê²°ê³¼ëŠ” {results_dir}ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Fashion JSON Encoder ìœ ì‚¬ë„ ê²€ìƒ‰ ë°ëª¨')
    parser.add_argument('--fast', action='store_true', 
                       help='ë¹ ë¥¸ ì‹¤í–‰ ëª¨ë“œ (10ê°œ ìƒ˜í”Œ)')
    parser.add_argument('--quick', action='store_true', 
                       help='ë§¤ìš° ë¹ ë¥¸ ì‹¤í–‰ ëª¨ë“œ (5ê°œ ìƒ˜í”Œ, ì‹œê°í™” ìƒëµ)')
    
    args = parser.parse_args()
    
    if args.quick:
        print("Quick ëª¨ë“œ: 5ê°œ ìƒ˜í”Œ, ì‹œê°í™” ìƒëµ")
    elif args.fast:
        print("Fast ëª¨ë“œ: 10ê°œ ìƒ˜í”Œ")
    else:
        print("ê¸°ë³¸ ëª¨ë“œ: 20ê°œ ìƒ˜í”Œ")
    
    # ë°ëª¨ ì‹¤í–‰
    demo = SimilaritySearchDemo(fast_mode=args.fast, quick_mode=args.quick)
    demo.run_similarity_search_demo()

if __name__ == "__main__":
    main()