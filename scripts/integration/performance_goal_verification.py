#!/usr/bin/env python3
"""
ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ê²€ì¦ ì‹œìŠ¤í…œ

- All Queries Recall@10: 75-80% ëª©í‘œ ë‹¬ì„±
- Anchor Queries Recall@10: 85-92% ëª©í‘œ ë‹¬ì„±  
- ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ì‹œìŠ¤í…œ ì™„ì „ ê²€ì¦

Requirements: ì„±ëŠ¥ ëª©í‘œ
"""

import sys
import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import torch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from main import FashionEncoderSystem
from utils.config import TrainingConfig
from scripts.integration.integrated_pipeline import IntegratedPipeline


class PerformanceGoalVerifier:
    """ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self, dataset_path: str):
        self.dataset_path = dataset_path
        self.performance_targets = {
            'all_queries_recall_10': {'min': 75.0, 'max': 80.0, 'unit': '%'},
            'anchor_queries_recall_10': {'min': 85.0, 'max': 92.0, 'unit': '%'},
            'top5_accuracy': {'min': 70.0, 'max': None, 'unit': '%'},  # ë³´ì¡° ëª©í‘œ
            'centrality_proxy_validation': {'min': 1.0, 'max': None, 'unit': '%p'}  # Anchor > All
        }
        
        self.verification_results = None
        
    def run_comprehensive_verification(self) -> Dict[str, Any]:
        """í¬ê´„ì  ì„±ëŠ¥ ëª©í‘œ ê²€ì¦ ì‹¤í–‰"""
        print("ğŸ¯ ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ê²€ì¦ ì‹œìŠ¤í…œ")
        print("=" * 80)
        print("ğŸ“Š ëª©í‘œ:")
        print("   - All Queries Recall@10: 75-80%")
        print("   - Anchor Queries Recall@10: 85-92%")
        print("   - ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ì‹œìŠ¤í…œ ì™„ì „ ê²€ì¦")
        print("=" * 80)
        
        # ë‹¤ì–‘í•œ ì„¤ì •ìœ¼ë¡œ ê²€ì¦ ì‹¤í–‰
        verification_configs = self._generate_verification_configs()
        
        all_results = {}
        best_config = None
        best_score = 0
        
        for config_name, config in verification_configs.items():
            print(f"\n{'='*60}")
            print(f"ê²€ì¦ ì„¤ì •: {config_name}")
            print(f"{'='*60}")
            
            try:
                # í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                pipeline = IntegratedPipeline(self.dataset_path, config)
                results = pipeline.run_complete_pipeline()
                
                # ì„±ëŠ¥ ê²€ì¦
                verification = self._verify_performance_goals(results, config_name)
                all_results[config_name] = verification
                
                # ìµœê³  ì„±ëŠ¥ ì„¤ì • ì¶”ì 
                if verification['overall_score'] > best_score:
                    best_score = verification['overall_score']
                    best_config = config_name
                
                print(f"âœ… {config_name} ê²€ì¦ ì™„ë£Œ (ì ìˆ˜: {verification['overall_score']:.1f})")
                
            except Exception as e:
                print(f"âŒ {config_name} ê²€ì¦ ì‹¤íŒ¨: {e}")
                all_results[config_name] = {
                    'status': 'failed',
                    'error': str(e),
                    'overall_score': 0
                }
        
        # ì¢…í•© ê²€ì¦ ê²°ê³¼ ìƒì„±
        self.verification_results = self._create_comprehensive_verification_report(
            all_results, best_config
        )
        
        # ê²°ê³¼ ì €ì¥
        self._save_verification_results()
        
        # ìµœì¢… ê²€ì¦ ê²°ê³¼ ì¶œë ¥
        self._print_verification_summary()
        
        return self.verification_results
    
    def _generate_verification_configs(self) -> Dict[str, TrainingConfig]:
        """ê²€ì¦ìš© ë‹¤ì–‘í•œ ì„¤ì • ìƒì„±"""
        configs = {}
        
        # 1. Baseline v1 ì„¤ì • (í˜„ì¬ ìµœì )
        baseline_config = TrainingConfig()
        baseline_config.temperature = 0.1
        baseline_config.batch_size = 32
        baseline_config.max_epochs = 8
        baseline_config.learning_rate = 1e-4
        configs['baseline_v1'] = baseline_config
        
        # 2. ìµœì í™”ëœ ì„¤ì • 1 (ë°°ì¹˜ í¬ê¸° ì¦ê°€)
        optimized_config1 = TrainingConfig()
        optimized_config1.temperature = 0.1
        optimized_config1.batch_size = 64  # ì¦ê°€
        optimized_config1.max_epochs = 8
        optimized_config1.learning_rate = 1e-4
        configs['optimized_batch64'] = optimized_config1
        
        # 3. Temperature ë¯¸ì„¸ ì¡°ì • 1
        temp_config1 = TrainingConfig()
        temp_config1.temperature = 0.08  # ë” ë‚®ì€ temperature
        temp_config1.batch_size = 32
        temp_config1.max_epochs = 8
        temp_config1.learning_rate = 1e-4
        configs['temperature_008'] = temp_config1
        
        # 4. Temperature ë¯¸ì„¸ ì¡°ì • 2
        temp_config2 = TrainingConfig()
        temp_config2.temperature = 0.12  # ë” ë†’ì€ temperature
        temp_config2.batch_size = 32
        temp_config2.max_epochs = 8
        temp_config2.learning_rate = 1e-4
        configs['temperature_012'] = temp_config2
        
        return configs
    
    def _verify_performance_goals(self, pipeline_results: Dict[str, Any], 
                                config_name: str) -> Dict[str, Any]:
        """ì„±ëŠ¥ ëª©í‘œ ê²€ì¦"""
        if 'performance_report' not in pipeline_results:
            return {
                'status': 'failed',
                'error': 'Performance report not found',
                'overall_score': 0
            }
        
        performance_report = pipeline_results['performance_report']
        current_perf = performance_report['performance_summary']['current_performance']
        
        # ê° ëª©í‘œë³„ ê²€ì¦
        verification = {
            'config_name': config_name,
            'timestamp': datetime.now().isoformat(),
            'goals': {},
            'overall_score': 0,
            'status': 'completed'
        }
        
        total_score = 0
        max_score = 0
        
        for goal_name, target in self.performance_targets.items():
            if goal_name == 'centrality_proxy_validation':
                # ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ê²€ì¦ (Anchor > All)
                anchor_recall = current_perf.get('anchor_queries_recall_10', 0)
                all_recall = current_perf.get('all_queries_recall_10', 0)
                actual_value = anchor_recall - all_recall
                achieved = actual_value >= target['min']
                score = 100 if achieved else max(0, actual_value / target['min'] * 100)
            else:
                # ì¼ë°˜ ë©”íŠ¸ë¦­ ê²€ì¦
                actual_value = current_perf.get(goal_name, 0)
                if goal_name.endswith('_recall_10'):
                    actual_value *= 100  # ë°±ë¶„ìœ¨ ë³€í™˜
                
                if target['max'] is None:
                    # ìµœì†Œê°’ë§Œ ìˆëŠ” ê²½ìš°
                    achieved = actual_value >= target['min']
                    score = min(100, actual_value / target['min'] * 100)
                else:
                    # ë²”ìœ„ê°€ ìˆëŠ” ê²½ìš°
                    achieved = target['min'] <= actual_value <= target['max']
                    if achieved:
                        score = 100
                    elif actual_value < target['min']:
                        score = actual_value / target['min'] * 100
                    else:
                        score = max(0, 100 - (actual_value - target['max']) / target['max'] * 50)
            
            verification['goals'][goal_name] = {
                'target': target,
                'actual': actual_value,
                'achieved': achieved,
                'score': score
            }
            
            total_score += score
            max_score += 100
        
        verification['overall_score'] = total_score / max_score * 100 if max_score > 0 else 0
        
        return verification
    
    def _create_comprehensive_verification_report(self, all_results: Dict[str, Any], 
                                                best_config: str) -> Dict[str, Any]:
        """ì¢…í•© ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'verification_summary': {
                'total_configs_tested': len(all_results),
                'successful_configs': len([r for r in all_results.values() if r.get('status') == 'completed']),
                'best_config': best_config,
                'best_score': all_results.get(best_config, {}).get('overall_score', 0) if best_config else 0
            },
            'performance_targets': self.performance_targets,
            'detailed_results': all_results,
            'goal_achievement_analysis': self._analyze_goal_achievement(all_results),
            'recommendations': self._generate_optimization_recommendations(all_results, best_config),
            'next_steps': self._generate_verification_next_steps(all_results, best_config)
        }
        
        return report
    
    def _analyze_goal_achievement(self, all_results: Dict[str, Any]) -> Dict[str, Any]:
        """ëª©í‘œ ë‹¬ì„± ë¶„ì„"""
        analysis = {
            'goals_achieved_by_config': {},
            'best_performance_by_goal': {},
            'achievement_summary': {}
        }
        
        # ê° ì„¤ì •ë³„ ëª©í‘œ ë‹¬ì„± í˜„í™©
        for config_name, result in all_results.items():
            if result.get('status') != 'completed':
                continue
                
            goals = result.get('goals', {})
            achieved_goals = [goal for goal, data in goals.items() if data.get('achieved', False)]
            analysis['goals_achieved_by_config'][config_name] = {
                'achieved_count': len(achieved_goals),
                'total_count': len(goals),
                'achieved_goals': achieved_goals,
                'overall_score': result.get('overall_score', 0)
            }
        
        # ê° ëª©í‘œë³„ ìµœê³  ì„±ëŠ¥
        for goal_name in self.performance_targets.keys():
            best_performance = None
            best_config = None
            
            for config_name, result in all_results.items():
                if result.get('status') != 'completed':
                    continue
                    
                goal_data = result.get('goals', {}).get(goal_name, {})
                actual_value = goal_data.get('actual', 0)
                
                if best_performance is None or actual_value > best_performance:
                    best_performance = actual_value
                    best_config = config_name
            
            analysis['best_performance_by_goal'][goal_name] = {
                'best_value': best_performance,
                'best_config': best_config,
                'target_achieved': best_performance >= self.performance_targets[goal_name]['min'] if best_performance else False
            }
        
        # ì „ì²´ ë‹¬ì„± ìš”ì•½
        total_goals = len(self.performance_targets)
        achieved_goals = len([g for g in analysis['best_performance_by_goal'].values() if g['target_achieved']])
        
        analysis['achievement_summary'] = {
            'total_goals': total_goals,
            'achieved_goals': achieved_goals,
            'achievement_rate': achieved_goals / total_goals * 100 if total_goals > 0 else 0,
            'critical_goals_status': {
                'all_queries_recall_10': analysis['best_performance_by_goal'].get('all_queries_recall_10', {}).get('target_achieved', False),
                'anchor_queries_recall_10': analysis['best_performance_by_goal'].get('anchor_queries_recall_10', {}).get('target_achieved', False)
            }
        }
        
        return analysis
    
    def _generate_optimization_recommendations(self, all_results: Dict[str, Any], 
                                             best_config: str) -> List[Dict[str, str]]:
        """ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if not best_config or not all_results.get(best_config):
            recommendations.append({
                'priority': 'Critical',
                'category': 'ì‹œìŠ¤í…œ ì•ˆì •ì„±',
                'action': 'ëª¨ë“  ì„¤ì •ì—ì„œ ì‹¤íŒ¨ - ê¸°ë³¸ ì‹œìŠ¤í…œ ì ê²€ í•„ìš”',
                'expected_impact': 'ì‹œìŠ¤í…œ ì •ìƒí™”'
            })
            return recommendations
        
        best_result = all_results[best_config]
        best_goals = best_result.get('goals', {})
        
        # All Queries Recall@10 ê°œì„ 
        all_queries_goal = best_goals.get('all_queries_recall_10', {})
        if not all_queries_goal.get('achieved', False):
            current_value = all_queries_goal.get('actual', 0)
            target_value = self.performance_targets['all_queries_recall_10']['min']
            gap = target_value - current_value
            
            if gap > 40:  # 40% ì´ìƒ ì°¨ì´
                recommendations.append({
                    'priority': 'Critical',
                    'category': 'ëª¨ë¸ ì•„í‚¤í…ì²˜',
                    'action': 'JSON Encoder ì°¨ì› í™•ì¥ (128â†’256â†’512) ë° Multi-head Attention ë„ì…',
                    'expected_impact': f'All Queries Recall@10 {gap/2:.1f}% í–¥ìƒ ì˜ˆìƒ'
                })
            elif gap > 20:  # 20% ì´ìƒ ì°¨ì´
                recommendations.append({
                    'priority': 'High',
                    'category': 'í•˜ì´í¼íŒŒë¼ë¯¸í„°',
                    'action': 'ë°°ì¹˜ í¬ê¸° ì¦ê°€ (64â†’128) ë° í•™ìŠµë¥  ì¡°ì •',
                    'expected_impact': f'All Queries Recall@10 {gap/3:.1f}% í–¥ìƒ ì˜ˆìƒ'
                })
            else:
                recommendations.append({
                    'priority': 'Medium',
                    'category': 'Temperature ìµœì í™”',
                    'action': 'Temperature ë¯¸ì„¸ ì¡°ì • (0.05-0.15 ë²”ìœ„ ì„¸ë°€ íƒìƒ‰)',
                    'expected_impact': f'All Queries Recall@10 {gap/2:.1f}% í–¥ìƒ ì˜ˆìƒ'
                })
        
        # Anchor Queries Recall@10 ê°œì„ 
        anchor_queries_goal = best_goals.get('anchor_queries_recall_10', {})
        if not anchor_queries_goal.get('achieved', False):
            current_value = anchor_queries_goal.get('actual', 0)
            target_value = self.performance_targets['anchor_queries_recall_10']['min']
            gap = target_value - current_value
            
            recommendations.append({
                'priority': 'High',
                'category': 'ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ìµœì í™”',
                'action': f'Anchor Set ë¹„ìœ¨ ì¡°ì • (5%, 15%, 20% ì‹¤í—˜) ë° ì¤‘ì‹¬ì„± ê³„ì‚° ë°©ë²• ê°œì„ ',
                'expected_impact': f'Anchor Queries Recall@10 {gap/2:.1f}% í–¥ìƒ ì˜ˆìƒ'
            })
        
        # ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ê²€ì¦
        proxy_goal = best_goals.get('centrality_proxy_validation', {})
        if not proxy_goal.get('achieved', False):
            recommendations.append({
                'priority': 'Medium',
                'category': 'ì¤‘ì‹¬ì„± ë¶„ì„',
                'action': 'ê¸€ë¡œë²Œ ì¤‘ì‹¬ ë²¡í„° ê³„ì‚° ë°©ë²• ê°œì„  (ê°€ì¤‘ í‰ê· , í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜)',
                'expected_impact': 'ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ê°€ì„¤ ê²€ì¦ ì„±ê³µ'
            })
        
        return recommendations
    
    def _generate_verification_next_steps(self, all_results: Dict[str, Any], 
                                        best_config: str) -> List[str]:
        """ê²€ì¦ ê¸°ë°˜ ë‹¤ìŒ ë‹¨ê³„ ìƒì„±"""
        next_steps = []
        
        if not best_config:
            next_steps.extend([
                "1. ì‹œìŠ¤í…œ ê¸°ë³¸ ì„¤ì • ì ê²€ ë° ë””ë²„ê¹…",
                "2. ë°ì´í„° ë¡œë”© ë° ëª¨ë¸ ì´ˆê¸°í™” ë¬¸ì œ í•´ê²°",
                "3. ê¸°ë³¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì•ˆì •í™”"
            ])
            return next_steps
        
        best_result = all_results[best_config]
        best_score = best_result.get('overall_score', 0)
        
        if best_score < 30:  # ë§¤ìš° ë‚®ì€ ì„±ëŠ¥
            next_steps.extend([
                "1. ê¸°ë³¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì¬ê²€í† ",
                "2. ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ê°œì„ ",
                "3. í•™ìŠµ ì•ˆì •ì„± í™•ë³´"
            ])
        elif best_score < 60:  # ì¤‘ê°„ ì„±ëŠ¥
            next_steps.extend([
                "1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ëŒ€ê·œëª¨ íŠœë‹",
                "2. ëª¨ë¸ ì•„í‚¤í…ì²˜ ê°œì„  (Multi-head Attention)",
                "3. ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©"
            ])
        else:  # ë†’ì€ ì„±ëŠ¥
            next_steps.extend([
                "1. ë¯¸ì„¸ ì¡°ì •ì„ í†µí•œ ì„±ëŠ¥ ìµœì í™”",
                "2. ì•™ìƒë¸” ê¸°ë²• ì ìš©",
                "3. ì‹¤ì‹œê°„ API ì‹œìŠ¤í…œ êµ¬ì¶• ì¤€ë¹„"
            ])
        
        # ê³µí†µ ë‹¤ìŒ ë‹¨ê³„
        next_steps.extend([
            "4. ë…¼ë¬¸/ì¡¸ì—…ì‘í’ˆ ê²°ê³¼ ì •ë¦¬",
            "5. ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ì‹œìŠ¤í…œ ìƒìš©í™” ê²€í† ",
            "6. ì¶”ê°€ ë°ì´í„°ì…‹ í™•ì¥ ì‹¤í—˜"
        ])
        
        return next_steps
    
    def _save_verification_results(self) -> None:
        """ê²€ì¦ ê²°ê³¼ ì €ì¥"""
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        # JSON ê²°ê³¼ ì €ì¥
        json_file = results_dir / "performance_goal_verification_results.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.verification_results, f, indent=2, ensure_ascii=False)
        
        # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        markdown_file = results_dir / "performance_goal_verification_report.md"
        self._generate_verification_markdown_report(markdown_file)
        
        print(f"ğŸ’¾ ê²€ì¦ ê²°ê³¼ ì €ì¥: {json_file}")
        print(f"ğŸ“„ ê²€ì¦ ë³´ê³ ì„œ ì €ì¥: {markdown_file}")
    
    def _generate_verification_markdown_report(self, output_path: Path) -> None:
        """ê²€ì¦ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±"""
        report = self.verification_results
        
        markdown_content = f"""# ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ê²€ì¦ ë³´ê³ ì„œ

ìƒì„±ì¼ì‹œ: {report['timestamp']}

## ğŸ¯ ê²€ì¦ ê°œìš”

### ì„±ëŠ¥ ëª©í‘œ
- **All Queries Recall@10**: 75-80%
- **Anchor Queries Recall@10**: 85-92%
- **ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ê²€ì¦**: Anchor > All Queries

### ê²€ì¦ ê²°ê³¼ ìš”ì•½
- **í…ŒìŠ¤íŠ¸ëœ ì„¤ì •**: {report['verification_summary']['total_configs_tested']}ê°œ
- **ì„±ê³µí•œ ì„¤ì •**: {report['verification_summary']['successful_configs']}ê°œ
- **ìµœê³  ì„±ëŠ¥ ì„¤ì •**: {report['verification_summary']['best_config']}
- **ìµœê³  ì ìˆ˜**: {report['verification_summary']['best_score']:.1f}ì 

## ğŸ“Š ëª©í‘œ ë‹¬ì„± ë¶„ì„

### ì „ì²´ ë‹¬ì„± í˜„í™©
- **ë‹¬ì„±ëœ ëª©í‘œ**: {report['goal_achievement_analysis']['achievement_summary']['achieved_goals']}/{report['goal_achievement_analysis']['achievement_summary']['total_goals']}ê°œ
- **ë‹¬ì„±ë¥ **: {report['goal_achievement_analysis']['achievement_summary']['achievement_rate']:.1f}%

### í•µì‹¬ ëª©í‘œ ìƒíƒœ
"""
        
        critical_goals = report['goal_achievement_analysis']['achievement_summary']['critical_goals_status']
        for goal, achieved in critical_goals.items():
            status = "âœ… ë‹¬ì„±" if achieved else "âŒ ë¯¸ë‹¬ì„±"
            markdown_content += f"- **{goal}**: {status}\n"
        
        markdown_content += f"""
## ğŸ† ìµœê³  ì„±ëŠ¥ ë¶„ì„

### ì„¤ì •ë³„ ì„±ê³¼
"""
        
        for config_name, config_data in report['goal_achievement_analysis']['goals_achieved_by_config'].items():
            markdown_content += f"""
#### {config_name}
- ë‹¬ì„± ëª©í‘œ: {config_data['achieved_count']}/{config_data['total_count']}ê°œ
- ì „ì²´ ì ìˆ˜: {config_data['overall_score']:.1f}ì 
- ë‹¬ì„±í•œ ëª©í‘œ: {', '.join(config_data['achieved_goals']) if config_data['achieved_goals'] else 'ì—†ìŒ'}
"""
        
        markdown_content += f"""
## ğŸ”§ ìµœì í™” ê¶Œì¥ì‚¬í•­

"""
        for rec in report['recommendations']:
            markdown_content += f"""
### {rec['category']} (ìš°ì„ ìˆœìœ„: {rec['priority']})
- **ì•¡ì…˜**: {rec['action']}
- **ì˜ˆìƒ íš¨ê³¼**: {rec['expected_impact']}
"""
        
        markdown_content += f"""
## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

"""
        for i, step in enumerate(report['next_steps'], 1):
            markdown_content += f"{step}\n"
        
        markdown_content += f"""
---

*ì´ ë³´ê³ ì„œëŠ” ì„±ëŠ¥ ëª©í‘œ ê²€ì¦ ì‹œìŠ¤í…œì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
    
    def _print_verification_summary(self) -> None:
        """ê²€ì¦ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ¯ ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ê²€ì¦ ìµœì¢… ìš”ì•½")
        print("=" * 80)
        
        if not self.verification_results:
            print("âŒ ê²€ì¦ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        summary = self.verification_results['verification_summary']
        achievement = self.verification_results['goal_achievement_analysis']['achievement_summary']
        
        print(f"\nğŸ“Š ê²€ì¦ í†µê³„:")
        print(f"   í…ŒìŠ¤íŠ¸ëœ ì„¤ì •: {summary['total_configs_tested']}ê°œ")
        print(f"   ì„±ê³µí•œ ì„¤ì •: {summary['successful_configs']}ê°œ")
        print(f"   ìµœê³  ì„±ëŠ¥ ì„¤ì •: {summary['best_config']}")
        print(f"   ìµœê³  ì ìˆ˜: {summary['best_score']:.1f}ì ")
        
        print(f"\nğŸ¯ ëª©í‘œ ë‹¬ì„± í˜„í™©:")
        print(f"   ë‹¬ì„±ëœ ëª©í‘œ: {achievement['achieved_goals']}/{achievement['total_goals']}ê°œ")
        print(f"   ë‹¬ì„±ë¥ : {achievement['achievement_rate']:.1f}%")
        
        critical_goals = achievement['critical_goals_status']
        print(f"\nâœ… í•µì‹¬ ëª©í‘œ:")
        for goal, achieved in critical_goals.items():
            status = "ë‹¬ì„±" if achieved else "ë¯¸ë‹¬ì„±"
            print(f"   {goal}: {status}")
        
        print(f"\nğŸ”§ ìš°ì„  ê¶Œì¥ì‚¬í•­:")
        high_priority_recs = [r for r in self.verification_results['recommendations'] if r['priority'] in ['Critical', 'High']]
        for rec in high_priority_recs[:3]:  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
            print(f"   {rec['action']}")


def run_performance_goal_verification():
    """ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ê²€ì¦ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ Fashion JSON Encoder - ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ê²€ì¦")
    print("=" * 80)
    print("ğŸ“Œ ëª©í‘œ: All Queries Recall@10 75-80%, Anchor Queries Recall@10 85-92%")
    print("ğŸ” ë‹¤ì–‘í•œ ì„¤ì •ìœ¼ë¡œ í¬ê´„ì  ê²€ì¦ ìˆ˜í–‰")
    print("=" * 80)
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ
    dataset_path = "C:/sample/ë¼ë²¨ë§ë°ì´í„°"
    
    try:
        # ì„±ëŠ¥ ëª©í‘œ ê²€ì¦ ì‹¤í–‰
        verifier = PerformanceGoalVerifier(dataset_path)
        results = verifier.run_comprehensive_verification()
        
        print(f"\nğŸ‰ ì„±ëŠ¥ ëª©í‘œ ê²€ì¦ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼:")
        print(f"   - results/performance_goal_verification_results.json")
        print(f"   - results/performance_goal_verification_report.md")
        
        return results
        
    except Exception as e:
        print(f"\nâŒ ì„±ëŠ¥ ëª©í‘œ ê²€ì¦ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    run_performance_goal_verification()