#!/usr/bin/env python3
"""
í†µí•© íŒŒì´í”„ë¼ì¸ êµ¬í˜„

ì¤‘ì‹¬ì„± ë¶„ì„ â†’ Query-Aware í‰ê°€ â†’ ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
ìë™í™”ëœ ì‹¤í—˜ ë° ë¶„ì„ ì›Œí¬í”Œë¡œìš°

Requirements: ì „ì²´ ì‹œìŠ¤í…œ í†µí•©
"""

import sys
import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import torch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from main import FashionEncoderSystem
from utils.config import TrainingConfig
from scripts.analysis.embedding_centrality_proxy import EmbeddingCentralityProxy
from scripts.analysis.anchor_based_evaluation import AnchorBasedEvaluator


class IntegratedPipeline:
    """í†µí•© íŒŒì´í”„ë¼ì¸: ì¤‘ì‹¬ì„± ë¶„ì„ â†’ Query-Aware í‰ê°€ â†’ ì„±ëŠ¥ ë³´ê³ ì„œ"""
    
    def __init__(self, dataset_path: str, config: Optional[TrainingConfig] = None):
        self.dataset_path = dataset_path
        self.config = config or self._create_optimized_config()
        self.system = None
        
        # ê²°ê³¼ ì €ì¥
        self.centrality_results = None
        self.evaluation_results = None
        self.performance_report = None
        
    def _create_optimized_config(self) -> TrainingConfig:
        """ìµœì í™”ëœ ì„¤ì • ìƒì„±"""
        config = TrainingConfig()
        config.temperature = 0.1  # Baseline v1 ìµœì  ì„¤ì •
        config.batch_size = 32    # Recall@10 ê³„ì‚°ì„ ìœ„í•´ ì¦ê°€
        config.max_epochs = 8
        config.learning_rate = 1e-4
        return config
    
    def initialize_system(self) -> None:
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ì„¤ì •"""
        print("ğŸš€ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print("=" * 60)
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.system = FashionEncoderSystem()
        self.system.config = self.config
        
        # ë°ì´í„° ì„¤ì •
        print("ğŸ“ ë°ì´í„° ì„¤ì • ì¤‘...")
        self.system.setup_data(self.dataset_path)
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        print("ğŸ‹ï¸ íŠ¸ë ˆì´ë„ˆ ì„¤ì • ì¤‘...")
        self.system.setup_trainer()
        
        # ìµœì  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        self._load_best_checkpoint()
        
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
    def _load_best_checkpoint(self) -> None:
        """ìµœì  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint_candidates = [
            "checkpoints/baseline_v1_best_model.pt",
            "checkpoints/baseline_v2_best_model.pt", 
            "checkpoints/best_model.pt"
        ]
        
        for checkpoint_path in checkpoint_candidates:
            if Path(checkpoint_path).exists():
                print(f"ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
                self.system.trainer.load_checkpoint(checkpoint_path)
                return
        
        print("âš ï¸ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ëª¨ë¸ ìƒíƒœë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    def run_centrality_analysis(self) -> Dict[str, Any]:
        """STEP 1: ì„ë² ë”© ì¤‘ì‹¬ì„± ë¶„ì„ ì‹¤í–‰"""
        print("\n" + "=" * 60)
        print("STEP 1: ì„ë² ë”© ì¤‘ì‹¬ì„± ê¸°ë°˜ ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ë¶„ì„")
        print("=" * 60)
        print("ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´: 'ë² ìŠ¤íŠ¸ì…€ëŸ¬ë¥¼ íŒë§¤ ë°ì´í„° ì—†ì´, ì„ë² ë”© ê³µê°„ì˜ ì¤‘ì‹¬ì„±ìœ¼ë¡œ ê·¼ì‚¬'")
        
        # ì¤‘ì‹¬ì„± ë¶„ì„ ì‹¤í–‰
        analyzer = EmbeddingCentralityProxy(self.system)
        self.centrality_results = analyzer.run_complete_analysis(
            anchor_percentile=90,  # ìƒìœ„ 10%
            tail_percentile=50     # í•˜ìœ„ 50%
        )
        
        # ê²°ê³¼ ì €ì¥
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        centrality_file = results_dir / "integrated_centrality_analysis.json"
        with open(centrality_file, 'w', encoding='utf-8') as f:
            json.dump(self.centrality_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ì¤‘ì‹¬ì„± ë¶„ì„ ê²°ê³¼ ì €ì¥: {centrality_file}")
        
        return self.centrality_results
    
    def run_query_aware_evaluation(self) -> Dict[str, Any]:
        """STEP 2: Query-Aware í‰ê°€ ì‹¤í–‰"""
        print("\n" + "=" * 60)
        print("STEP 2: Query-Aware í‰ê°€ ì‹œìŠ¤í…œ")
        print("=" * 60)
        print("ğŸ¯ ëª©í‘œ: Anchor Queries Recall@10 â‰¥ 85% ë‹¬ì„±")
        
        if self.centrality_results is None:
            raise ValueError("ë¨¼ì € ì¤‘ì‹¬ì„± ë¶„ì„ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # Anchor & Tail ì¸ë±ìŠ¤ ì¶”ì¶œ
        anchor_indices = self.centrality_results['sets_info']['anchor_indices']
        tail_indices = self.centrality_results['sets_info']['tail_indices']
        
        print(f"ğŸ“Š Anchor Set: {len(anchor_indices)}ê°œ (ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy)")
        print(f"ğŸ“Š Tail Set: {len(tail_indices)}ê°œ")
        
        # Query-Aware í‰ê°€ ì‹¤í–‰
        evaluator = AnchorBasedEvaluator(self.system, anchor_indices, tail_indices)
        self.evaluation_results = evaluator.run_anchor_based_evaluation()
        
        # ê²°ê³¼ ì €ì¥
        results_dir = Path("results")
        evaluation_file = results_dir / "integrated_query_aware_evaluation.json"
        with open(evaluation_file, 'w', encoding='utf-8') as f:
            json.dump(self.evaluation_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ Query-Aware í‰ê°€ ê²°ê³¼ ì €ì¥: {evaluation_file}")
        
        return self.evaluation_results
    
    def generate_performance_report(self) -> Dict[str, Any]:
        """STEP 3: í¬ê´„ì  ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "=" * 60)
        print("STEP 3: í¬ê´„ì  ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±")
        print("=" * 60)
        
        if self.centrality_results is None or self.evaluation_results is None:
            raise ValueError("ë¨¼ì € ì¤‘ì‹¬ì„± ë¶„ì„ê³¼ Query-Aware í‰ê°€ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
        self.performance_report = self._create_comprehensive_report()
        
        # ê²°ê³¼ ì €ì¥
        results_dir = Path("results")
        report_file = results_dir / "integrated_performance_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.performance_report, f, indent=2, ensure_ascii=False)
        
        # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        markdown_file = results_dir / "integrated_performance_report.md"
        self._generate_markdown_report(markdown_file)
        
        print(f"ğŸ’¾ ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥: {report_file}")
        print(f"ğŸ“„ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ì €ì¥: {markdown_file}")
        
        return self.performance_report
    
    def _create_comprehensive_report(self) -> Dict[str, Any]:
        """í¬ê´„ì  ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        # í‰ê°€ ê²°ê³¼ì—ì„œ ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ
        eval_summary = self.evaluation_results.get('summary', {})
        
        # ëª©í‘œ ë‹¬ì„± ë¶„ì„
        goal_achievement = eval_summary.get('goal_achievement', {})
        
        # ì¤‘ì‹¬ì„± ë¶„ì„ í†µê³„
        centrality_stats = self.centrality_results.get('centrality_analysis', {}).get('statistics', {})
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        category_analysis = self._analyze_category_performance()
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        recommendations = self._generate_recommendations()
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'pipeline_version': '1.0',
            'system_configuration': {
                'temperature': self.config.temperature,
                'batch_size': self.config.batch_size,
                'dataset_items': self.centrality_results.get('embedding_info', {}).get('num_items', 0),
                'anchor_set_size': len(self.centrality_results.get('sets_info', {}).get('anchor_indices', [])),
                'tail_set_size': len(self.centrality_results.get('sets_info', {}).get('tail_indices', []))
            },
            'performance_summary': {
                'current_performance': {
                    'all_queries_recall_10': eval_summary.get('all_queries', {}).get('recall_at_10', 0),
                    'anchor_queries_recall_10': eval_summary.get('anchor_queries', {}).get('recall_at_10', 0),
                    'tail_queries_recall_10': eval_summary.get('tail_queries', {}).get('recall_at_10', 0),
                    'top5_accuracy': eval_summary.get('all_queries', {}).get('recall_at_5', 0),
                    'top1_accuracy': eval_summary.get('all_queries', {}).get('top1_accuracy', 0)
                },
                'target_performance': {
                    'all_queries_recall_10_target': '75-80%',
                    'anchor_queries_recall_10_target': '85-92%'
                },
                'goal_achievement': {
                    'all_queries_achieved': goal_achievement.get('all_queries_achieved', False),
                    'anchor_queries_achieved': goal_achievement.get('anchor_achieved', False),
                    'improvement_needed': goal_achievement.get('improvement', 0)
                }
            },
            'centrality_analysis_summary': {
                'mean_centrality': centrality_stats.get('mean', 0),
                'centrality_range': [centrality_stats.get('min', 0), centrality_stats.get('max', 0)],
                'anchor_threshold': self.centrality_results.get('sets_info', {}).get('anchor_threshold', 0),
                'proxy_validation': eval_summary.get('anchor_queries', {}).get('recall_at_10', 0) > eval_summary.get('all_queries', {}).get('recall_at_10', 0)
            },
            'category_analysis': category_analysis,
            'key_insights': self._extract_key_insights(),
            'recommendations': recommendations,
            'next_steps': self._generate_next_steps()
        }
        
        return report
    
    def _analyze_category_performance(self) -> Dict[str, Any]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„"""
        category_stats = self.centrality_results.get('distribution_analysis', {}).get('category_stats', {})
        anchor_categories = self.centrality_results.get('sets_info', {}).get('anchor_categories', {})
        all_categories = self.centrality_results.get('sets_info', {}).get('all_categories', {})
        
        analysis = {}
        for category in category_stats.keys():
            anchor_count = anchor_categories.get(category, 0)
            total_count = all_categories.get(category, 1)
            anchor_ratio = anchor_count / total_count * 100
            
            analysis[category] = {
                'centrality_mean': category_stats[category]['mean'],
                'centrality_std': category_stats[category]['std'],
                'total_items': total_count,
                'anchor_items': anchor_count,
                'anchor_ratio': anchor_ratio,
                'popularity_rank': 0  # ë‚˜ì¤‘ì— ê³„ì‚°
            }
        
        # ì¸ê¸°ë„ ìˆœìœ„ ê³„ì‚° (ì¤‘ì‹¬ì„± í‰ê·  ê¸°ì¤€)
        sorted_categories = sorted(analysis.items(), key=lambda x: x[1]['centrality_mean'], reverse=True)
        for rank, (category, data) in enumerate(sorted_categories, 1):
            analysis[category]['popularity_rank'] = rank
        
        return analysis
    
    def _extract_key_insights(self) -> List[str]:
        """í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        insights = []
        
        # ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ê²€ì¦
        eval_summary = self.evaluation_results.get('summary', {})
        anchor_recall = eval_summary.get('anchor_queries', {}).get('recall_at_10', 0)
        all_recall = eval_summary.get('all_queries', {}).get('recall_at_10', 0)
        
        if anchor_recall > all_recall:
            improvement = anchor_recall - all_recall
            insights.append(f"âœ… ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ê°€ì„¤ ê²€ì¦: Anchor Queriesê°€ {improvement:.1f}%p ë” ë†’ì€ ì„±ëŠ¥")
        else:
            insights.append("âŒ ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ê°€ì„¤ ë¯¸ê²€ì¦: ì¶”ê°€ ìµœì í™” í•„ìš”")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì¸ì‚¬ì´íŠ¸
        category_analysis = self._analyze_category_performance()
        most_popular = max(category_analysis.items(), key=lambda x: x[1]['centrality_mean'])
        least_popular = min(category_analysis.items(), key=lambda x: x[1]['centrality_mean'])
        
        insights.append(f"ğŸ“Š ê°€ì¥ ëŒ€ì¤‘ì  ì¹´í…Œê³ ë¦¬: {most_popular[0]} (ì¤‘ì‹¬ì„±: {most_popular[1]['centrality_mean']:.4f})")
        insights.append(f"ğŸ“Š ê°€ì¥ ë…íŠ¹í•œ ì¹´í…Œê³ ë¦¬: {least_popular[0]} (ì¤‘ì‹¬ì„±: {least_popular[1]['centrality_mean']:.4f})")
        
        # ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± í˜„í™©
        goal_achievement = eval_summary.get('goal_achievement', {})
        if goal_achievement.get('anchor_achieved', False):
            insights.append("ğŸ¯ Anchor Queries ëª©í‘œ ë‹¬ì„±: 85-92% ë²”ìœ„ ë‚´")
        else:
            current = goal_achievement.get('anchor_actual', '0%')
            insights.append(f"ğŸ¯ Anchor Queries ëª©í‘œ ë¯¸ë‹¬ì„±: í˜„ì¬ {current}, ëª©í‘œ 85-92%")
        
        return insights
    
    def _generate_recommendations(self) -> List[Dict[str, str]]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        eval_summary = self.evaluation_results.get('summary', {})
        anchor_recall = eval_summary.get('anchor_queries', {}).get('recall_at_10', 0)
        all_recall = eval_summary.get('all_queries', {}).get('recall_at_10', 0)
        
        # ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­
        if anchor_recall < 85:
            recommendations.append({
                'category': 'ëª¨ë¸ ìµœì í™”',
                'priority': 'High',
                'action': 'Temperature ë¯¸ì„¸ ì¡°ì • (0.08, 0.09, 0.11, 0.12 ì‹¤í—˜)',
                'expected_impact': 'Anchor Queries Recall@10 5-10% í–¥ìƒ'
            })
            
            recommendations.append({
                'category': 'ì•„í‚¤í…ì²˜ ê°œì„ ',
                'priority': 'Medium',
                'action': 'JSON Encoder ì°¨ì› í™•ì¥ (128â†’256)',
                'expected_impact': 'ì „ì²´ì ì¸ ì„ë² ë”© í’ˆì§ˆ í–¥ìƒ'
            })
        
        if all_recall < 75:
            recommendations.append({
                'category': 'ë°ì´í„° ìµœì í™”',
                'priority': 'High',
                'action': 'ë°°ì¹˜ í¬ê¸° ì¦ê°€ (32â†’64) ë° ì „ì²´ ë°ì´í„° í™œìš©',
                'expected_impact': 'All Queries Recall@10 10-15% í–¥ìƒ'
            })
        
        # ì¤‘ì‹¬ì„± ë¶„ì„ ê°œì„ 
        centrality_stats = self.centrality_results.get('centrality_analysis', {}).get('statistics', {})
        if centrality_stats.get('std', 0) > 0.06:
            recommendations.append({
                'category': 'ì¤‘ì‹¬ì„± ë¶„ì„',
                'priority': 'Medium',
                'action': 'Anchor Set ë¹„ìœ¨ ì¡°ì • (5%, 15% ì‹¤í—˜)',
                'expected_impact': 'ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ì •í™•ë„ í–¥ìƒ'
            })
        
        return recommendations
    
    def _generate_next_steps(self) -> List[str]:
        """ë‹¤ìŒ ë‹¨ê³„ ìƒì„±"""
        next_steps = []
        
        eval_summary = self.evaluation_results.get('summary', {})
        goal_achievement = eval_summary.get('goal_achievement', {})
        
        if not goal_achievement.get('anchor_achieved', False):
            next_steps.append("1. Temperature ìµœì í™” ì‹¤í—˜ (0.08-0.12 ë²”ìœ„)")
            next_steps.append("2. ë°°ì¹˜ í¬ê¸° ì¦ê°€ ë° ì „ì²´ ë°ì´í„° í™œìš©")
            next_steps.append("3. JSON Encoder ì•„í‚¤í…ì²˜ ê°œì„ ")
        
        if not goal_achievement.get('all_queries_achieved', False):
            next_steps.append("4. ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©")
            next_steps.append("5. Multi-head Attention ë©”ì»¤ë‹ˆì¦˜ ë„ì…")
        
        next_steps.extend([
            "6. ì‹¤ì‹œê°„ ì¶”ì²œ API ì‹œìŠ¤í…œ êµ¬ì¶•",
            "7. ë…¼ë¬¸/ì¡¸ì—…ì‘í’ˆ ê²°ê³¼ ì •ë¦¬",
            "8. ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ì‹œìŠ¤í…œ ìƒìš©í™” ê²€í† "
        ])
        
        return next_steps
    
    def _generate_markdown_report(self, output_path: Path) -> None:
        """ë§ˆí¬ë‹¤ìš´ í˜•íƒœì˜ ë³´ê³ ì„œ ìƒì„±"""
        report = self.performance_report
        
        markdown_content = f"""# í†µí•© íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ë³´ê³ ì„œ

ìƒì„±ì¼ì‹œ: {report['timestamp']}
íŒŒì´í”„ë¼ì¸ ë²„ì „: {report['pipeline_version']}

## ğŸ¯ í•µì‹¬ ì„±ê³¼ ìš”ì•½

### í˜„ì¬ ì„±ëŠ¥
- **All Queries Recall@10**: {report['performance_summary']['current_performance']['all_queries_recall_10']:.1f}%
- **Anchor Queries Recall@10**: {report['performance_summary']['current_performance']['anchor_queries_recall_10']:.1f}%
- **Top-5 ì •í™•ë„**: {report['performance_summary']['current_performance']['top5_accuracy']:.1f}%
- **Top-1 ì •í™•ë„**: {report['performance_summary']['current_performance']['top1_accuracy']:.1f}%

### ëª©í‘œ ë‹¬ì„± í˜„í™©
- **All Queries ëª©í‘œ**: {report['performance_summary']['target_performance']['all_queries_recall_10_target']} 
  â†’ {'âœ… ë‹¬ì„±' if report['performance_summary']['goal_achievement']['all_queries_achieved'] else 'âŒ ë¯¸ë‹¬ì„±'}
- **Anchor Queries ëª©í‘œ**: {report['performance_summary']['target_performance']['anchor_queries_recall_10_target']} 
  â†’ {'âœ… ë‹¬ì„±' if report['performance_summary']['goal_achievement']['anchor_queries_achieved'] else 'âŒ ë¯¸ë‹¬ì„±'}

## ğŸ“Š ì¤‘ì‹¬ì„± ë¶„ì„ ê²°ê³¼

### ë² ìŠ¤íŠ¸ì…€ëŸ¬ Proxy ì‹œìŠ¤í…œ
- **í‰ê·  ì¤‘ì‹¬ì„±**: {report['centrality_analysis_summary']['mean_centrality']:.4f}
- **ì¤‘ì‹¬ì„± ë²”ìœ„**: [{report['centrality_analysis_summary']['centrality_range'][0]:.4f}, {report['centrality_analysis_summary']['centrality_range'][1]:.4f}]
- **Anchor ì„ê³„ê°’**: {report['centrality_analysis_summary']['anchor_threshold']:.4f}
- **Proxy ê²€ì¦**: {'âœ… ì„±ê³µ' if report['centrality_analysis_summary']['proxy_validation'] else 'âŒ ì‹¤íŒ¨'}

### ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
"""
        
        for category, data in report['category_analysis'].items():
            markdown_content += f"""
#### {category}
- ì¤‘ì‹¬ì„±: {data['centrality_mean']:.4f} Â± {data['centrality_std']:.4f}
- ì „ì²´ ì•„ì´í…œ: {data['total_items']}ê°œ
- Anchor ì•„ì´í…œ: {data['anchor_items']}ê°œ ({data['anchor_ratio']:.1f}%)
- ì¸ê¸°ë„ ìˆœìœ„: {data['popularity_rank']}ìœ„
"""
        
        markdown_content += f"""
## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

"""
        for insight in report['key_insights']:
            markdown_content += f"- {insight}\n"
        
        markdown_content += f"""
## ğŸ”§ ê°œì„  ê¶Œì¥ì‚¬í•­

"""
        for rec in report['recommendations']:
            markdown_content += f"""
### {rec['category']} (ìš°ì„ ìˆœìœ„: {rec['priority']})
- **ì•¡ì…˜**: {rec['action']}
- **ì˜ˆìƒ íš¨ê³¼**: {rec['expected_impact']}
"""
        
        markdown_content += f"""
## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

"""
        for step in report['next_steps']:
            markdown_content += f"{step}\n"
        
        markdown_content += f"""
## ğŸ“ˆ ì‹œìŠ¤í…œ ì„¤ì •

- **Temperature**: {report['system_configuration']['temperature']}
- **Batch Size**: {report['system_configuration']['batch_size']}
- **Dataset Items**: {report['system_configuration']['dataset_items']:,}ê°œ
- **Anchor Set**: {report['system_configuration']['anchor_set_size']}ê°œ
- **Tail Set**: {report['system_configuration']['tail_set_size']}ê°œ

---

*ì´ ë³´ê³ ì„œëŠ” í†µí•© íŒŒì´í”„ë¼ì¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
    
    def run_complete_pipeline(self) -> Dict[str, Any]:
        """ì „ì²´ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸš€ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 80)
        print("ğŸ¯ ëª©í‘œ: ì¤‘ì‹¬ì„± ë¶„ì„ â†’ Query-Aware í‰ê°€ â†’ ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±")
        print("ğŸ”„ ìë™í™”ëœ ì‹¤í—˜ ë° ë¶„ì„ ì›Œí¬í”Œë¡œìš°")
        print("=" * 80)
        
        try:
            # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            self.initialize_system()
            
            # STEP 1: ì¤‘ì‹¬ì„± ë¶„ì„
            centrality_results = self.run_centrality_analysis()
            
            # STEP 2: Query-Aware í‰ê°€
            evaluation_results = self.run_query_aware_evaluation()
            
            # STEP 3: ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
            performance_report = self.generate_performance_report()
            
            # ìµœì¢… ê²°ê³¼ ì¶œë ¥
            self._print_final_summary()
            
            # ì •ë¦¬
            self.system.cleanup()
            
            print("\nâœ¨ í†µí•© íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            
            return {
                'centrality_results': centrality_results,
                'evaluation_results': evaluation_results,
                'performance_report': performance_report
            }
            
        except Exception as e:
            print(f"\nâŒ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            if self.system:
                self.system.cleanup()
            raise
    
    def _print_final_summary(self) -> None:
        """ìµœì¢… ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ“Š í†µí•© íŒŒì´í”„ë¼ì¸ ìµœì¢… ìš”ì•½")
        print("=" * 80)
        
        if self.performance_report:
            current_perf = self.performance_report['performance_summary']['current_performance']
            goal_achievement = self.performance_report['performance_summary']['goal_achievement']
            
            print(f"\nğŸ¯ í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ:")
            print(f"   All Queries Recall@10: {current_perf['all_queries_recall_10']:.1f}% (ëª©í‘œ: 75-80%)")
            print(f"   Anchor Queries Recall@10: {current_perf['anchor_queries_recall_10']:.1f}% (ëª©í‘œ: 85-92%)")
            print(f"   Top-5 ì •í™•ë„: {current_perf['top5_accuracy']:.1f}%")
            
            print(f"\nâœ… ëª©í‘œ ë‹¬ì„± í˜„í™©:")
            print(f"   All Queries: {'ë‹¬ì„±' if goal_achievement['all_queries_achieved'] else 'ë¯¸ë‹¬ì„±'}")
            print(f"   Anchor Queries: {'ë‹¬ì„±' if goal_achievement['anchor_queries_achieved'] else 'ë¯¸ë‹¬ì„±'}")
            
            print(f"\nğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:")
            for insight in self.performance_report['key_insights'][:3]:  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
                print(f"   {insight}")
            
            print(f"\nğŸ”§ ìš°ì„  ê¶Œì¥ì‚¬í•­:")
            high_priority_recs = [r for r in self.performance_report['recommendations'] if r['priority'] == 'High']
            for rec in high_priority_recs[:2]:  # ìƒìœ„ 2ê°œë§Œ ì¶œë ¥
                print(f"   {rec['action']}")


def run_integrated_pipeline():
    """í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ Fashion JSON Encoder - í†µí•© íŒŒì´í”„ë¼ì¸")
    print("=" * 80)
    print("ğŸ“Œ ì¤‘ì‹¬ì„± ë¶„ì„ â†’ Query-Aware í‰ê°€ â†’ ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±")
    print("ğŸ”„ ìë™í™”ëœ ì‹¤í—˜ ë° ë¶„ì„ ì›Œí¬í”Œë¡œìš°")
    print("=" * 80)
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ
    dataset_path = "C:/sample/ë¼ë²¨ë§ë°ì´í„°"
    
    # ìµœì í™”ëœ ì„¤ì •
    config = TrainingConfig()
    config.temperature = 0.1
    config.batch_size = 32
    config.max_epochs = 8
    
    try:
        # í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        pipeline = IntegratedPipeline(dataset_path, config)
        results = pipeline.run_complete_pipeline()
        
        print(f"\nğŸ‰ í†µí•© íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼:")
        print(f"   - results/integrated_centrality_analysis.json")
        print(f"   - results/integrated_query_aware_evaluation.json")
        print(f"   - results/integrated_performance_report.json")
        print(f"   - results/integrated_performance_report.md")
        
        return results
        
    except Exception as e:
        print(f"\nâŒ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    run_integrated_pipeline()