#!/usr/bin/env python3
"""
íŒ¨ì…˜ JSON ì¸ì½”ë” - í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

í•©ë¦¬ì ì¸ ê¸°ë³¸ê°’ìœ¼ë¡œ íŒ¨ì…˜ JSON ì¸ì½”ë” ì‹œìŠ¤í…œì„ í•™ìŠµí•˜ê¸° ìœ„í•œ
ì‰¬ìš´ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ê°„ì†Œí™”ëœ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python train.py --dataset_path /path/to/kfashion
    python train.py --dataset_path /path/to/kfashion --epochs 50 --batch_size 64
    python train.py --dataset_path /path/to/kfashion --config my_config.json
"""

import argparse
import sys
import json
from pathlib import Path

# ë©”ì¸ ì‹œìŠ¤í…œ ê°€ì ¸ì˜¤ê¸°
from main import FashionEncoderSystem, create_config_file


def main():
    """ê°„ì†Œí™”ëœ í•™ìŠµ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤."""
    parser = argparse.ArgumentParser(
        description="íŒ¨ì…˜ JSON ì¸ì½”ë” - ê°„ì†Œí™”ëœ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python train.py --dataset_path /path/to/kfashion
  python train.py --dataset_path /path/to/kfashion --epochs 50
  python train.py --dataset_path /path/to/kfashion --batch_size 32 --lr 0.001
  python train.py --dataset_path /path/to/kfashion --config config.json
  python train.py --sanity_check  # í•©ì„± ë°ì´í„°ë¡œ ì •ìƒì„± ê²€ì‚¬ ì‹¤í–‰
        """
    )
    
    # ì£¼ìš” ì¸ìˆ˜
    parser.add_argument('--dataset_path', 
                       help='K-Fashion ë°ì´í„°ì…‹ ê²½ë¡œ (--sanity_checkê°€ ì•„ë‹Œ ê²½ìš° í•„ìˆ˜)')
    parser.add_argument('--config', 
                       help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    
    # í•™ìŠµ ë§¤ê°œë³€ìˆ˜
    parser.add_argument('--epochs', type=int, default=20,
                       help='í•™ìŠµ ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ê°’: 20)')
    parser.add_argument('--standalone_epochs', type=int, default=5,
                       help='ë…ë¦½ í•™ìŠµ ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ê°’: 5)')
    parser.add_argument('--batch_size', type=int,
                       help='ë°°ì¹˜ í¬ê¸° (ì„¤ì • ì¬ì •ì˜)')
    parser.add_argument('--lr', '--learning_rate', type=float, dest='learning_rate',
                       help='í•™ìŠµë¥  (ì„¤ì • ì¬ì •ì˜)')
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    parser.add_argument('--output_dir', default='training_output',
                       help='ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: training_output)')
    
    # íŠ¹ìˆ˜ ëª¨ë“œ
    parser.add_argument('--sanity_check', action='store_true',
                       help='ì „ì²´ í•™ìŠµ ëŒ€ì‹  ì •ìƒì„± ê²€ì‚¬ ì‹¤í–‰')
    parser.add_argument('--create_config', 
                       help='ìƒ˜í”Œ ì„¤ì • íŒŒì¼ ìƒì„± í›„ ì¢…ë£Œ')
    
    # ê³ ê¸‰ ì˜µì…˜
    parser.add_argument('--no_standalone', action='store_true',
                       help='ë…ë¦½ í•™ìŠµ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--gpu', type=int,
                       help='ì‚¬ìš©í•  GPU ì¥ì¹˜ ID')
    
    args = parser.parse_args()
    
    # íŠ¹ìˆ˜ ëª…ë ¹ì–´ ì²˜ë¦¬
    if args.create_config:
        create_config_file(args.create_config)
        return
    
    if args.sanity_check:
        print("í•©ì„± ë°ì´í„°ë¡œ ì •ìƒì„± ê²€ì‚¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
        run_sanity_check(args)
        return
    
    # í•„ìˆ˜ ì¸ìˆ˜ ê²€ì¦
    if not args.dataset_path:
        print("ì˜¤ë¥˜: --dataset_pathê°€ í•„ìš”í•©ë‹ˆë‹¤ (--sanity_check ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°)")
        parser.print_help()
        sys.exit(1)
    
    if not Path(args.dataset_path).exists():
        print(f"ì˜¤ë¥˜: ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {args.dataset_path}")
        sys.exit(1)
    
    # í•™ìŠµ ì‹¤í–‰
    run_training(args)


def run_training(args):
    """ë©”ì¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    print("íŒ¨ì…˜ JSON ì¸ì½”ë” - í•™ìŠµ")
    print("=" * 50)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = Path(args.output_dir)
    checkpoint_dir = output_dir / "checkpoints"
    log_dir = output_dir / "logs"
    
    output_dir.mkdir(exist_ok=True)
    checkpoint_dir.mkdir(exist_ok=True)
    log_dir.mkdir(exist_ok=True)
    
    print(f"ë°ì´í„°ì…‹: {args.dataset_path}")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"ì´ ì—í¬í¬: {args.epochs}")
    print(f"ë…ë¦½ í•™ìŠµ ì—í¬í¬: {args.standalone_epochs}")
    
    try:
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        system = FashionEncoderSystem(config_path=args.config)
        
        # ëª…ë ¹ì¤„ ì¸ìˆ˜ë¡œ ì„¤ì • ì¬ì •ì˜
        if args.batch_size:
            system.config.batch_size = args.batch_size
            print(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
        
        if args.learning_rate:
            system.config.learning_rate = args.learning_rate
            print(f"í•™ìŠµë¥ : {args.learning_rate}")
        
        # ë°ì´í„° ë° íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        print("\në°ì´í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤...")
        system.setup_data(args.dataset_path)
        
        print("íŠ¸ë ˆì´ë„ˆë¥¼ ì„¤ì •í•©ë‹ˆë‹¤...")
        system.setup_trainer(
            checkpoint_dir=str(checkpoint_dir),
            log_dir=str(log_dir)
        )
        
        # í•™ìŠµ ì‹¤í–‰
        print("\ní•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        standalone_epochs = 0 if args.no_standalone else args.standalone_epochs
        contrastive_epochs = args.epochs - standalone_epochs
        
        results = system.train(
            standalone_epochs=standalone_epochs,
            contrastive_epochs=contrastive_epochs,
            save_results=True
        )
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print_training_summary(results, output_dir)
        
        # ì •ë¦¬
        system.cleanup()
        
        print(f"\ní•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜: {checkpoint_dir}")
        print(f"ë¡œê·¸ ì €ì¥ ìœ„ì¹˜: {log_dir}")
        print(f"í•™ìŠµ ì§„í–‰ ìƒí™© ë³´ê¸°: tensorboard --logdir {log_dir}")
        
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        sys.exit(1)
    except Exception as e:
        print(f"\ní•™ìŠµ ì‹¤íŒ¨: {e}")
        sys.exit(1)


def run_sanity_check(args):
    """ì •ìƒì„± ê²€ì‚¬ ëª¨ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    print("íŒ¨ì…˜ JSON ì¸ì½”ë” - ì •ìƒì„± ê²€ì‚¬")
    print("=" * 50)
    
    try:
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        system = FashionEncoderSystem(config_path=args.config)
        
        # ì •ìƒì„± ê²€ì‚¬ ì‹¤í–‰
        results = system.sanity_check(
            dataset_path=args.dataset_path,  # í•©ì„± ë°ì´í„°ì˜ ê²½ìš° None ê°€ëŠ¥
            num_epochs=3
        )
        
        # ìš”ì•½ ì¶œë ¥
        print_sanity_check_summary(results)
        
        # ì •ë¦¬
        system.cleanup()
        
    except Exception as e:
        print(f"ì •ìƒì„± ê²€ì‚¬ ì‹¤íŒ¨: {e}")
        sys.exit(1)


def print_training_summary(results, output_dir):
    """í•™ìŠµ ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n" + "=" * 60)
    print("í•™ìŠµ ìš”ì•½")
    print("=" * 60)
    
    if 'standalone' in results:
        standalone = results['standalone']
        print(f"ë…ë¦½ í•™ìŠµ:")
        print(f"  ì—í¬í¬: {len(standalone['train_losses'])}")
        print(f"  ì´ˆê¸° ì†ì‹¤: {standalone['train_losses'][0]:.4f}")
        print(f"  ìµœì¢… ì†ì‹¤: {standalone['train_losses'][-1]:.4f}")
        
        final_analysis = standalone.get('final_analysis', {})
        if final_analysis:
            print(f"  ì¶œë ¥ ì •ê·œí™”ë¨: {final_analysis.get('is_normalized', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
            print(f"  í‰ê·  ë…¸ë¦„: {final_analysis.get('norm_mean', 0):.4f}")
    
    if 'contrastive' in results:
        contrastive = results['contrastive']
        print(f"\nëŒ€ì¡° í•™ìŠµ:")
        print(f"  ì—í¬í¬: {contrastive['total_epochs']}")
        print(f"  ìµœê³  ê²€ì¦ ì†ì‹¤: {contrastive['best_val_loss']:.4f}")
        
        final_metrics = contrastive.get('final_metrics', {})
        if final_metrics:
            print(f"  Top-1 ì •í™•ë„: {final_metrics.get('top1_accuracy', 0):.4f}")
            print(f"  Top-5 ì •í™•ë„: {final_metrics.get('top5_accuracy', 0):.4f}")
            print(f"  í‰ê·  ì—­ìˆœìœ„: {final_metrics.get('mean_reciprocal_rank', 0):.4f}")
    
    print(f"\nê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")


def print_sanity_check_summary(results):
    """ì •ìƒì„± ê²€ì‚¬ ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n" + "=" * 60)
    print("ì •ìƒì„± ê²€ì‚¬ ìš”ì•½")
    print("=" * 60)
    
    validation = results.get('validation_results', {})
    
    print("ê²€ì¦ í™•ì¸:")
    checks = [
        ('dimension_check', 'ì¶œë ¥ ì°¨ì›'),
        ('normalization_check', 'L2 ì •ê·œí™”'),
        ('gradient_check', 'ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°'),
        ('field_processing_check', 'í•„ë“œ ì²˜ë¦¬'),
        ('batch_consistency_check', 'ë°°ì¹˜ ì¼ê´€ì„±')
    ]
    
    all_passed = True
    for check_key, check_name in checks:
        passed = validation.get(check_key, False)
        status = "âœ“ í†µê³¼" if passed else "âœ— ì‹¤íŒ¨"
        print(f"  {status} {check_name}")
        if not passed:
            all_passed = False
    
    if validation.get('errors'):
        print("\nì˜¤ë¥˜:")
        for error in validation['errors']:
            print(f"  - {error}")
    
    # í•™ìŠµ ì§„í–‰ ìƒí™©
    training = results.get('training_results', {})
    if training:
        print(f"\ní•™ìŠµ ì§„í–‰ ìƒí™©:")
        print(f"  ì†ì‹¤ ê°ì†Œ: {training['train_losses'][0]:.4f} â†’ {training['train_losses'][-1]:.4f}")
    
    # ìµœì¢… í‰ê°€
    final_analysis = results.get('final_analysis', {})
    is_normalized = final_analysis.get('is_normalized', False)
    correct_dim = final_analysis.get('embedding_dim', 0) == 512
    
    print(f"\n{'='*40}")
    if all_passed and is_normalized and correct_dim:
        print("ğŸ‰ ì •ìƒì„± ê²€ì‚¬ í†µê³¼")
        print("JSON ì¸ì½”ë”ê°€ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•©ë‹ˆë‹¤!")
    else:
        print("âš ï¸  ì •ìƒì„± ê²€ì‚¬ì—ì„œ ë¬¸ì œê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤")
        print("ìì„¸í•œ ë‚´ìš©ì€ ìœ„ì˜ ê²°ê³¼ë¥¼ ê²€í† í•˜ì„¸ìš”.")
    print("="*40)


if __name__ == "__main__":
    main()