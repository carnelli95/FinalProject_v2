#!/usr/bin/env python3
"""
Baseline v2 ìƒì„± ë° v1ê³¼ ë¹„êµ

í˜„ì¬ í•™ìŠµëœ ëª¨ë¸ì„ ê¸°ì¤€ìœ¼ë¡œ Baseline v2ë¥¼ ìƒì„±í•˜ê³  v1ê³¼ ì„±ëŠ¥ ë¹„êµ
"""

import sys
import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any
import torch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from main import FashionEncoderSystem
from utils.config import TrainingConfig


def evaluate_current_model() -> Dict[str, Any]:
    """í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    print("ğŸ” í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ
    dataset_path = "C:/sample/ë¼ë²¨ë§ë°ì´í„°"
    
    # í˜„ì¬ ì„¤ì • (Temperature 0.1 ê¸°ì¤€)
    config = TrainingConfig()
    config.temperature = 0.1
    config.batch_size = 16
    config.max_epochs = 8
    
    try:
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        system = FashionEncoderSystem()
        system.config = config
        
        # ë°ì´í„° ì„¤ì •
        print("ğŸ“ ë°ì´í„° ì„¤ì • ì¤‘...")
        system.setup_data(dataset_path)
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        print("ğŸ‹ï¸ íŠ¸ë ˆì´ë„ˆ ì„¤ì • ì¤‘...")
        system.setup_trainer()
        
        # í˜„ì¬ ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint_path = "checkpoints/baseline_v1_best_model.pt"
        if Path(checkpoint_path).exists():
            print(f"ğŸ“¦ Baseline v1 ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
            system.trainer.load_checkpoint(checkpoint_path)
        else:
            checkpoint_path = "checkpoints/best_model.pt"
            if Path(checkpoint_path).exists():
                print(f"ğŸ“¦ í˜„ì¬ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
                system.trainer.load_checkpoint(checkpoint_path)
            else:
                print("âš ï¸ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ëª¨ë¸ ìƒíƒœë¡œ í‰ê°€í•©ë‹ˆë‹¤.")
        
        # í‰ê°€ ìˆ˜í–‰
        print("ğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
        metrics = system.trainer._final_evaluation(system.data_module.val_dataloader())
        
        # ê²°ê³¼ ì •ë¦¬
        evaluation_results = {
            'timestamp': datetime.now().isoformat(),
            'model_name': 'Fashion JSON Encoder Baseline v2',
            'configuration': {
                'temperature': config.temperature,
                'batch_size': config.batch_size,
                'epochs': config.max_epochs,
                'learning_rate': config.learning_rate,
                'dataset': 'K-Fashion 2,172 items',
                'class_distribution': {
                    'ë ˆíŠ¸ë¡œ': len([item for item in system.data_module.train_dataset.fashion_items + system.data_module.val_dataset.fashion_items if item.category == 'ë ˆíŠ¸ë¡œ']),
                    'ë¡œë§¨í‹±': len([item for item in system.data_module.train_dataset.fashion_items + system.data_module.val_dataset.fashion_items if item.category == 'ë¡œë§¨í‹±']),
                    'ë¦¬ì¡°íŠ¸': len([item for item in system.data_module.train_dataset.fashion_items + system.data_module.val_dataset.fashion_items if item.category == 'ë¦¬ì¡°íŠ¸'])
                }
            },
            'final_performance': {
                'top1_accuracy': metrics.get('top1_accuracy', 0.0),
                'top5_accuracy': metrics.get('top5_accuracy', 0.0),
                'mrr': metrics.get('mean_reciprocal_rank', 0.0),
                'validation_loss': metrics.get('val_loss', 0.0),
                'positive_similarity': metrics.get('avg_positive_similarity', 0.0),
                'negative_similarity': metrics.get('avg_negative_similarity', 0.0)
            },
            'additional_metrics': {
                'recall_at_3': metrics.get('recall_at_3', 0.0),
                'recall_at_10': metrics.get('recall_at_10', 0.0),
                'recall_at_20': metrics.get('recall_at_20', 0.0)
            }
        }
        
        # ì •ë¦¬
        system.cleanup()
        
        return evaluation_results
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


def load_baseline_v1_results() -> Dict[str, Any]:
    """Baseline v1 ê²°ê³¼ ë¡œë“œ"""
    v1_path = Path("results/baseline_v1_results.json")
    if v1_path.exists():
        with open(v1_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        print("âš ï¸ Baseline v1 ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None


def compare_baselines(v1_results: Dict[str, Any], v2_results: Dict[str, Any]) -> Dict[str, Any]:
    """Baseline v1ê³¼ v2 ë¹„êµ"""
    print("\nğŸ“Š Baseline v1 vs v2 ë¹„êµ ë¶„ì„ ì¤‘...")
    
    if not v1_results or not v2_results:
        print("âŒ ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ
    v1_perf = v1_results['final_performance']
    v2_perf = v2_results['final_performance']
    
    # ì„±ëŠ¥ ë¹„êµ
    comparison = {
        'timestamp': datetime.now().isoformat(),
        'comparison_type': 'Baseline v1 vs v2',
        
        'v1_performance': {
            'top1_accuracy': v1_perf['top1_accuracy'] * 100,
            'top5_accuracy': v1_perf['top5_accuracy'] * 100,
            'mrr': v1_perf['mrr'],
            'validation_loss': v1_perf['validation_loss'],
            'positive_similarity': v1_perf['positive_similarity']
        },
        
        'v2_performance': {
            'top1_accuracy': v2_perf['top1_accuracy'] * 100,
            'top5_accuracy': v2_perf['top5_accuracy'] * 100,
            'mrr': v2_perf['mrr'],
            'validation_loss': v2_perf['validation_loss'],
            'positive_similarity': v2_perf['positive_similarity']
        },
        
        'improvements': {
            'top1_accuracy_diff': (v2_perf['top1_accuracy'] - v1_perf['top1_accuracy']) * 100,
            'top5_accuracy_diff': (v2_perf['top5_accuracy'] - v1_perf['top5_accuracy']) * 100,
            'mrr_diff': v2_perf['mrr'] - v1_perf['mrr'],
            'validation_loss_diff': v2_perf['validation_loss'] - v1_perf['validation_loss'],
            'positive_similarity_diff': v2_perf['positive_similarity'] - v1_perf['positive_similarity']
        },
        
        'relative_improvements': {
            'top1_accuracy_rel': ((v2_perf['top1_accuracy'] - v1_perf['top1_accuracy']) / v1_perf['top1_accuracy']) * 100 if v1_perf['top1_accuracy'] > 0 else 0,
            'top5_accuracy_rel': ((v2_perf['top5_accuracy'] - v1_perf['top5_accuracy']) / v1_perf['top5_accuracy']) * 100 if v1_perf['top5_accuracy'] > 0 else 0,
            'mrr_rel': ((v2_perf['mrr'] - v1_perf['mrr']) / v1_perf['mrr']) * 100 if v1_perf['mrr'] > 0 else 0
        },
        
        'configuration_comparison': {
            'v1_config': v1_results['configuration'],
            'v2_config': v2_results['configuration']
        }
    }
    
    return comparison


def print_comparison_summary(comparison: Dict[str, Any]):
    """ë¹„êµ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print(f"\n{'='*80}")
    print("ğŸ“Š Baseline v1 vs v2 ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
    print(f"{'='*80}")
    
    v1_perf = comparison['v1_performance']
    v2_perf = comparison['v2_performance']
    improvements = comparison['improvements']
    rel_improvements = comparison['relative_improvements']
    
    print(f"\nğŸ¯ í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ:")
    print(f"{'ë©”íŠ¸ë¦­':<20} {'v1':<15} {'v2':<15} {'ì ˆëŒ€ ê°œì„ ':<15} {'ìƒëŒ€ ê°œì„ ':<15}")
    print(f"{'-'*80}")
    print(f"{'Top-1 ì •í™•ë„':<20} {v1_perf['top1_accuracy']:<15.1f}% {v2_perf['top1_accuracy']:<15.1f}% {improvements['top1_accuracy_diff']:<15.1f}%p {rel_improvements['top1_accuracy_rel']:<15.1f}%")
    print(f"{'Top-5 ì •í™•ë„':<20} {v1_perf['top5_accuracy']:<15.1f}% {v2_perf['top5_accuracy']:<15.1f}% {improvements['top5_accuracy_diff']:<15.1f}%p {rel_improvements['top5_accuracy_rel']:<15.1f}%")
    print(f"{'MRR':<20} {v1_perf['mrr']:<15.3f} {v2_perf['mrr']:<15.3f} {improvements['mrr_diff']:<15.3f} {rel_improvements['mrr_rel']:<15.1f}%")
    print(f"{'ê²€ì¦ ì†ì‹¤':<20} {v1_perf['validation_loss']:<15.3f} {v2_perf['validation_loss']:<15.3f} {improvements['validation_loss_diff']:<15.3f} {'N/A':<15}")
    print(f"{'ì–‘ì„± ìœ ì‚¬ë„':<20} {v1_perf['positive_similarity']:<15.3f} {v2_perf['positive_similarity']:<15.3f} {improvements['positive_similarity_diff']:<15.3f} {'N/A':<15}")
    
    print(f"\nğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
    
    # Top-5 ì •í™•ë„ ê¸°ì¤€ ë¶„ì„
    if improvements['top5_accuracy_diff'] > 0:
        print(f"   âœ… Top-5 ì •í™•ë„ ê°œì„ : {improvements['top5_accuracy_diff']:.1f}%p í–¥ìƒ")
    elif improvements['top5_accuracy_diff'] < 0:
        print(f"   âŒ Top-5 ì •í™•ë„ í•˜ë½: {abs(improvements['top5_accuracy_diff']):.1f}%p ê°ì†Œ")
    else:
        print(f"   â– Top-5 ì •í™•ë„ ë™ì¼")
    
    # MRR ê¸°ì¤€ ë¶„ì„
    if improvements['mrr_diff'] > 0:
        print(f"   âœ… MRR ê°œì„ : {improvements['mrr_diff']:.3f} í–¥ìƒ")
    elif improvements['mrr_diff'] < 0:
        print(f"   âŒ MRR í•˜ë½: {abs(improvements['mrr_diff']):.3f} ê°ì†Œ")
    else:
        print(f"   â– MRR ë™ì¼")
    
    # ì „ì²´ì ì¸ ì„±ëŠ¥ í‰ê°€
    positive_changes = sum([
        1 if improvements['top1_accuracy_diff'] > 0 else 0,
        1 if improvements['top5_accuracy_diff'] > 0 else 0,
        1 if improvements['mrr_diff'] > 0 else 0,
        1 if improvements['validation_loss_diff'] < 0 else 0  # ì†ì‹¤ì€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
    ])
    
    print(f"\nğŸ† ì „ì²´ í‰ê°€:")
    if positive_changes >= 3:
        print(f"   ğŸ‰ Baseline v2ê°€ v1 ëŒ€ë¹„ ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤!")
    elif positive_changes >= 2:
        print(f"   ğŸ‘ Baseline v2ê°€ v1 ëŒ€ë¹„ ì¼ë¶€ ê°œì„ ì„ ë³´ì…ë‹ˆë‹¤.")
    elif positive_changes >= 1:
        print(f"   ğŸ¤” Baseline v2ê°€ v1 ëŒ€ë¹„ ë¯¸ë¯¸í•œ ê°œì„ ì„ ë³´ì…ë‹ˆë‹¤.")
    else:
        print(f"   ğŸ˜” Baseline v2ê°€ v1 ëŒ€ë¹„ ì„±ëŠ¥ ê°œì„ ì´ ì—†ê±°ë‚˜ í•˜ë½í–ˆìŠµë‹ˆë‹¤.")
    
    print(f"\nğŸ”§ ì„¤ì • ë¹„êµ:")
    v1_config = comparison['configuration_comparison']['v1_config']
    v2_config = comparison['configuration_comparison']['v2_config']
    
    print(f"   Temperature: v1={v1_config['temperature']} vs v2={v2_config['temperature']}")
    print(f"   Batch Size: v1={v1_config['batch_size']} vs v2={v2_config['batch_size']}")
    print(f"   Epochs: v1={v1_config['epochs']} vs v2={v2_config['epochs']}")


def create_baseline_v2():
    """Baseline v2 ìƒì„± ë° ë¹„êµ"""
    print("ğŸš€ Baseline v2 ìƒì„± ë° v1ê³¼ ë¹„êµ")
    print("=" * 60)
    
    # STEP 1: í˜„ì¬ ëª¨ë¸ í‰ê°€
    print("STEP 1: í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
    v2_results = evaluate_current_model()
    
    if v2_results is None:
        print("âŒ v2 í‰ê°€ ì‹¤íŒ¨")
        return
    
    # STEP 2: v1 ê²°ê³¼ ë¡œë“œ
    print("\nSTEP 2: Baseline v1 ê²°ê³¼ ë¡œë“œ")
    v1_results = load_baseline_v1_results()
    
    if v1_results is None:
        print("âŒ v1 ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨")
        return
    
    # STEP 3: ë¹„êµ ë¶„ì„
    print("\nSTEP 3: v1 vs v2 ë¹„êµ ë¶„ì„")
    comparison = compare_baselines(v1_results, v2_results)
    
    if comparison is None:
        print("âŒ ë¹„êµ ë¶„ì„ ì‹¤íŒ¨")
        return
    
    # STEP 4: ê²°ê³¼ ì €ì¥
    print("\nSTEP 4: ê²°ê³¼ ì €ì¥")
    results_dir = Path("results")
    results_dir.mkdir(exist_ok=True)
    
    # v2 ê²°ê³¼ ì €ì¥
    v2_file = results_dir / "baseline_v2_results.json"
    with open(v2_file, 'w', encoding='utf-8') as f:
        json.dump(v2_results, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ Baseline v2 ê²°ê³¼ ì €ì¥: {v2_file}")
    
    # ë¹„êµ ê²°ê³¼ ì €ì¥
    comparison_file = results_dir / "baseline_v1_vs_v2_comparison.json"
    with open(comparison_file, 'w', encoding='utf-8') as f:
        json.dump(comparison, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥: {comparison_file}")
    
    # STEP 5: ìš”ì•½ ì¶œë ¥
    print("\nSTEP 5: ë¹„êµ ê²°ê³¼ ìš”ì•½")
    print_comparison_summary(comparison)
    
    # ì²´í¬í¬ì¸íŠ¸ ë°±ì—…
    current_checkpoint = Path("checkpoints/best_model.pt")
    if current_checkpoint.exists():
        v2_checkpoint = Path("checkpoints/baseline_v2_best_model.pt")
        import shutil
        shutil.copy2(current_checkpoint, v2_checkpoint)
        print(f"\nğŸ“¦ Baseline v2 ì²´í¬í¬ì¸íŠ¸ ë°±ì—…: {v2_checkpoint}")
    
    print(f"\nâœ¨ Baseline v2 ìƒì„± ë° ë¹„êµ ì™„ë£Œ!")
    
    return {
        'v1_results': v1_results,
        'v2_results': v2_results,
        'comparison': comparison
    }


if __name__ == "__main__":
    create_baseline_v2()