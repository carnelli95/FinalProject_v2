#!/usr/bin/env python3
"""
ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ (65GB) ê³ ì„±ëŠ¥ GPU í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

ìµœì í™” í¬ì¸íŠ¸:
1. GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì‚¬ìš©
2. ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
3. ë¶„ì‚° í•™ìŠµ ì§€ì›
4. ì²´í¬í¬ì¸íŠ¸ ìë™ ì €ì¥
5. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
"""

import os
import sys
import json
import time
import psutil
import gc
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
import torch.multiprocessing as mp

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from main import FashionEncoderSystem
from utils.config import TrainingConfig


class LargeScaleTrainingConfig(TrainingConfig):
    """ëŒ€ê·œëª¨ í•™ìŠµì„ ìœ„í•œ í™•ì¥ ì„¤ì •"""
    
    def __init__(self):
        super().__init__()
        
        # GPU ìµœì í™” ì„¤ì •
        self.mixed_precision = True
        self.gradient_accumulation_steps = 4
        self.max_grad_norm = 1.0
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
        self.batch_size = 64  # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
        self.num_workers = 8  # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¼ ì¡°ì •
        self.pin_memory = True
        self.persistent_workers = True
        
        # í•™ìŠµ ìµœì í™”
        self.learning_rate = 0.0001
        self.weight_decay = 1e-5
        self.temperature = 0.09  # v2 ìµœì í™”ëœ ê°’
        self.max_epochs = 15
        
        # ì²´í¬í¬ì¸íŠ¸ ë° ë¡œê¹…
        self.save_every_n_epochs = 2
        self.eval_every_n_epochs = 1
        self.log_every_n_steps = 100
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.empty_cache_every_n_steps = 500
        self.gc_collect_every_n_steps = 1000


def setup_gpu_environment():
    """GPU í™˜ê²½ ì„¤ì • ë° ìµœì í™”"""
    print("ğŸ”§ GPU í™˜ê²½ ì„¤ì • ì¤‘...")
    
    # CUDA ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
    if not torch.cuda.is_available():
        raise RuntimeError("CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. GPU í™˜ê²½ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    # GPU ì •ë³´ ì¶œë ¥
    gpu_count = torch.cuda.device_count()
    print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {gpu_count}ê°œ")
    
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    
    # CUDA ìµœì í™” ì„¤ì •
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
    
    return gpu_count


def setup_distributed_training(rank, world_size):
    """ë¶„ì‚° í•™ìŠµ ì„¤ì •"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    
    # ë¶„ì‚° í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ì´ˆê¸°í™”
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)


def cleanup_distributed():
    """ë¶„ì‚° í•™ìŠµ ì •ë¦¬"""
    dist.destroy_process_group()


class MemoryMonitor:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.start_time = time.time()
        self.peak_memory = 0
        
    def log_memory_usage(self, step: int, prefix: str = ""):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…"""
        if torch.cuda.is_available():
            current_memory = torch.cuda.memory_allocated() / 1024**3
            max_memory = torch.cuda.max_memory_allocated() / 1024**3
            self.peak_memory = max(self.peak_memory, max_memory)
            
            if step % 100 == 0:  # 100 ìŠ¤í…ë§ˆë‹¤ ë¡œê¹…
                print(f"   {prefix}Step {step}: GPU ë©”ëª¨ë¦¬ {current_memory:.2f}GB / ìµœëŒ€ {max_memory:.2f}GB")
    
    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()


class LargeScaleTrainer:
    """ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ í•™ìŠµì„ ìœ„í•œ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, config: LargeScaleTrainingConfig, rank: int = 0, world_size: int = 1):
        self.config = config
        self.rank = rank
        self.world_size = world_size
        self.device = f'cuda:{rank}' if torch.cuda.is_available() else 'cpu'
        
        self.memory_monitor = MemoryMonitor()
        self.scaler = torch.cuda.amp.GradScaler() if config.mixed_precision else None
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        self.log_dir = Path("logs/large_scale_training")
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        self.checkpoint_dir = Path("checkpoints/large_scale")
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # í•™ìŠµ ë¡œê·¸ íŒŒì¼
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file = self.log_dir / f"training_log_{timestamp}.txt"
        
    def log_message(self, message: str):
        """ë©”ì‹œì§€ ë¡œê¹…"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_msg = f"[{timestamp}] {message}"
        print(log_msg)
        
        if self.rank == 0:  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ë§Œ íŒŒì¼ì— ê¸°ë¡
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(log_msg + '\n')
    
    def setup_model_and_data(self, dataset_path: str):
        """ëª¨ë¸ ë° ë°ì´í„° ì„¤ì •"""
        self.log_message("ğŸ“ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.system = FashionEncoderSystem()
        self.system.config = self.config
        
        # ë°ì´í„° ì„¤ì •
        self.system.setup_data(dataset_path)
        
        # ë°ì´í„° ë¡œë” ìµœì í™”
        train_sampler = DistributedSampler(
            self.system.data_module.train_dataset,
            num_replicas=self.world_size,
            rank=self.rank,
            shuffle=True
        ) if self.world_size > 1 else None
        
        self.train_loader = DataLoader(
            self.system.data_module.train_dataset,
            batch_size=self.config.batch_size,
            sampler=train_sampler,
            num_workers=self.config.num_workers,
            pin_memory=self.config.pin_memory,
            persistent_workers=self.config.persistent_workers,
            drop_last=True
        )
        
        self.val_loader = DataLoader(
            self.system.data_module.val_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=self.config.num_workers,
            pin_memory=self.config.pin_memory,
            persistent_workers=self.config.persistent_workers
        )
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        self.system.setup_trainer()
        
        # ëª¨ë¸ì„ GPUë¡œ ì´ë™
        self.system.trainer.contrastive_learner.to(self.device)
        
        # ë¶„ì‚° í•™ìŠµ ì„¤ì •
        if self.world_size > 1:
            self.system.trainer.contrastive_learner = DDP(
                self.system.trainer.contrastive_learner,
                device_ids=[self.rank],
                output_device=self.rank
            )
        
        # v2 ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì‹œì‘
        v2_checkpoint = "checkpoints/baseline_v2_final_best_model.pt"
        if Path(v2_checkpoint).exists():
            self.log_message(f"ğŸ“¦ v2 ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì‹œì‘: {v2_checkpoint}")
            self.system.trainer.load_checkpoint(v2_checkpoint)
        
        self.log_message(f"âœ… ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ:")
        self.log_message(f"   í•™ìŠµ ìƒ˜í”Œ: {len(self.system.data_module.train_dataset):,}")
        self.log_message(f"   ê²€ì¦ ìƒ˜í”Œ: {len(self.system.data_module.val_dataset):,}")
        self.log_message(f"   ë°°ì¹˜ í¬ê¸°: {self.config.batch_size}")
        self.log_message(f"   ì´ ë°°ì¹˜ ìˆ˜: {len(self.train_loader):,}")
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        self.system.trainer.contrastive_learner.train()
        
        total_loss = 0.0
        num_batches = len(self.train_loader)
        
        # ë¶„ì‚° í•™ìŠµ ì‹œ sampler ì—í¬í¬ ì„¤ì •
        if hasattr(self.train_loader.sampler, 'set_epoch'):
            self.train_loader.sampler.set_epoch(epoch)
        
        for step, batch in enumerate(self.train_loader):
            # ë°°ì¹˜ë¥¼ GPUë¡œ ì´ë™
            batch = self.system.trainer._move_batch_to_device(batch)
            
            # Mixed Precision Training
            with torch.cuda.amp.autocast(enabled=self.config.mixed_precision):
                # Forward pass
                json_batch = self.system.trainer._convert_batch_to_dict(batch)
                embeddings = self.system.trainer.contrastive_learner.get_embeddings(
                    batch.images, json_batch
                )
                
                # Loss ê³„ì‚°
                loss = self.system.trainer.contrastive_learner.compute_contrastive_loss(
                    embeddings['image_embeddings'],
                    embeddings['json_embeddings'],
                    temperature=self.config.temperature
                )
                
                # Gradient Accumulation
                loss = loss / self.config.gradient_accumulation_steps
            
            # Backward pass
            if self.scaler:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            # Optimizer step
            if (step + 1) % self.config.gradient_accumulation_steps == 0:
                if self.scaler:
                    self.scaler.unscale_(self.system.trainer.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.system.trainer.contrastive_learner.parameters(),
                        self.config.max_grad_norm
                    )
                    self.scaler.step(self.system.trainer.optimizer)
                    self.scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(
                        self.system.trainer.contrastive_learner.parameters(),
                        self.config.max_grad_norm
                    )
                    self.system.trainer.optimizer.step()
                
                self.system.trainer.optimizer.zero_grad()
            
            total_loss += loss.item() * self.config.gradient_accumulation_steps
            
            # ë¡œê¹…
            if step % self.config.log_every_n_steps == 0:
                avg_loss = total_loss / (step + 1)
                self.log_message(f"   Epoch {epoch}, Step {step}/{num_batches}, Loss: {avg_loss:.4f}")
                self.memory_monitor.log_memory_usage(step, f"Epoch {epoch} ")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if step % self.config.empty_cache_every_n_steps == 0:
                torch.cuda.empty_cache()
            
            if step % self.config.gc_collect_every_n_steps == 0:
                gc.collect()
        
        return {'train_loss': total_loss / num_batches}
    
    def validate(self, epoch: int) -> Dict[str, float]:
        """ê²€ì¦"""
        self.log_message(f"ğŸ“Š Epoch {epoch} ê²€ì¦ ì¤‘...")
        
        metrics = self.system.trainer._final_evaluation(self.val_loader)
        
        self.log_message(f"   ê²€ì¦ ê²°ê³¼:")
        self.log_message(f"     Top-1: {metrics.get('top1_accuracy', 0)*100:.1f}%")
        self.log_message(f"     Top-5: {metrics.get('top5_accuracy', 0)*100:.1f}%")
        self.log_message(f"     MRR: {metrics.get('mean_reciprocal_rank', 0):.3f}")
        
        return metrics
    
    def save_checkpoint(self, epoch: int, metrics: Dict[str, float]):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        if self.rank != 0:  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ë§Œ ì €ì¥
            return
        
        checkpoint_path = self.checkpoint_dir / f"large_scale_v2_epoch_{epoch}.pt"
        
        # ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ì¶”ì 
        top5_accuracy = metrics.get('top5_accuracy', 0)
        
        # í˜„ì¬ ìµœê³  ì„±ëŠ¥ í™•ì¸
        best_checkpoint = self.checkpoint_dir / "large_scale_v2_best.pt"
        current_best = 0.0
        
        if best_checkpoint.exists():
            try:
                checkpoint = torch.load(best_checkpoint, map_location='cpu')
                current_best = checkpoint.get('best_top5_accuracy', 0.0)
            except:
                pass
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        save_dict = {
            'epoch': epoch,
            'model_state_dict': self.system.trainer.contrastive_learner.state_dict(),
            'optimizer_state_dict': self.system.trainer.optimizer.state_dict(),
            'metrics': metrics,
            'config': self.config.__dict__,
            'best_top5_accuracy': max(top5_accuracy, current_best)
        }
        
        torch.save(save_dict, checkpoint_path)
        self.log_message(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
        
        # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
        if top5_accuracy > current_best:
            torch.save(save_dict, best_checkpoint)
            self.log_message(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! Top-5: {top5_accuracy*100:.1f}%")
    
    def train(self, dataset_path: str):
        """ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤"""
        self.log_message("ğŸš€ ëŒ€ê·œëª¨ v2 í•™ìŠµ ì‹œì‘")
        self.log_message("=" * 80)
        
        # ëª¨ë¸ ë° ë°ì´í„° ì„¤ì •
        self.setup_model_and_data(dataset_path)
        
        # í•™ìŠµ ë£¨í”„
        best_top5 = 0.0
        
        for epoch in range(1, self.config.max_epochs + 1):
            self.log_message(f"\nğŸ“š Epoch {epoch}/{self.config.max_epochs} ì‹œì‘")
            
            # í•™ìŠµ
            train_metrics = self.train_epoch(epoch)
            
            # ê²€ì¦
            if epoch % self.config.eval_every_n_epochs == 0:
                val_metrics = self.validate(epoch)
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if epoch % self.config.save_every_n_epochs == 0:
                    self.save_checkpoint(epoch, val_metrics)
                
                # ìµœê³  ì„±ëŠ¥ ì¶”ì 
                current_top5 = val_metrics.get('top5_accuracy', 0)
                if current_top5 > best_top5:
                    best_top5 = current_top5
                    self.log_message(f"ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥: {best_top5*100:.1f}%")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_monitor.cleanup_memory()
        
        # ìµœì¢… ê²°ê³¼
        self.log_message(f"\nâœ¨ í•™ìŠµ ì™„ë£Œ!")
        self.log_message(f"   ìµœê³  Top-5 ì •í™•ë„: {best_top5*100:.1f}%")
        self.log_message(f"   ì´ í•™ìŠµ ì‹œê°„: {(time.time() - self.memory_monitor.start_time)/3600:.1f}ì‹œê°„")
        
        # ì •ë¦¬
        self.system.cleanup()


def run_distributed_training(rank, world_size, dataset_path: str):
    """ë¶„ì‚° í•™ìŠµ ì‹¤í–‰"""
    try:
        # ë¶„ì‚° í•™ìŠµ ì„¤ì •
        setup_distributed_training(rank, world_size)
        
        # ì„¤ì • ë° íŠ¸ë ˆì´ë„ˆ ìƒì„±
        config = LargeScaleTrainingConfig()
        trainer = LargeScaleTrainer(config, rank, world_size)
        
        # í•™ìŠµ ì‹¤í–‰
        trainer.train(dataset_path)
        
    except Exception as e:
        print(f"âŒ Rank {rank} í•™ìŠµ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
    finally:
        cleanup_distributed()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ëŒ€ê·œëª¨ v2 í•™ìŠµ ì‹œìŠ¤í…œ")
    print("=" * 80)
    
    # GPU í™˜ê²½ ì„¤ì •
    gpu_count = setup_gpu_environment()
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ (65GB ë°ì´í„°)
    dataset_path = input("ğŸ“ 65GB ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    if not dataset_path:
        dataset_path = "/path/to/your/65gb/dataset"  # ê¸°ë³¸ê°’
    
    if not Path(dataset_path).exists():
        print(f"âŒ ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dataset_path}")
        return
    
    print(f"ğŸ“Š ì„¤ì • ì •ë³´:")
    config = LargeScaleTrainingConfig()
    print(f"   ë°°ì¹˜ í¬ê¸°: {config.batch_size}")
    print(f"   ìµœëŒ€ ì—í¬í¬: {config.max_epochs}")
    print(f"   í•™ìŠµë¥ : {config.learning_rate}")
    print(f"   Temperature: {config.temperature}")
    print(f"   Mixed Precision: {config.mixed_precision}")
    print(f"   Gradient Accumulation: {config.gradient_accumulation_steps}")
    
    # ë¶„ì‚° í•™ìŠµ ì—¬ë¶€ ê²°ì •
    if gpu_count > 1:
        print(f"\nğŸ”¥ {gpu_count}ê°œ GPUë¡œ ë¶„ì‚° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        mp.spawn(
            run_distributed_training,
            args=(gpu_count, dataset_path),
            nprocs=gpu_count,
            join=True
        )
    else:
        print(f"\nğŸ”¥ ë‹¨ì¼ GPUë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        trainer = LargeScaleTrainer(config)
        trainer.train(dataset_path)


if __name__ == "__main__":
    main()