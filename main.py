#!/usr/bin/env python3
"""
íŒ¨ì…˜ JSON ì¸ì½”ë” - ë©”ì¸ í†µí•© ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë°ì´í„° ì²˜ë¦¬, ëª¨ë¸ í•™ìŠµ, í‰ê°€ë¥¼ í¬í•¨í•œ ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ í†µí•©í•˜ì—¬
íŒ¨ì…˜ JSON ì¸ì½”ë” ì‹œìŠ¤í…œì˜ ë©”ì¸ ì§„ì…ì ì„ ì œê³µí•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python main.py --help                          # ë„ì›€ë§ í‘œì‹œ
    python main.py train --dataset_path /path/to/data  # ëª¨ë¸ í•™ìŠµ
    python main.py evaluate --checkpoint_path /path/to/checkpoint  # ëª¨ë¸ í‰ê°€
    python main.py sanity_check                    # ì •ìƒì„± ê²€ì‚¬ ì‹¤í–‰
"""

import argparse
import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional

import torch

from data.fashion_dataset import FashionDataModule
from training.trainer import FashionTrainer, create_trainer_from_data_module
from utils.config import TrainingConfig
from utils.validators import InputValidator, ModelValidator
from examples.json_encoder_sanity_check import JSONEncoderSanityChecker


# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('fashion_encoder.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class FashionEncoderSystem:
    """
    ëª¨ë“  íŒ¨ì…˜ JSON ì¸ì½”ë” êµ¬ì„± ìš”ì†Œë¥¼ í†µí•©í•˜ëŠ” ë©”ì¸ ì‹œìŠ¤í…œ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    
    ì´ í´ë˜ìŠ¤ëŠ” íŒ¨ì…˜ JSON ì¸ì½”ë” ì‹œìŠ¤í…œì˜ í•™ìŠµ, í‰ê°€, ì¶”ë¡ ì„ ìœ„í•œ
    í†µí•©ëœ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """
        íŒ¨ì…˜ ì¸ì½”ë” ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        """
        self.config = self._load_config(config_path)
        self.device = self._setup_device()
        self.data_module = None
        self.trainer = None
        
        logger.info(f"íŒ¨ì…˜ ì¸ì½”ë” ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")
        logger.info(f"ì¥ì¹˜: {self.device}")
        logger.info(f"ì„¤ì •: {self.config}")
    
    def _load_config(self, config_path: Optional[str]) -> TrainingConfig:
        """íŒŒì¼ì—ì„œ ì„¤ì •ì„ ë¡œë“œí•˜ê±°ë‚˜ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."""
        if config_path and Path(config_path).exists():
            logger.info(f"{config_path}ì—ì„œ ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤")
            with open(config_path, 'r', encoding='utf-8') as f:
                config_dict = json.load(f)
            
            # dictë¥¼ TrainingConfigë¡œ ë³€í™˜
            config = TrainingConfig(**config_dict)
        else:
            logger.info("ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
            config = TrainingConfig()
        
        return config
    
    def _setup_device(self) -> str:
        """ì»´í“¨íŒ… ì¥ì¹˜ë¥¼ ì„¤ì •í•˜ê³  ê²€ì¦í•©ë‹ˆë‹¤."""
        if torch.cuda.is_available():
            device = 'cuda'
            logger.info(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
            logger.info(f"CUDA ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        else:
            device = 'cpu'
            logger.warning("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
        
        return device
    def create_config_file(output_path: str, auto_categories: list = None) -> None:
        """ìƒ˜í”Œ ì„¤ì • íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤. ìŠ¤íƒ€ì¼ ê¸°ë°˜ target_categories ìë™ ì ìš© ê°€ëŠ¥"""
        
        # ê¸°ë³¸ config ìƒì„±
        config = TrainingConfig()
        
        # target_categories ì„¤ì •
        if auto_categories is not None and len(auto_categories) > 0:
            config.target_categories = auto_categories
        else:
            # ê¸°ë³¸ê°’
            config.target_categories = ["ë ˆíŠ¸ë¡œ", "ë¡œë§¨í‹±", "ë¦¬ì¡°íŠ¸"]

        # dictë¡œ ë³€í™˜
        config_dict = {
            'batch_size': config.batch_size,
            'learning_rate': config.learning_rate,
            'temperature': config.temperature,
            'embedding_dim': config.embedding_dim,
            'hidden_dim': config.hidden_dim,
            'output_dim': config.output_dim,
            'dropout_rate': config.dropout_rate,
            'weight_decay': config.weight_decay,
            'max_epochs': config.max_epochs,
            'target_categories': config.target_categories,
            'image_size': config.image_size,
            'crop_padding': config.crop_padding
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)

        print(f"ìƒ˜í”Œ ì„¤ì • íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}")

    
    
    def setup_data(self, dataset_path: str, **kwargs) -> None:
        logger.info(f"ë°ì´í„°ì…‹ìœ¼ë¡œ ë°ì´í„° ëª¨ë“ˆì„ ì„¤ì •í•©ë‹ˆë‹¤: {dataset_path}")
        
        # JSON í´ë” í™•ì¸
        annotations_dir = Path(dataset_path) / "annotations"
        if not annotations_dir.exists():
            logger.warning("JSON ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ target_categories ì‚¬ìš©")
            detected_categories = self.config.target_categories
        else:
            # JSON íŒŒì¼ì—ì„œ ìŠ¤íƒ€ì¼ ì½ê¸°
            detected_categories = set()
            for json_file in annotations_dir.glob("*.json"):
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    labels = data.get("ë¼ë²¨ë§", {})
                    styles = labels.get("ìŠ¤íƒ€ì¼", [])
                    for style_entry in styles:
                        style_name = style_entry.get("ìŠ¤íƒ€ì¼")
                        if style_name:
                            detected_categories.add(style_name)
            detected_categories = list(detected_categories)
            if len(detected_categories) == 0:
                logger.warning("ìŠ¤íƒ€ì¼ ì •ë³´ê°€ JSONì—ì„œ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ target_categories ì‚¬ìš©")
                detected_categories = self.config.target_categories

        logger.info(f"ìë™ ê°ì§€ëœ target_categories: {detected_categories}")
        
        # data_module ìƒì„±
        data_config = {
            'dataset_path': dataset_path,
            'target_categories': detected_categories,
            'batch_size': self.config.batch_size,
            'image_size': self.config.image_size,
            **kwargs
        }
        
        self.data_module = FashionDataModule(**data_config)
        
        try:
            self.data_module.setup()
            vocab_sizes = self.data_module.get_vocab_sizes()
            logger.info(f"ë°ì´í„° ì„¤ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
            logger.info(f"í•™ìŠµ ìƒ˜í”Œ: {len(self.data_module.train_dataset)}")
            logger.info(f"ê²€ì¦ ìƒ˜í”Œ: {len(self.data_module.val_dataset)}")
            logger.info(f"ì–´íœ˜ í¬ê¸°: {vocab_sizes}")
        except Exception as e:
            logger.error(f"ë°ì´í„° ì„¤ì • ì‹¤íŒ¨: {e}")
            raise


    
    def setup_trainer(self, checkpoint_dir: str = 'checkpoints', 
                     log_dir: str = 'logs') -> None:
        """
        ì„¤ì •ëœ ë°ì´í„° ëª¨ë“ˆë¡œ íŠ¸ë ˆì´ë„ˆë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        
        Args:
            checkpoint_dir: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
            log_dir: í•™ìŠµ ë¡œê·¸ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
        """
        if self.data_module is None:
            raise ValueError("ë°ì´í„° ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € setup_data()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.")
        
        logger.info(f"íŠ¸ë ˆì´ë„ˆë¥¼ ì„¤ì •í•©ë‹ˆë‹¤")
        logger.info(f"ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬: {checkpoint_dir}")
        logger.info(f"ë¡œê·¸ ë””ë ‰í† ë¦¬: {log_dir}")
        
        self.trainer = create_trainer_from_data_module(
            data_module=self.data_module,
            config=self.config,
            device=self.device,
            checkpoint_dir=checkpoint_dir,
            log_dir=log_dir
        )
        
        logger.info("íŠ¸ë ˆì´ë„ˆ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
    
    def train(self, standalone_epochs: int = 5, 
              contrastive_epochs: Optional[int] = None,
              save_results: bool = True) -> Dict[str, Any]:
        """
        ì™„ì „í•œ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            standalone_epochs: ë…ë¦½ JSON ì¸ì½”ë” í•™ìŠµì„ ìœ„í•œ ì—í¬í¬ ìˆ˜
            contrastive_epochs: ëŒ€ì¡° í•™ìŠµì„ ìœ„í•œ ì—í¬í¬ ìˆ˜ (Noneì´ë©´ config ì‚¬ìš©)
            save_results: í•™ìŠµ ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥í• ì§€ ì—¬ë¶€
            
        Returns:
            í•™ìŠµ ê²°ê³¼ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        """
        if self.trainer is None:
            raise ValueError("íŠ¸ë ˆì´ë„ˆê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € setup_trainer()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.")
        
        logger.info("ì™„ì „í•œ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤")
        
        # ë°ì´í„° ë¡œë” ê°€ì ¸ì˜¤ê¸°
        train_loader = self.data_module.train_dataloader()
        val_loader = self.data_module.val_dataloader()
        
        results = {}
        
        # 1ë‹¨ê³„: ë…ë¦½ JSON ì¸ì½”ë” í•™ìŠµ
        if standalone_epochs > 0:
            logger.info(f"1ë‹¨ê³„: ë…ë¦½ JSON ì¸ì½”ë” í•™ìŠµ ({standalone_epochs} ì—í¬í¬)")
            
            standalone_results = self.trainer.train_json_encoder_standalone(
                train_loader=train_loader,
                val_loader=val_loader,
                num_epochs=standalone_epochs
            )
            
            results['standalone'] = standalone_results
            logger.info(f"ë…ë¦½ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì¢… ì†ì‹¤: {standalone_results['val_losses'][-1]:.4f}")
        
        # 2ë‹¨ê³„: ëŒ€ì¡° í•™ìŠµ
        if contrastive_epochs is None:
            contrastive_epochs = self.config.max_epochs
        
        if contrastive_epochs > 0:
            logger.info(f"2ë‹¨ê³„: ëŒ€ì¡° í•™ìŠµ ({contrastive_epochs} ì—í¬í¬)")
            
            contrastive_results = self.trainer.train_contrastive_learning(
                train_loader=train_loader,
                val_loader=val_loader,
                num_epochs=contrastive_epochs
            )
            
            results['contrastive'] = contrastive_results
            logger.info(f"ëŒ€ì¡° í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìµœê³  ì†ì‹¤: {contrastive_results['best_val_loss']:.4f}")
        
        # ê²°ê³¼ ì €ì¥
        if save_results:
            self._save_training_results(results)
        
        logger.info("ì™„ì „í•œ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
        return results
    
    def evaluate(self, checkpoint_path: str, 
                 dataset_path: Optional[str] = None) -> Dict[str, Any]:
        """
        í•™ìŠµëœ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.
        
        Args:
            checkpoint_path: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            dataset_path: í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ (Noneì´ë©´ í•™ìŠµ ë°ì´í„° ì‚¬ìš©)
            
        Returns:
            í‰ê°€ ê²°ê³¼ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤: {checkpoint_path}")
        
        # ì•„ì§ ì„¤ì •ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë°ì´í„° ì„¤ì •
        if self.data_module is None and dataset_path:
            self.setup_data(dataset_path)
        elif self.data_module is None:
            raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤. dataset_pathë¥¼ ì œê³µí•˜ê±°ë‚˜ ë¨¼ì € setup_data()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # ì•„ì§ ì„¤ì •ë˜ì§€ ì•Šì•˜ë‹¤ë©´ íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        if self.trainer is None:
            self.setup_trainer()
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint = self.trainer.load_checkpoint(checkpoint_path)
        
        # í‰ê°€ ì‹¤í–‰
        val_loader = self.data_module.val_dataloader()
        evaluation_results = self.trainer._final_evaluation(val_loader)
        
        logger.info(f"í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
        logger.info(f"ê²°ê³¼: {evaluation_results}")
        
        return evaluation_results
    
    def sanity_check(self, dataset_path: Optional[str] = None,
                    num_epochs: int = 3) -> Dict[str, Any]:
        """
        ì‹œìŠ¤í…œì˜ í¬ê´„ì ì¸ ì •ìƒì„± ê²€ì‚¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            dataset_path: ë°ì´í„°ì…‹ ê²½ë¡œ (Noneì´ë©´ í•©ì„± ë°ì´í„° ì‚¬ìš©)
            num_epochs: ì •ìƒì„± ê²€ì‚¬ í•™ìŠµì„ ìœ„í•œ ì—í¬í¬ ìˆ˜
            
        Returns:
            ì •ìƒì„± ê²€ì‚¬ ê²°ê³¼ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        """
        logger.info("ì‹œìŠ¤í…œ ì •ìƒì„± ê²€ì‚¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤")
        
        try:
            # ë°ì´í„° ì„¤ì •
            if dataset_path:
                self.setup_data(dataset_path, batch_size=16, num_workers=0)
                vocab_sizes = self.data_module.get_vocab_sizes()
            else:
                # í•©ì„± ë°ì´í„° ì‚¬ìš©
                logger.info("ì •ìƒì„± ê²€ì‚¬ë¥¼ ìœ„í•´ í•©ì„± ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
                vocab_sizes = {
                    'category': 10,
                    'style': 20,
                    'silhouette': 15,
                    'material': 25,
                    'detail': 30
                }
                self.data_module = self._create_synthetic_data_module(vocab_sizes)
            
            # ì •ìƒì„± ê²€ì‚¬ ì‹¤í–‰
            checker = JSONEncoderSanityChecker(vocab_sizes, self.device)
            results = checker.run_sanity_check(self.data_module, num_epochs)
            
            # ê²°ê³¼ ì €ì¥
            self._save_sanity_check_results(results)
            
            logger.info("ì •ìƒì„± ê²€ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
            return results
            
        except Exception as e:
            logger.error(f"ì •ìƒì„± ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _create_synthetic_data_module(self, vocab_sizes: Dict[str, int]):
        """í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ í•©ì„± ë°ì´í„° ëª¨ë“ˆì„ ìƒì„±í•©ë‹ˆë‹¤."""
        from examples.json_encoder_sanity_check import create_synthetic_data_module
        return create_synthetic_data_module(vocab_sizes, self.device)
    
    def _save_training_results(self, results: Dict[str, Any]) -> None:
        """í•™ìŠµ ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        output_dir = Path("results")
        output_dir.mkdir(exist_ok=True)
        
        results_file = output_dir / "training_results.json"
        
        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        json_results = self._convert_to_json_serializable(results)
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"í•™ìŠµ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {results_file}")
    
    def _save_sanity_check_results(self, results: Dict[str, Any]) -> None:
        """ì •ìƒì„± ê²€ì‚¬ ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        output_dir = Path("temp_logs")
        output_dir.mkdir(exist_ok=True)
        
        results_file = output_dir / "sanity_check_results.json"
        
        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        json_results = self._convert_to_json_serializable(results)
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ì •ìƒì„± ê²€ì‚¬ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {results_file}")
    
    def _convert_to_json_serializable(self, obj: Any) -> Any:
        """ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if isinstance(obj, dict):
            return {k: self._convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_json_serializable(item) for item in obj]
        elif isinstance(obj, torch.Tensor):
            return obj.tolist() if obj.numel() <= 1000 else f"<Tensor shape={list(obj.shape)}>"
        elif hasattr(obj, '__dict__'):
            return self._convert_to_json_serializable(obj.__dict__)
        else:
            return obj
    
    def cleanup(self) -> None:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        if self.trainer:
            self.trainer.close()
        
        logger.info("ì‹œìŠ¤í…œ ì •ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")


def create_config_file(output_path: str) -> None:
    """ìƒ˜í”Œ ì„¤ì • íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    config = TrainingConfig()
    config.target_categories = ["ë ˆíŠ¸ë¡œ", "ë¡œë§¨í‹±", "ë¦¬ì¡°íŠ¸"]
    
    config_dict = {
        'batch_size': config.batch_size,
        'learning_rate': config.learning_rate,
        'temperature': config.temperature,
        'embedding_dim': config.embedding_dim,
        'hidden_dim': config.hidden_dim,
        'output_dim': config.output_dim,
        'dropout_rate': config.dropout_rate,
        'weight_decay': config.weight_decay,
        'max_epochs': config.max_epochs,
        'target_categories': config.target_categories,
        'image_size': config.image_size,
        'crop_padding': config.crop_padding
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)
    
    print(f"ìƒ˜í”Œ ì„¤ì • íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}")


def main():
    """ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ê°€ ìˆëŠ” ë©”ì¸ ì§„ì…ì ì…ë‹ˆë‹¤."""
    parser = argparse.ArgumentParser(
        description="íŒ¨ì…˜ JSON ì¸ì½”ë” - ë©”ì¸ í†µí•© ì‹œìŠ¤í…œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python main.py train --dataset_path /path/to/kfashion
  python main.py train --dataset_path /path/to/kfashion --config config.json
  python main.py evaluate --checkpoint_path checkpoints/best_model.pt
  python main.py sanity_check
  python main.py sanity_check --dataset_path /path/to/kfashion
  python main.py create_config --output config.json
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´')
    
    # í•™ìŠµ ëª…ë ¹ì–´
    train_parser = subparsers.add_parser('train', help='ëª¨ë¸ í•™ìŠµ')
    train_parser.add_argument('--dataset_path', required=True, 
                             help='K-Fashion ë°ì´í„°ì…‹ ê²½ë¡œ')
    train_parser.add_argument('--config', 
                             help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    train_parser.add_argument('--checkpoint_dir', default='checkpoints',
                             help='ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬')
    train_parser.add_argument('--log_dir', default='logs',
                             help='ë¡œê·¸ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬')
    train_parser.add_argument('--standalone_epochs', type=int, default=5,
                             help='ë…ë¦½ í•™ìŠµì„ ìœ„í•œ ì—í¬í¬ ìˆ˜')
    train_parser.add_argument('--contrastive_epochs', type=int,
                             help='ëŒ€ì¡° í•™ìŠµì„ ìœ„í•œ ì—í¬í¬ ìˆ˜')
    train_parser.add_argument('--batch_size', type=int,
                             help='ë°°ì¹˜ í¬ê¸° (ì„¤ì • ì¬ì •ì˜)')
    train_parser.add_argument('--learning_rate', type=float,
                             help='í•™ìŠµë¥  (ì„¤ì • ì¬ì •ì˜)')
    
    # í‰ê°€ ëª…ë ¹ì–´
    eval_parser = subparsers.add_parser('evaluate', help='í•™ìŠµëœ ëª¨ë¸ í‰ê°€')
    eval_parser.add_argument('--checkpoint_path', required=True,
                            help='ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ')
    eval_parser.add_argument('--dataset_path',
                            help='í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ')
    eval_parser.add_argument('--config',
                            help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    
    # ì •ìƒì„± ê²€ì‚¬ ëª…ë ¹ì–´
    sanity_parser = subparsers.add_parser('sanity_check', help='ì‹œìŠ¤í…œ ì •ìƒì„± ê²€ì‚¬ ì‹¤í–‰')
    sanity_parser.add_argument('--dataset_path',
                              help='ë°ì´í„°ì…‹ ê²½ë¡œ (ì œê³µë˜ì§€ ì•Šìœ¼ë©´ í•©ì„± ë°ì´í„° ì‚¬ìš©)')
    sanity_parser.add_argument('--config',
                              help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    sanity_parser.add_argument('--epochs', type=int, default=3,
                              help='ì •ìƒì„± ê²€ì‚¬ë¥¼ ìœ„í•œ ì—í¬í¬ ìˆ˜')
    
    # ì„¤ì • ìƒì„± ëª…ë ¹ì–´
    config_parser = subparsers.add_parser('create_config', help='ìƒ˜í”Œ ì„¤ì • íŒŒì¼ ìƒì„±')
    config_parser.add_argument('--output', default='config.json',
                              help='ì„¤ì • íŒŒì¼ ì¶œë ¥ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    if args.command is None:
        parser.print_help()
        return
    
    # create_config ëª…ë ¹ì–´ ì²˜ë¦¬
    if args.command == 'create_config':
        create_config_file(args.output)
        return
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    try:
        system = FashionEncoderSystem(config_path=getattr(args, 'config', None))
        
        # ëª…ë ¹ì¤„ ì¸ìˆ˜ë¡œ ì„¤ì • ì¬ì •ì˜
        if hasattr(args, 'batch_size') and args.batch_size:
            system.config.batch_size = args.batch_size
        if hasattr(args, 'learning_rate') and args.learning_rate:
            system.config.learning_rate = args.learning_rate
        
        # ëª…ë ¹ì–´ ì‹¤í–‰
        if args.command == 'train':
            system.setup_data(args.dataset_path)
            system.setup_trainer(args.checkpoint_dir, args.log_dir)
            
            contrastive_epochs = args.contrastive_epochs
            if contrastive_epochs is None:
                contrastive_epochs = system.config.max_epochs
            
            results = system.train(
                standalone_epochs=args.standalone_epochs,
                contrastive_epochs=contrastive_epochs
            )
            
            print("\n" + "="*60)
            print("í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
            print("="*60)
            if 'standalone' in results:
                print(f"ë…ë¦½ í•™ìŠµ ìµœì¢… ì†ì‹¤: {results['standalone']['val_losses'][-1]:.4f}")
            if 'contrastive' in results:
                print(f"ëŒ€ì¡° í•™ìŠµ ìµœê³  ì†ì‹¤: {results['contrastive']['best_val_loss']:.4f}")
                print(f"ìµœì¢… ë©”íŠ¸ë¦­: {results['contrastive']['final_metrics']}")
        
        elif args.command == 'evaluate':
            results = system.evaluate(
                checkpoint_path=args.checkpoint_path,
                dataset_path=getattr(args, 'dataset_path', None)
            )
            
            print("\n" + "="*60)
            print("í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
            print("="*60)
            for key, value in results.items():
                if isinstance(value, float):
                    print(f"{key}: {value:.4f}")
                else:
                    print(f"{key}: {value}")
        
        elif args.command == 'sanity_check':
            results = system.sanity_check(
                dataset_path=getattr(args, 'dataset_path', None),
                num_epochs=args.epochs
            )
            
            print("\n" + "="*60)
            print("ì •ìƒì„± ê²€ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
            print("="*60)
            
            validation = results.get('validation_results', {})
            all_passed = all(validation.get(check, False) for check in validation if check != 'errors')
            
            if all_passed:
                print("ğŸ‰ ëª¨ë“  ì •ìƒì„± ê²€ì‚¬ê°€ í†µê³¼í–ˆìŠµë‹ˆë‹¤!")
            else:
                print("âš ï¸  ì¼ë¶€ ì •ìƒì„± ê²€ì‚¬ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        # ì •ë¦¬
        system.cleanup()
        
    except Exception as e:
        logger.error(f"ëª…ë ¹ì–´ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        print(f"ì˜¤ë¥˜: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()