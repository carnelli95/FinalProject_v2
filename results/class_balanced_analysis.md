# 클래스 균형 샘플링 실험 분석 보고서

## 🔬 실험 개요
- **목표**: 클래스 불균형 해결로 Top-5 정확도 1-2% 향상
- **방법**: ClassBalancedSampler 사용 (레트로 오버샘플링)
- **결과**: 예상과 반대로 성능 하락

## 📊 성능 비교

| 메트릭 | Baseline v1 | Class-Balanced | 변화 |
|--------|-------------|----------------|------|
| Top-5 정확도 | 64.1% | 62.3% | **-1.8%** |
| Top-1 정확도 | 22.2% | 18.1% | **-4.1%** |
| MRR | 0.407 | 0.375 | **-0.032** |
| 검증 손실 | 2.488 | 2.529 | **+0.041** |

## 🤔 성능 하락 원인 분석

### 1. **오버샘플링의 부작용**
- **레트로 클래스 오버샘플링**: 193개 → 988개 (5배 증가)
- **데이터 중복**: 같은 레트로 샘플이 여러 번 반복됨
- **과적합 위험**: 제한된 레트로 데이터에 과도하게 학습

### 2. **Contrastive Learning 특성**
- **Negative 샘플의 중요성**: 다양한 negative 샘플이 중요
- **균형의 역효과**: 인위적 균형이 자연스러운 분포를 왜곡
- **배치 구성**: 강제적 균형이 효과적인 대조 학습을 방해

### 3. **실제 데이터 분포 반영**
- **자연스러운 불균형**: 실제 패션 데이터에서 로맨틱/리조트가 더 많음
- **도메인 특성**: 패션 도메인에서 자연스러운 분포가 더 효과적일 수 있음

## 🎯 다음 실험 방향

### 1. **Temperature 튜닝 (우선순위 1)**
```python
temperatures = [0.05, 0.15, 0.2]
# 0.1이 최적이었으므로 주변 값들 테스트
```

### 2. **Batch Size 증가 (우선순위 2)**
```python
batch_sizes = [32, 64]  # 현재 16에서 증가
# Contrastive Learning에서 더 많은 negative 샘플 확보
```

### 3. **Weighted Loss 접근법**
```python
# 클래스 균형 샘플링 대신 손실 함수에서 가중치 적용
class_weights = {
    '레트로': 2.0,    # 소수 클래스 가중치 증가
    '로맨틱': 1.0,
    '리조트': 1.0
}
```

### 4. **Hybrid 접근법**
```python
# 부분적 오버샘플링 (완전 균형이 아닌 적당한 균형)
oversample_ratio = 0.5  # 50% 오버샘플링
```

## 📈 실험 우선순위

1. **Temperature 0.15 실험** (3-5 에포크, 빠른 검증)
2. **Batch Size 32 실험** (8 에포크, 전체 학습)
3. **Temperature 종합 실험** (0.05, 0.07, 0.1, 0.15, 0.2)

## 🔍 학습된 교훈

1. **클래스 불균형이 항상 문제는 아님**: 자연스러운 분포가 더 효과적일 수 있음
2. **Contrastive Learning의 특성**: 다양성이 균형보다 중요할 수 있음
3. **도메인 특성 고려**: 패션 데이터의 자연스러운 분포 존중

## 🚀 다음 단계

**즉시 실행**: Temperature 0.15 실험 (예상 소요시간: 10분)
- 목표: 64.1% → 65-66% Top-5 정확도
- 근거: 패션/텍스트 contrastive learning에서 0.1-0.2 범위가 효과적

---

**결론**: 클래스 균형 샘플링은 이 도메인에서 효과적이지 않았음. Temperature 튜닝으로 방향 전환.